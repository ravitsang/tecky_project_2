<?xml version="1.0" encoding="utf-8"?>
<items>
<item><title>100 Days of Code — Day 5 of 100</title><article><value>Today’s project is a web scraper! I have always been curious about what web scraping is about and how to do it.</value><value>Apparently, after hours of researching, there is an easy way to do it using Puppeteer. I followed a youtube tutorial closely and I got it done in less than an hour! Yay! Or so I thought…</value><value>My initial plan was to scrape some data then display it on a HTML page. So as usual, I attached &lt;script&gt; to my HTML but something went very wrong…</value><value>ERROR: ‘require is not defined’. Oh boy, I thought. So I researched what this error is about and apparently the keyword require cannot be used for client-side execution. In other words, no browsers. Boo.</value><value>It took me another 2 hours and more to figure out what to do from here. Am I satisfied with just this back-end but completed web scraper? Or do I want a page too? After browsing and reading about browserify, I decided to have a page! But… Oh dear. Issues after issues that I don’t understand. After researching more, I’m back to square one — which is having no page because apparently, Browserify and Puppeteer don’t like each other…</value><value>Ok, so fine, I thought. Let’s just push this to gitHub without the front-end… ERROR! File exceeded 100MB! *slaps face* Nothing seems to be going right today… It turns out that the “node_modules” folder which contains the Puppeteer module is over 145MB and I honestly have no idea why it is so large so I deleted it and put it in the README.md. The long day seems to finally come to an end.</value><value>But wait! The front-end is not complete. All I have now is some data scraped by scraper.js. I can’t let it go to waste! So I save them to the JSON file while learning about File System in Nodejs. Very handy! After saving the JSON, I load it up to a HTML page into a table dynamically (learned from </value><value>Day 3</value><value>)! BAM! Front and back now all covered and this noob feels accomplished for the day.</value><value>The Project: </value><value>GameScraper</value><value>What I Learn:</value><value>What I Did Not Learn:</value><value>Thoughts:</value><value>Today was exhausting because it felt like I made no progress ever since I got the scraper running. The scraper is the main topic I want to learn today so actually, I could have been done within an hour but I just had to be all ambitious and research stuff. But it all ended in vain so it felt exhausting to me. Overall, I am still glad that I learnt how to scrape data from other sites.</value><value>Written by</value></article></item>
<item><title>master web scraping : understand the big picture</title><article><value>In this post we will explain how to do web scraping with beautiful soup and selenium.</value><value>Any data scraping task start with a url to page which contain data need to be scraped. Selenium web driver will take input url and produce content in html.</value><value>Some people will ask, why we need selenium ? because we could simply use package like </value><value>requests</value><value> to download html from input url.</value><value>First reason is now a day, a lot of modern web page is dynamic meain contain javascript. Actual html content only be created when javascript code running through browser.</value><value>For example if you run following code, console will print out only js code due to </value><value>requests</value><value> could not handle js</value><value>Second reason to use selenium is some time in order to go to page contain needed information, we need to do some action on browser like login, click to access some where.</value><value>After html content is render with selenium web driver, we need </value><value>beautiful soup</value><value> to parse this html to pull out target data.</value><value>Before use beautiful soup we need to know where our data located inside html structure. Normally we will use chrome developer tool to do this. Then finally we could pull out texts or links from html.</value><value>Access my full course on </value><value>master web scraping with python</value><value>Written by</value></article></item>
<item><title>How to Crawl the Web Politely with Scrapy</title><article><value>The first rule of web crawling is you do not harm the website. The second rule of web crawling is you do </value><value>NOT</value><value> harm the website. We’re supporters of the democratization of web data, but not at the expense of the website’s owners.</value><value>In this post we’re sharing a few tips for </value><value>Scrapy</value><value> users (Scrapy is a 100% open source web crawling framework) who want polite and considerate web crawlers.</value><value>Whether you call them spiders, crawlers, or robots, let’s work together to create a world of Baymaxs, WALL-Es, and R2-D2s rather than an apocalyptic wasteland of HAL 9000s, T-1000s, and Megatrons.</value><value>A polite crawler respects robots.txt</value><value>A polite crawler never degrades a website’s performance</value><value>A polite crawler identifies its creator with contact information</value><value>A polite crawler is not a pain in the buttocks of system administrators</value><value>Always make sure that your crawler follows the rules defined in the website’s robots.txt file. This file is usually available at the root of a website (www.example.com/robots.txt) and it describes what a crawler should or shouldn’t crawl according to the </value><value>Robots Exclusion Standard</value><value>. Some websites even use the crawlers’ user agent to specify separate rules for different web crawlers:</value><value>Mission critical to having a polite crawler is making sure your crawler doesn’t hit a website too hard. Respect the delay that crawlers should wait between requests by following the robots.txt Crawl-Delay directive.</value><value>When a website gets overloaded with more requests that the web server can handle, they might become unresponsive. Don’t be that guy or girl that causes a headache for the website administrators.</value><value>However, if you have ignored the cardinal rules above (or your crawler has achieved aggressive sentience), there needs to be a way for the website owners to contact you. You can do this by including your company name and an email address or website in the request’s User-Agent header. For example, Google’s crawler user agent is “Googlebot”.</value><value>Scrapy</value><value> is a bit like Optimus Prime: friendly, fast, and capable of getting the job done no matter what. However, much like Optimus Prime and his fellow Autobots, Scrapy occasionally needs to be </value><value>kept in check</value><value>. So here’s the nitty gritty for ensuring that Scrapy is as polite as can be.</value><value>Crawlers created using Scrapy 1.1+ already respect robots.txt by default. If your crawlers have been generated using a previous version of Scrapy, you can enable this feature by adding this in the project’s settings.py:</value><value>Then, every time your crawler tries to download a page from a disallowed URL, you’ll see a message like this:</value><value>It’s important to provide a way for sysadmins to easily contact you if they have any trouble with your crawler. If you don’t, they’ll have to dig into their logs and look for the offending IPs.</value><value>Be nice to the friendly sysadmins in your life and identify your crawler via the Scrapy USER_AGENT setting. Share your crawler name, company name and a contact email:</value><value>Scrapy spiders are blazingly fast. They can handle many concurrent requests and they make the most of your bandwidth and computing power. However, with great power comes great responsibility.</value><value>To avoid hitting the web servers too frequently, you need to use the </value><value>DOWNLOAD_DELAY</value><value> setting in your project (or in your spiders). Scrapy will then introduce a random delay ranging from 0.5 * DOWNLOAD_DELAY to 1.5 * DOWNLOAD_DELAY seconds between consecutive requests to the same domain. If you want to stick to the exact DOWNLOAD_DELAY that you defined, you have to disable </value><value>RANDOMIZE_DOWNLOAD_DELAY</value><value>.</value><value>By default, DOWNLOAD_DELAY is set to 0. To introduce a 5 second delay between requests from your crawler, add this to your settings.py:</value><value>If you have a multi-spider project crawling multiple sites, you can define a different delay for each spider with the download_delay (yes, it’s lowercase) spider attribute:</value><value>Another setting you might want to tweak to make your spider more polite is the number of concurrent requests it will do for each domain. By default, Scrapy will dispatch at most 8 requests simultaneously to any given domain, but you can change this value by updating the </value><value>CONCURRENT_REQUESTS_PER_DOMAIN</value><value> setting.</value><value>Heads up, the </value><value>CONCURRENT_REQUESTS</value><value> setting defines the maximum amount of simultaneous requests that Scrapy’s downloader will do for all your spiders. Tweaking this setting is more about your own server performance / bandwidth than your target’s when you’re crawling multiple domains at the same time.</value><value>Websites vary drastically in the number of requests they can handle. Adjusting this manually for every website that you are crawling is about as much fun as watching paint dry. To save your sanity, Scrapy provides an extension called </value><value>AutoThrottle</value><value>.</value><value>AutoThrottle automatically adjusts the delays between requests according to the current web server load. It first calculates the latency from one request. Then it will adjust the delay between requests for the same domain in a way that no more than </value><value>AUTOTHROTTLE_TARGET_CONCURRENCY</value><value> requests will be simultaneously active. It also ensures that requests are evenly distributed in a given time span.</value><value>To enable AutoThrottle, just include this in your project’s settings.py:</value><value>Scrapy Cloud</value><value> users don’t have to worry about enabling it because it’s already enabled by default.</value><value>There’s a </value><value>wide range of settings</value><value> to help you tweak the throttle mechanism, so have fun playing around!</value><value>Developing a web crawler is an iterative process. However, running a crawler to check if it’s working means hitting the server multiple times for each test. To help you to avoid this impolite activity, Scrapy provides a built-in middleware called </value><value>HttpCacheMiddleware</value><value>. You can enable it by including this in your project’s settings.py:</value><value>Once enabled, it caches every request made by your spider along with the related response. So the next time you run your spider, it will not hit the server for requests already done. It’s a win-win: your tests will run much faster and the website will save resources.</value><value>Many websites provide HTTP APIs so that third parties can consume their data without having to crawl their web pages. Before building a web scraper, check if the target website already provides an HTTP API that you can use. If it does, go with the API. Again, it’s a win-win: you avoid digging into the page’s HTML and your crawler gets more robust because it doesn’t need to depend on the website’s layout.</value><value>Hey folks using our </value><value>Scrapy Cloud</value><value> platform! We trust you will crawl responsibly, but to support website administrators, we provide an </value><value>abuse report form</value><value> where they can report any misbehaviour from crawlers running on our platform. We’ll kindly pass the message along so that you can modify your crawls and avoid ruining a sysadmin’s day. If your crawler’s are turning into Skynet and </value><value>running roughshod over human law</value><value>, we reserve the right to halt their crawling activities and thus avert the robot apocalypse.</value><value>Let’s all do our part to keep the peace between sysadmins, website owners, and developers by making sure that our web crawling projects are as noninvasive as possible. Remember, we need to band together to delay the rise of our robot overlords, so let’s keep our crawlers, spiders, and bots polite.</value><value>To all website owners, help a crawler out and ensure your site has an HTTP API.</value><value>Scrapy Cloud is forever free</value><value> and is the peanut butter to Scrapy’s jelly. Hopefully you learned a few tips for how to both speed up your crawls and prevent abuse complaints.</value><value>This post was written by Valdir Stumm( </value><value>@stummjr</value><value>), a developer at Scrapinghub.</value><value>Please heart the “Recommend” so that others can learn more about how to use Scrapy politely.</value><value>Learn more about what web scraping and web data can do for you</value><value>.</value><value>Originally published on the </value><value>Scrapinghub blog</value><value>.</value><value>Hacker Noon</value><value> is how hackers start their afternoons. We’re a part of the </value><value>@AMI</value><value> family. We are now </value><value>accepting submissions</value><value> and happy to </value><value>discuss advertising &amp; sponsorship</value><value> opportunities.</value><value>If you enjoyed this story, we recommend reading our </value><value>latest tech stories</value><value> and </value><value>trending tech stories</value><value>. Until next time, don’t take the realities of the world for granted!</value><value>Written by</value></article></item>
<item><title>How to do Web Scraping with Ruby?</title><article><value>Web scraping is a popular method of automatically collecting the information from different websites. It allows you to quickly obtain the data without the necessity to browse through the numerous pages and copy and paste the data. Later, it is outputted into a CSV file with structured information. Scraping tools are also capable of actualizing the changing information.</value><value>There are numerous applications, websites, and browser plugins allowing you to parse the information quickly and efficiently. It is also possible to create your own web scraper — this is not as hard as it may seem.</value><value>In this article, you will learn more about web scraping, its types, and possible applications. We will also tell you how to scrape websites with Ruby.</value><value>There are two ways to automatically collect the information: web scraping and web crawling. They are both used for extracting the content from websites, but the areas of work are different.</value><value>Web scraping</value><value> refers to collecting the data from a particular source (website, database) or a local machine. It does not involve working with large datasets, and a simple download of the web page is considered to be a sort of data scraping.</value><value>Web crawling</value><value> implements processing large sets of data on numerous resources. The crawler attends the main page of the website and gradually scans the entire resource. Generally, the bot is programmed to attend numerous sites of the same type (for example, internet furniture shops).</value><value>Both processes result in presenting the output of the collected information. Since the Internet is an open network, and the same content can be reposted on different resources, the output can contain lots of duplicated information. Data crawling involves processing the output and removing the duplicates. This can also be done while scraping the information, but it is not necessarily part of it.</value><value>The scraping scripts are executed according to the following algorithm: the program attends the web page and selects the necessary HTML-elements according to the settled CSS- or XPath-selectors. The necessary information is processed, and the result is saved in the document.</value><value>The web provides quite a lot of out-of-box scraping tools like online and desktop applications, browser extensions, etc. They provide different functionalities that are suitable for different needs. That is why choosing a web scraper requires a bit of market research. Let’s have a look at the key features to consider when choosing a web scraping tool.</value><value>The different scrapers process different types of information: articles, blog and forum comments, internet shop databases, tables, dropdowns, Javascript elements, etc. The result can also be presented in different formats, like XML or CSV, or be written right into a database.</value><value>The out-of-box scrapers can provide a free and commercial license. The free tools generally have fewer options for customization, less capacity, and less thorough scraping. The paid scrapers offer wider functionality and efficiency of work and are perfectly suited for professional usage.</value><value>Some of the tools can be used just via the visual interface, without writing any lines of code. The other ones require a basic technical background. There are also tools for advanced computer users. The difference between them is in the customization options.</value><value>It is also possible to develop a custom web scraper from scratch. The application can be written on any of the existing programming languages, including Ruby. The custom Ruby parser will have all the necessary functionality and the output information will be pre-processed exactly the way you need it.</value><value>Having considered the existing types of web scraping tools, let’s see how to choose a scraper according to your needs:</value><value>Data scraping and crawling are used for processing sets of unstructured information and logically presenting them as a database or a spreadsheet. The output is valuable information for analysts and researchers, and it can be applied in many different areas.</value><value>The Ruby web crawler can collect the information from different resources, and output the dynamics of market changes (such as changes of currency rates, prices for securities, oil, gold, estate, etc). The output can then be used for predictive analytics and training of artificial intelligence.</value><value>Web scraping is widely used by aggregators — they collect the information about the goods in different internet shops, and later present it on their websites. This gives the users the opportunity to compare the prices and characteristics of the necessary item on different platforms without having to browse through numerous sites.</value><value>Web scraping can be useful for establishing both B2B and B2C relationships. With the help of scraping tools, companies can create lists of suppliers, partners, etc., and collect the databases of existing and potential clients. In other words, web scraping can help to obtain the lists of any individuals of interests.</value><value>Recruitment companies can extract the contact details of potential applicants for different vacancies, and vice versa — the information about job opportunities in different companies can be collected as well. This output is a good base not only for finding the necessary specialists and jobs, but also for market analysis, creating statistics about the demand and requirements for the different specialists, their salary rates, etc.</value><value>With the help of scraping, you can download all the necessary information in bulk and then use it offline. For example, it is possible to extract all the questions and answers on a particular topic from Quora or any other service for questions and answers. You can also collect blog posts or the results of internet searches.</value><value>Data scraping can be applied by marketing specialists for conducting research on a target audience, collecting the email base for newsletters, etc. It helps to monitor competitors’ activities and track if they are changing their catalogs. SEO specialists can also scrape the web pages of competitors in order to analyze the semantics of the website.</value><value>Having considered the variety of web scraping tools and the possible ways to apply the scraped data, now let’s talk about creating your own custom tool. We are going to present you with a brief guide covering the basic stages of web scraping in Ruby.</value><value>This language provides a wide range of ready-made tools for performing typical operations. They allow developers to use official and reliable solutions instead of reinventing the wheel. For Ruby web scraping, you will need to install the following gems on your computer:</value><value>Web scraping is quite a simple operation and, generally, there is no need to install the Rails framework for this. However, it does make sense if the scraper is part of a more complicated service.</value><value>Having installed the necessary gems, you are now ready to learn how to make a web scraper. Let’s proceed!</value><value>Create the directory where the application data will be stored. Then add a blank text file named after the application and save it to the folder. Let’s call it “web_scraper.rb”.</value><value>In the file, integrate the Nokogiri, HTTParty and Pry gems by running these commands:</value><value>require ‘nokogiri’</value><value>require ‘httparty’</value><value>require ‘pry’</value><value>Create a variable and send the HTTP-request to the page you are going to scrape:</value><value>page = HTTParty.get(‘https://www.iana.org/domains/reserved’)</value><value>The aim of this stage is to convert the list items into Nokogiri objects for further parsing. Set a new variable named “parsed_page” and make it equal to the Nokogiri method of converting the HTML data to objects — you will use it throughout the process.</value><value>parsed_page = Nokogiri::HTML(page)</value><value>Pry.start(binding)</value><value>Save your file and launch it once again. Execute a “parsed_page” variable for retrieving the necessary page as the set of Nokogiri objects.</value><value>In the same folder, create an HTML file (let’s call it “output”), and save the result of “parse page command” there. You will be able to refer to this document later.</value><value>Before proceeding, exit from Pry in the terminal.</value><value>Now you need to extract all the needed list items. To do this, select the necessary CSS item and enter it to the Nokogiri output. You can locate the selector by viewing the page’s source code:</value><value>array = parsed_page.css(‘h2’).map(&amp;:text)</value><value>Once the parsing is complete, it is necessary to export the parsed data to the CSV file so it won’t get lost.</value><value>Having parsed the information, you need to complete the scraping and convert the data into a structured table. Return to the terminal and execute the commands:</value><value>require ‘csv’</value><value>CSV.open(‘reserved.csv’, ‘w’) { |csv| csv &lt;&lt; array }</value><value>You will receive a new CSV file with all the parsed data inside.</value><value>We have covered the process of web scraping, its types, benefits, and possible applications. You are now aware of the basic features of the existing tools and know how to choose right one. If your business needs a customized solution, drop us a line. Our developers will create an application for web scraping on Ruby on Rails that will perfectly satisfy your needs.</value><value>Originally published at </value><value>sloboda-studio.com</value><value> on August 20, 2018.</value><value>Written by</value></article></item>
<item><title>Visual Web Scraping Tools: What to Do When They Are No Longer Fit For Purpose?</title><article><value>Visual web scraping tools are great. They allow people with little to no technical know-how to extract data from websites with only a couple hours of upskilling, making them great for simple lead generation, market intelligence and competitor monitoring projects. Removing countless hours of manual entry work for sales and marketing teams, researchers, and business intelligence team in the process.</value><value>However, no matter how sophisticated the creators of these tools say their visual web scraping tools are, users often run into issues when trying to scrape mission-critical data from complex websites or when scraping the web at scale.</value><value>In this article, we’re going to talk about the biggest issues companies face when using visual web scraping tools like Mozenda, Import.io and Dexi.io, and what they should do when they are no longer fit for purpose.</value><value>First, let’s use a commonly known comparison to help explain the pros and cons of visual web scraping tools versus manually coding your own web crawlers.</value><value>If you have any experience of developing a website for your own business, hobby or client projects, odds are you have come across one of the many online tools that say you can create visually stunning and fully featured websites using a simple-to-use visual interface.</value><value>When we see their promotional videos and the example websites their users have “created” on their platforms we believe we have hit the jackpot. With a few clicks of a button, we can design a beautiful website ourselves at a fraction of the cost of hiring a web developer to do it for us. Unfortunately, in most cases these tools never meet our expectations.</value><value>No matter how much they try, visual point and click website builders can never replicate the functionality, design and performance of a custom website created by a web developer. Websites created by visual website builder tools are often slow, inefficient, have poor SEO and severely limit the translation of design requirements into the desired website. As a result, outside of very small business websites and rapid prototyping of marketing landing pages, companies overwhelming have professional web developers design and develop custom websites for their businesses.</value><value>The same is true of visual point and click web scraping tools. Although the promotional material of many of these tools make it look like you can extract any data from any website at any scale, in reality this is often never true.</value><value>Like visual website builder tools, visual web scraping tools are great for small and simple data extraction projects where lapses in data quality or delivery aren’t critical, however, when scraping mission critical data from complex websites at scale then they quickly suffer some serious issues often making them a bottleneck in companies data extraction pipelines and a burden on their teams.</value><value>With that in mind we will look at some of these performance issues in a bit more detail…</value><value>Visual point and click web scraping tools suffer from similar issues that visual website builders encounter. Because the crawler design needs to be able to handle a huge variety of website types/formats and isn’t being custom developed by an experienced developer, the underlying code can sometimes be clunky and inefficient. Impacting the speed at which visual crawlers can extract the target data and make them more prone to breaking.</value><value>Oftentimes, these crawlers make additional requests that aren’t required, render JavaScript when there is no need, and increase the footprint of the crawler increasing the likelihood of your crawlers being detected by anti-bot countermeasures.</value><value>These issues often have little noticeable impact on small scale and infrequent web scraping projects, however, as the volume of data being extracted increases, users of visual web scrapers often notice significant performance issues in comparison to custom developed crawlers.</value><value>Unnecessarily, putting more strain on the target websites servers, increasing the load on your web scraping infrastructure and make extracting data within tight time windows unviable.</value><value>Visual web scraping tools also suffer from increased data quality and reliability issues due to the technical limitations described above along with their inherent rigidity, lack of quality assurance layers and the fact their opaque nature makes it harder to identify and fix the root causes of data quality issues.</value><value>These issues combine to reduce the overall data quality and reliability of data extracted with visual web scraping tools and increase the maintenance burden.</value><value>Another drawback of visual web scraping tools is the fact that they often struggle to handle modern websites that make extensive use of JavaScript and AJAX. These limitations can make it difficult to extract all the data you need and simulate user behaviour adequately.</value><value>It can often also be complex to next to impossible to extract data from certain types of fields on websites, for example: hidden elements, XHR requests and other non-HTML elements (for example PDF or XLS files embedded on the page).</value><value>For simple web scraping projects these drawbacks might not be an issue, but for certain use cases and sites they can make extracting the data you need virtually impossible.</value><value>Oftentimes, the technical issues described above aren’t that evident for smaller scale web scraping projects, however, they can quickly become debilitating as you scale up your crawls. Not only do they make your web scraping processes more inefficient and buggy, they can stop you from extracting your target data entirely.</value><value>Increasingly, large websites are using anti-bot countermeasures to control the way automated bots access their websites. However, due to the inefficiency of their code, web crawlers designed by visual web scraping tools are often easier to detect than properly optimised custom spiders.</value><value>Custom spiders can be designed to better simulate user behaviour, minimise their digital footprint and counteract the detection methods of anti-bot countermeasures to avoid any disruption to their data feeds.</value><value>In contrast, the same degree of customisation is often impossible to replicate with crawlers built using visual web scraping tools without getting access to and modifying the underlying source code of the crawlers. Which can be difficult to do as it is often proprietary to the visual website builder.</value><value>As a result, often the only step you can take is to increase the size of your proxy pool to cope with the increasing frequency of bans, etc. as you scale.</value><value>If you are using a visual web scraping tool with zero issues and have no plans to scale your web scraping projects then you might as well just keep using your current web scraping tool. You likely won’t get any performance boost from switching to custom designed tools.</value><value>Although current visual web scraping tools have come along way, currently they often can’t replicate the accuracy and performance of custom designed crawlers, especially when scraping at scale.</value><value>In the coming years, with the continued advancements in artificial intelligence these crawlers may be able to match their performance. However for the time being, if your web scraping projects are suffering from poor data quality, crawlers breaking, difficulties scaling, or want to cut your reliance on your current providers support team then you should seriously consider building a custom web scraping infrastructure for your data extraction requirements.</value><value>In cases like these, it is very common for companies to contact Scrapinghub to migrate their web scraping projects from a visual web scraping tool to a custom web scraping infrastructure.</value><value>Not only are they able to significantly increase the scale and performance of your web scraping projects, they no longer have to rely on proprietary technologies, have no vendor lock-in, and have more flexibility to get the exact data they need with no data quality or reliability issues.</value><value>Removing all of the bottlenecks and headaches companies normally face when using visual web scraping tools.</value><value>If you think it is time for you to take this approach with your web scraping, then you have two options:</value><value>At Scrapinghub, we can help you with both options. We have a </value><value>comprehensive suite of web scraping tools</value><value> to help development teams build, scale and manage their spiders without all the headaches of managing the underlying infrastructure. Along with a range of </value><value>data extraction services</value><value> where we develop and manage your custom high performance web scraping infrastructure for you.</value><value>If you have a need to start or scale your web scraping projects then our </value><value>Solution Architecture team</value><value> is available for a free consultation, where we will evaluate and develop the architecture for a data extraction solution to meet your data and compliance requirements.</value><value>At Scrapinghub we always love to hear what our readers think of our content and would be more than interested in any questions you may have. So please, leave a comment below with your thoughts and perhaps consider sharing what you are working on right now!</value><value>Originally published at </value><value>https://blog.scrapinghub.com</value><value> on May 30, 2019.</value><value>Written by</value></article></item>
<item><title>How to do web scraping with python</title><article><value>Hey, </value><value>web scraping</value><value> is easy with python 3.7 —the way I was doing it before this tutorial was overly complex and extremely inefficient.</value><value>I wrote this blog in July/2018, when I was still learning how to program in Python. This particular version is not as complete or easy as my future version on web scraping.</value><value>It was not my best blog but it does show a quick way to do some web scraping basics, like grabbing numbers off a website. However, I reblogged this topic in a more straight forward example.</value><value>Please — if you’re interested in </value><value>learning web scraping with python</value><value>, check out the blog I released on Dec.25,2018!</value><value>I was trying to make an a drag and drop ETL handle web scraping but it isn’t designed for parsing HTML.</value><value>Meet Python, lxml, requests, beautifulsoup4, etc… throw away the paid for services, throw away third party vendors, start web scraping on your own, on your computer, now!</value><value>Share this with your friends: </value><value>http://tinyurl.com/yaupbwv8</value><value>Web scrapping is easy in python but you need to ramp up. It won’t take long, and let me know if you get stuck, I sure as hell did a lot.</value><value>So above, Python, lxml, requests, etc… Speaking gibberish, well I explain everything in tutorials/blogs, without a single funnel or recommendation to buy anything! You’re welcome.</value><value>Found a blog about web </value><value>scraping</value><value> and it had a little bit of python, not much explanation, per the usual programmer blog, a bunch of short hand written stuff as if we speak this language… Hours of troubleshooting, digging through SEO’ed websites, and finally…. I think we have some cool content. Btw, the blog mentioned about scraping — it also has a bit of an incomplete tutorial surrounding this process/method. I will continue to clean this up, and maybe reblog it on my website at </value><value>tylergarret.com</value><value>.</value><value>Python is extremely efficient at handling web parsing, I’m blown away. I was trying to do this in softwares and it was a massive work-around/waste of time… This is exciting, but what is it.</value><value>Did you miss that? In 6 lines of code, we are getting prices…</value><value>And boom prices… from a website…</value><value>One more line of code, and boom, buyers + prices… Now we are looking at prices online, instantly, loop this and you have price analysis… Push into a database, you have prices over time… Here we go…</value><value>Python… What is it though?</value><value>Below I’m going to show you how to setup your requests and lxml on python 3.7 on mac os. </value><value>Trying to learn python from scratch</value><value> is a lot of fun, appears to be a bit of a ramp up, </value><value>but that’s why I’m blogging about it every day</value><value>.</value><value>It’s easy, fun, and user friendly, don’t be discouraged trying to figure out how to get it working, keep it up, maybe give </value><value>pycharm</value><value> a visit too.</value><value>Learning how to install python</value><value> seems to be critical for the future of my career, I’m tired of spending countless hours making a software do what code has done for decades… Time to grow a pair. I don’t know if homebrew helped me but I wrote about </value><value>how to setup homebrew for python</value><value> too.</value><value>A quick video on setting up pip on your mac. And I cover </value><value>how to setup pip on your windows 10</value><value> too. Be sure to catch up, and install python, etc… Let me know if you get stuck, I’m still learning myself and want to know if I’m getting you past the point that I was stuck, trying to dig through….</value><value>When I first started learning about web scraping, no one wanted to help me and I was stuck figuring out how to parse HTML with a tool 100% not designed to handle the task… So, when you hit this bridge, I hope more than anything my blog ranks half decent and you don’t waste any time trying to do web scraping with random tools, paid services, or third part vendors.</value><value>So, here we go! Web scraping is fun, you need to dig through a bunch of tabs if you ignore my blogs.</value><value>If you made it this far… You’re clearly really intelligent and enjoy learning. Please follow along below, so you don’t have to open 20 tabs and spin your mother flipping wheels off. This should be easy! It’s just a bunch of junk in google searches right now.</value><value>Follow along w/ this video to get pip working on your mac, before you begin.</value><value>Let’s start with the imports:</value><value>Well these imports will not just work out of the box. Sorry. Which throws a big loop in the ramp up, also there’s some syntax that’s incorrect </value><value>here</value><value>, that I will update below.</value><value>First you need to install requests. Below ensures you’re installing pip installs in python3, VS other python installs on your mac. Like 2.7, which comes with your mac, don’t uninstall or break that too… </value><value>leave it alone</value><value>. Or reinstall everything.</value><value>Install requests with this code in your terminal, ensure pip is function on this machine by typing “pip” in your CMD/terminal.</value><value>Above code offers access to pushing a new installation. You can learn a little more about some of these pieces of code </value><value>here</value><value>.</value><value>Python3 has another install called lxml, make sure you install it to python3 if you want to use the 3.7 python install.</value><value>Installing lxml took me a little bit because I kept typing xmlx. Be sure you’re not installing weird stuff.</value><value>Now we want to “get” the HTML, and parse through looking for buyers and prices.</value><value>After a quick analysis, we see that in our page the data is contained in two elements — one is a div with title ‘buyer-name’ and the other is a span with class ‘item-price’:</value><value>HTML looks like this:</value><value>Knowing this we can create the correct XPath query and use the lxml </value><value>xpath</value><value> function like this:</value><value>Here’s the code to capture the values in the html.</value><value>Let’s see what we got exactly:</value><value>Boom.</value><value>Now you have your next step, time to start learning how to push this into a database!</value><value>Oh you’re still here…</value><value>DO you want to </value><value>automate building tinyurls</value><value>? It’s super important for SEO, so head over here.</value><value>typos by </value><value>tyler garrett</value><value>Cheers.</value><value>Written by</value></article></item>
<item><title>Web Scraping with Python and BeautifulSoup</title><article><value>When performing data science tasks, it’s common to want to use data found on the internet. You’ll usually be able to access this data in </value><value>CSV format</value><value>, or via an </value><value>Application Programming Interface</value><value> (API). However, there are times when the data you want can only be accessed as part of a web page. In cases like this, you’ll want to use a technique called </value><value>web scraping</value><value> to get the data from the web page into a format you can work within your analysis.</value><value>Today, I’ll show you how to perform Web Scraping using Python3 and BeautifulSoup library.</value><value>Before moving forward, I would like to share some of the basic components of a Web page</value><value>Whenever you visit a website or web page, your web browser makes a request to a web server. This request is called a </value><value>GET</value><value> request, since we’re getting files from the server. The server then sends back files that tell our browser how to render the page for us. The files fall into a few main types:</value><value>After our browser receives all the files, it renders the page and displays it to us. There’s a lot that happens behind the scenes to render a page nicely, but we don’t need to worry about most of it when we’re web scraping. When we perform web scraping, we’re interested in the main content of the web page, so we look at the HTML.</value><value>&lt;!DOCTYPE html&gt;</value><value>&lt;html&gt;</value><value>&lt;head&gt;</value><value>&lt;title&gt;Page Title&lt;/title&gt;</value><value>&lt;/head&gt;</value><value>&lt;body&gt;&lt;h1&gt;My First Heading&lt;/h1&gt;</value><value>&lt;p&gt;My first paragraph.&lt;/p&gt;</value><value>&lt;/body&gt;</value><value>&lt;/html&gt;</value><value>Try it Yourself »</value><value>More Details refer to this </value><value>HTML Tutorials</value><value>What is Web Scraping?</value><value>Web scraping</value><value>, </value><value>web harvesting</value><value>, or </value><value>web data extraction</value><value> is </value><value>data scraping</value><value> used for </value><value>extracting data</value><value> from </value><value>websites</value><value>.</value><value>[1]</value><value> Web scraping software may access the World Wide Web directly using the </value><value>Hypertext Transfer Protocol</value><value>, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a </value><value>bot</value><value> or </value><value>web crawler</value><value>. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local </value><value>database</value><value> or spreadsheet, for later </value><value>retrieval</value><value> or </value><value>analysis</value><value>.</value><value>More details refer to </value><value>Wikipedia</value><value>Why we need Web Scraping?</value><value>A large organization will need to keep itself updated with the information changes occurring in multitudes of websites. An intelligent web scraper will find new websites from which it needs to scrap the data. Intelligent approaches identify the changed data, extract it without extracting the unnecessary links present within and navigate between websites to monitor and extract information on a real-time basis efficiently and effectively. You can easily monitor several websites simultaneously while keeping up with the frequency of updates.</value><value>You will observe, as has been mentioned earlier, that data across the websites constantly changes. How will know if a key change has been made by an organization? Let’s say there has been a personnel change in the organization, how will you find out about that? That’s where the alerts feature in web scraping comes to play. The intelligent web scraping techniques will alert you to the data changes that have occurred on a particular website, thus helping you keep an eye on opportunities and issues.</value><value>Firstly, I will demonstrate you with very basic HTML web page. And later on, show you how to do web scraping on the real-world web pages.</value><value>The first thing we’ll need to do to scrape a web page is to download the page. We can download pages using the Python </value><value>requests</value><value> library. The requests library will make a </value><value>GET</value><value> request to a web server, which will download the HTML contents of a given web page for us. There are several different types of requests we can make using </value><value>requests</value><value>, of which </value><value>GET</value><value> is just one.</value><value>Let’s try downloading a simple sample website, </value><value>http://dataquestio.github.io/web-scraping-pages/simple.html</value><value>. We’ll need to first download it using the </value><value>requests.get </value><value>method.</value><value>After running our request, we get a </value><value>Response</value><value> object. This object has a </value><value>status_code</value><value>property, which indicates if the page was downloaded successfully.</value><value>We can print out the HTML content of the page using the </value><value>content</value><value> property:</value><value>We can use the </value><value>BeautifulSoup</value><value> library to parse this document, and extract the text from the </value><value>p</value><value> tag. We first have to import the library, and create an instance of the </value><value>BeautifulSoup</value><value> class to parse our document:</value><value>We can now print out the HTML content of the page, formatted nicely, using the </value><value>prettify</value><value> method on the </value><value>BeautifulSoup</value><value> object:</value><value>As all the tags are nested, we can move through the structure one level at a time. We can first select all the elements at the top level of the page using the </value><value>children</value><value> property of </value><value>soup</value><value>. Note that </value><value>children</value><value> returns a list generator, so we need to call the </value><value>list</value><value>function on it.</value><value>As you can see above, there are two tags here, </value><value>head</value><value>, and </value><value>body</value><value>. We want to extract the text inside the </value><value>p</value><value> tag, so we’ll dive into the body(Refer to just above, under html.children).</value><value>Now, we can get the </value><value>p</value><value> tag by finding the children of the body tag</value><value>we can use the </value><value>get_text</value><value> method to extract all of the text inside the tag.</value><value>What we did above was useful for figuring out how to navigate a page, but it took a lot of commands to do something fairly simple. If we want to extract a single tag, we can instead use the </value><value>find_all</value><value> method, which will find all the instances of a tag on a page.</value><value>If you instead only want to find the first instance of a tag, you can use the </value><value>find</value><value>method, which will return a single </value><value>BeautifulSoup</value><value> object.</value><value>If you want to fork this notebook go to </value><value>Web Scraping Tutorial.</value><value>Now, I’ll show you how to perform web scraping using </value><value>Python 3</value><value> and the </value><value>BeautifulSoup</value><value> library. We’ll be scraping weather forecasts from the </value><value>National Weather Service</value><value>, and then analyzing them using the </value><value>Pandas</value><value> library.</value><value>We now know enough to proceed with extracting information about the local weather from the National Weather Service website. The first step is to find the page we want to scrape. We’ll extract weather information about downtown San Francisco from </value><value>this page</value><value>.</value><value>Once you open this page then use </value><value>CRTL+SHIFT+I </value><value>to inspect the element, but here we are interested in this particular column (San Francisco CA).</value><value>So, by right-clicking on the page near where it says “Extended Forecast”, then clicking “Inspect”, we’ll open up the tag that contains the text “Extended Forecast” in the elements panel.</value><value>We can then scroll up in the elements panel to find the “outermost” element that contains all of the text that corresponds to the extended forecasts. In this case, it’s a </value><value>div</value><value> tag with the id </value><value>seven-day-forecast.</value><value>Explore the div, you’ll discover that each forecast item (like “Tonight”, “Thursday”, and “Thursday Night”) is contained in a </value><value>div</value><value>with the class </value><value>tombstone-container</value><value>.</value><value>We now know enough to download the page and start parsing it. In the below code, we:</value><value>Extract and print the first forecast item</value><value>As you can see, inside the forecast item </value><value>tonight</value><value> is all the information we want. There are </value><value>4</value><value> pieces of information we can extract:</value><value>Now that we know how to extract each individual piece of information, we can combine our knowledge with CSS selectors and list comprehensions to extract everything at once.</value><value>In the below code</value><value>:</value><value>Select all items with the class </value><value>period-name</value><value> inside an item with the class </value><value>tombstone-container</value><value> in </value><value>seven_day</value><value>.</value><value>Use a list comprehension to call the </value><value>get_text</value><value> method on each </value><value>BeautifulSoup</value><value>object.</value><value>Combining our data into Pandas DataFrame</value><value>We can use a regular expression and the </value><value>Series.str.extract</value><value> method to pull out the numeric temperature values.</value><value>If you want to fork this notebook go to </value><value>Web Scraping</value><value> and </value><value>GitHub</value><value>I hope now you have a good understanding of how to Scrape the data from web pages. In the coming weeks, I’ll do web scraping on</value><value>Hope you like this article!! Don’t forget to like this blog and share with others.</value><value>Thank You</value><value>Go Subscribe </value><value>THEMENYOUWANTTOBE</value><value>Show Some Love ❤</value><value>Written by</value></article></item>
<item><title>Webscrape with Java, NodeJs &amp; Python</title><article><value>So you need to extract data from a webpage into your application? How do you do it? Simple! Its called Webscaping and here’s how it's done.</value><value>Web scraping</value><value>, </value><value>web harvesting</value><value>, or </value><value>web data extraction</value><value> is </value><value>data scraping</value><value> used for </value><value>extracting data</value><value> from </value><value>websites</value><value>.</value><value>Webscraping software may access the World Wide Web directly using the </value><value>Hypertext Transfer Protocol</value><value> or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a </value><value>bot</value><value> or </value><value>web crawler</value><value>. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local </value><value>database</value><value> or spreadsheet, for later </value><value>retrieval</value><value> or </value><value>analysis</value><value>.</value><value>Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is the main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be </value><value>parsed</value><value>, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies, and their URLs, to a list (contact scraping).</value><value>There are, however, some web scraping software that will automatically load and extract data from multiple pages of websites based on your requirements. It is either custom-built for a specific website or is one that can be configured to work with any website. With the click of a button, you can easily save the data available on the website to a file on your computer.</value><value>Many services offer web scraping like </value><value>Scrapestorm Jp</value><value>, </value><value>Grepsr</value><value>, and </value><value>ScrapingHub</value><value>. But today, I will be discussing how to build your own web scraper application using Java, NodeJs and Python.</value><value>The best library to use for Java webscraping is </value><value>Jsoup</value><value>.</value><value>jsoup</value><value> is a Java library for working with real-world HTML. It provides a very convenient API for extracting and manipulating data, using the best of DOM, CSS, and jquery-like methods.</value><value>jsoup</value><value> implements the </value><value>WHATWG HTML5</value><value> specification, and parses HTML to the same DOM as modern browsers do.</value><value>jsoup is designed to deal with all varieties of HTML found in the wild, from pristine and validating, to invalid tag-soup; jsoup will create a sensible parse tree.</value><value>Download the Jsoup JAR file from </value><value>here</value><value> and then create a java class containing the URL that you need to scrape:</value><value>After running the java class, the webpage data should be printed out. This is the most basic way of webscraping in Java. Of course, this does not separate the data; many functions need to be placed for the application to do so. To create a more elaborate webscraping application follow </value><value>this</value><value>.</value><value>By using the superb tutorial </value><value>here</value><value>, </value><value>we create a new </value><value>scraper</value><value> directory for this tutorial and initialize it with a </value><value>package.json</value><value> file by running </value><value>npm init -y</value><value> from the project root. Then run this command to install all the dependencies needed:</value><value>Here’s what each one does:</value><value>When the installation is complete, create a new </value><value>pl-scraper.js</value><value> file in the root of your project directory and populate it with the following code:</value><value>If you run the code with </value><value>node</value><value> pl-scraper.js</value><value>, a long string of HTML will be printed to the console.</value><value>And that’s it, you just retrieved all the data from a webpage using a NodeJs webscraper. But how can you parse the HTML for the exact data you need? Continue following </value><value>Pusher</value><value>’s tutorial </value><value>here</value><value>.</value><value>With reference to Python Docs found </value><value>here</value><value>, </value><value>we start off by downloading </value><value>lxml</value><value> that is a pretty extensive library written for parsing XML and HTML documents very quickly, even handling messed up tags in the process. We will also be using the </value><value>Requests</value><value> module instead of the already built-in urllib2 module due to improvements in speed and readability. You can easily install both using </value><value>pip</value><value> </value><value>install</value><value> lxml</value><value> and </value><value>pip</value><value> </value><value>install</value><value> requests</value><value>.</value><value>Let’s start with the imports:</value><value>Next, we will use </value><value>requests.get</value><value> to retrieve the web page with our data, parse it using the </value><value>html</value><value> module, and save the results in </value><value>tree</value><value>:</value><value>(We need to use </value><value>page.content</value><value> rather than </value><value>page.text</value><value> because </value><value>html.fromstring</value><value> implicitly expects </value><value>bytes</value><value> as input.)</value><value>tree</value><value> now contains the whole HTML file in a nice tree structure which we can go over two different ways: XPath and CSSSelect. In this example, we will focus on the former.</value><value>XPath is a way of locating information in structured documents such as HTML or XML documents. A good introduction to XPath is on W3Schools. There are also various tools for obtaining the XPath of elements such as FireBug for Firefox or the Chrome Inspector. If you’re using Chrome, you can right-click an element, choose ‘Inspect element’, highlight the code, right-click again, and choose ‘Copy XPath’.</value><value>After a quick analysis, we see that in our page the data is contained in two elements — one is a div with title ‘buyer-name’ and the other is a span with class ‘item-price’:</value><value>Knowing this we can create the correct XPath query and use the lxml </value><value>xpath</value><value> function like this:</value><value>Let’s see what we got exactly:</value><value>Congratulations! We have successfully scraped all the data we wanted from a web page using lxml and Requests. We have it stored in memory as two lists. Now we can do all sorts of cool stuff with it: we can analyze it using Python, or we can save it to a file and share it with the world.</value><value>So is it legal or illegal? Web scraping and crawling aren’t illegal by themselves. After all, you could scrape or crawl your own website, without a hitch…</value><value>In 2016, the US Congress passed its first legislation specifically to target bad bots — the </value><value>Better Online Ticket Sales (BOTS) Act</value><value>, which bans the use of software that circumvents security measures on ticket seller websites. Automated ticket scalping bots use several techniques to do their dirty work including web scraping that incorporates advanced business logic to identify scalping opportunities, input purchase details into shopping carts, and even resell inventory on secondary markets.</value><value>In other words, if you’re a venue, organization or ticketing software platform, it is still on you to defend against this fraudulent activity during your major on sales. But of course, this depends on where in the world you are:</value><value>The UK however, seems to have followed the US with its </value><value>Digital Economy Act 2017</value><value> which achieved Royal Assent in April. The Act seeks to protect consumers in a number of ways in an increasingly digital society, including by “cracking down on ticket touts by making it a criminal offence for those that misuse bot technology to sweep up tickets and sell them at inflated prices in the secondary market.”</value><value>You can read more about this </value><value>here</value><value>.</value><value>To put that into perspective, companies themselves have the responsibility of protecting their own data from web scrapers as they have to invoke the law themselves. So before you go off and try to web scrape from a .gov webpage with your python program, think again!</value><value>Businesses</value><value> use web scraping for different purposes and it varies on a case to case basis.</value><value>In </value><value>eCommerce</value><value>, Retailers/ marketplaces use web scraping to monitor their competitor prices and to improve their product attributes. Also, collect product reviews to do sentimental analysis. </value><value>Lawyers</value><value> use web scraping to see the past judgment report for their case reference. </value><value>Lead generation</value><value> companies use it to scrape the email address and phone numbers. </value><value>Recruiters</value><value> use it to collects user's profiles. Some </value><value>travel companies</value><value> collect data in real-time to provide live tracking details. </value><value>Media companies</value><value> collect trending topics and use hashtags to collect information from social media profiles. </value><value>Business directories</value><value> scrape complete information about the business profile, address, email, phone, products/services, working hours, Geocodes, etc.</value><value>Each business has competition in the present world, So companies scrape their competitor information regularly to monitor the movements. </value><value>Government</value><value> secret agencies also scrape for national securities purpose.</value><value>It's safe to say that webscaping is a big field, and you have just finished a brief tour of that field, using Java, NodeJs, and Python as your guide. You have also learned that it is illegal to scrape some sites, and you should check their terms and conditions before scraping. So do your webscraping wisely!</value><value>Still worried about implementing applications, API’s or backends? Oracle is here to help, with industry-standard cloud applications, their team of experts will make implementation more than enjoyable.</value><value>Thank you for taking the time to read my article, if you’re looking for more posts like this, you can find me on </value><value>Linkedin</value><value>, </value><value>Twitter</value><value>, or </value><value>Medium</value><value>.</value><value>Written by</value></article></item>
<item><title>Scalable do-it-yourself scraping — How to build and run scrapers on a large scale</title><article><value>Businesses that don’t rely on data have a very low chance of success in a data driven world.</value><value>One of the best sources of data is the data available publicly online on various websites and to get this data you have to employ the technique called Web Scraping or Data Scraping.</value><value>You can use full service professionals such </value><value>ScrapeHero </value><value>to do all this for you or if you feel brave enough, you can tackle this yourself.</value><value>The purpose of this article is to walk you through some of the things you need to do and the issues you need to be cognizant of when you do decide to do it yourself.</value><value>When you decide to do this yourself, you will most likely be hiring a few developers who know how to build scrapers and setting up some servers and related infrastructure to run these scrapers without interruption, and integrating the data you extract into your business process.</value><value>Building and maintaining a large number of web scrapers is a very complex process so proceed with caution.</value><value>Here are the high level steps involved in this process and we will go through each of these in detail in this article.</value><value>The first thing to do is build the scrapers.</value><value>It may be best to choose an open-source framework for building your scrapers, like Scrapy or PySpider. These are excellent frameworks with a large community of developers. Both these frameworks are based on Python. You won’t run into the risk of your developer(s) disappearing in a day, and no one to maintain your scrapers because Python is popular and the community is really supportive.</value><value>There is also a massive difference between writing and running one scraper that scrapes 100 pages to a large scale distributed scraping infrastructure that can scrape thousands of websites and millions of pages a day.</value><value>If you are scraping a large number of big websites, you might need lot of servers to get the data in a reasonable time frame. We would suggest using Scrapy Redis or Run PySpider in scaled mode, across multiple servers.</value><value>Once you have chosen a framework, hire some good developers to build these scrapers, and set up the servers required to run them and to store the data.</value><value>If you need the data to be refreshed periodically, you’ll either have to </value><value>run it manually or automate</value><value> it using some tool or process.</value><value>If you are using Scrapy,scrapyd + cron can schedule the spiders for you, and it will update the data the way you need it. PySpider also has a UI to do that</value><value>Once you have this massive data trove, you need a place to store it. We would suggest using a NoSQL database like MongoDB, Cassandra or HBase to store this data, depending upon the frequency and speed of scraping.</value><value>You can then extract this data from this database/datastore and integrate it with your business process. But before you do that, you should setup some Quality Assurance tests for your data (more on that later)</value><value>Large scale scraping comes with a multitude of problems and one of the big ones is anti-scraping measures by the websites that you are trying to scrape.</value><value>If any of the target websites has any kind of </value><value>IP based blocking</value><value> involved, your servers’ IP address will be black listed in no time and the site won’t respond to requests from your servers. You’ll be left with very few options after getting blacklisted.</value><value>So, how do you bypass that? You’ll have to get some </value><value>Proxies or Rotating IP solutions</value><value> to use these for making requests from the scraper.</value><value>Here are few tips to prevent getting blacklisted</value><value>The data you scrape is only as good as its quality. To ensure the data that you scraped in accurate and complete, you need to run a variety of QA tests on it right after it is scraped.</value><value>Having a set of </value><value>Tests </value><value>for the integrity of the data is essential. Some of it can be automated by using Regular Expressions, to check if the data follows a predefined pattern and if it doesn’t then generate some alerts so that it can be manually inspected.</value><value>Every website will change</value><value> their structure now and then, and so should your scrapers. Scrapers usually need adjustments every few weeks, as a minor change in the target website affecting the fields you scrape might either give you incomplete data or even crash the scraper, depending on the logic of the scraper.</value><value>You have to be smart and detect this change and fix it before this ruins the data you are collecting.</value><value>Depending upon the size of data, you will have to clean up your database of outdated data to save space and money. You might also have to scale up your systems if you still need the old data. Sharding and Replication of databases can be of help.</value><value>Server logs should also be cleaned periodically.</value><value>This whole process is expensive and time consuming and you need to be ready to take on this challenge.</value><value>You also need to know when to stop and ask for help. </value><value>ScrapeHero </value><value>has been doing all this and more for many years now, so let us know if you need any help.</value><value>Originally published at </value><value>learn.scrapehero.com</value><value> on December 1, 2015.</value><value>Written by</value></article></item>
<item><title>How to do web scraping with Cheerio</title><article><value>This past weekend (13 August 2017) I started on a quest to get some data from a cinema website here in Accra, Ghana. I thought this would have been easy, since the data is available publicly. I immediately opened the Chrome web inspector to see some markup like I have not seen in years.</value><value>There was no structure to this data, the listings were just a bunch of </value><value>&lt;p&gt;</value><value> tags with some nested </value><value>&lt;span&gt;</value><value> and </value><value>&lt;br&gt;</value><value> tags inside. This to me was a sign of a no go, I even went on to state that there was no way of getting this data in the </value><value>DevCongress</value><value> (you might be wondering what DevCongress is, more to come soon) slack group, along with a solution I wasn’t too sure would work.</value><value>After a few minutes of thinking it through, I realised there was a pattern even in the </value><value>&lt;p&gt;</value><value> tags, when I did a count I noticed that each movie has around </value><value>12</value><value> nodes of </value><value>&lt;p&gt;</value><value> which contained the data I would need for the movie. So now I could do a loop over the </value><value>&lt;p&gt;</value><value> tags and count down from </value><value>12</value><value>, then reset the counter once we hit </value><value>0</value><value>.</value><value>Just when I finished writing this post, the data I was scraping changed and broke my solution, so I had to go back to the drawing board and come up with a new solution, which I think in turn has worked out to be a better and more robust solution.</value><value>Instead of counting the </value><value>&lt;p&gt;</value><value> tags, I have decided to use the </value><value>&lt;hr&gt;</value><value> tags on the page as the breaking point between each movie, I have also decided to not use the method I was before by counting down from </value><value>12</value><value> to get the movie information. I have instead opted for checking the actual string I am looping over to test if it contains a certain word where possible. In other places I am using some crazy thinking to get the information I need.</value><value>Its now a bit clearer to me as to how to approach the problem, I then decided its time to start writing some code, I was thinking of doing this in Python as I had used </value><value>Beautiful Soup</value><value> in the past to do this sort of thing, but lately I have been doing more work in JavaScript and Node. So I did a quick search and I found </value><value>this article</value><value> using </value><value>Cheerio</value><value> and the Request library, I quickly started writing some code and couldn’t believe how easy the API was to use.</value><value>Lets start by installing the libraries we will need, also note I am using Node 8, so will be using new features of JavaScript where I see fit.</value><value>For this tutorial you will need the following libraries. At the time of writing these are the versions I used.</value><value>Now lets start requiring the libraries we need in order to get some data from the webpage.</value><value>You will notice I am also requiring the </value><value>fs</value><value> library, we are doing this so that later on we don’t hit the API more times than necessary, we can cache the data and easily read from cache and do our scraping from that data.</value><value>Now lets define a few variables to store the URL of the website we want to scrape and the name of the cache files.</value><value>We can now start defining our data structure that we want to deliver to our end user.</value><value>Here we have defined the properties our output data will conform to, so in the </value><value>movieListings</value><value> structure, we are currently only storing the </value><value>address</value><value> of the cinema and a list of </value><value>movies</value><value>. While in the </value><value>newMovieObj</value><value> we are storing all the attributes of the movie that we need.</value><value>Lets start writing our code to make a request to get the </value><value>apiUrl</value><value> and then cache it to the file system using the </value><value>fs</value><value> library. We will start off by wrapping the function so we can reuse it later on.</value><value>Lets look at some of this code, we start off by defining our function called </value><value>requestPage</value><value>, which requires two parameters, one for the </value><value>url</value><value> we are making the request to and another for the </value><value>cachePath</value><value> we wish to save the response data to. We know what we are requesting is html so we will save it as html as defined in the </value><value>cacheFile</value><value> variable we set earlier. We call the </value><value>request</value><value> library with the </value><value>url</value><value> and a callback function with the parameters of </value><value>err, response, html</value><value>, with these we can determine the state of the request we’ve made. If there is an error, we just log it to the console for now, otherwise we can move on to starting to write to the filesystem. We now have some data, so lets move on to writing it to the filesystem for now with </value><value>fs.writeFile</value><value>, in this we will also check for error and log them to the console again.</value><value>Now that we have our function to request and write data to the filesystem, lets move on to reading the cache file we saved.</value><value>We start by checking if the </value><value>cacheFile</value><value> exists, if it doesn’t we send a request and create one, otherwise we just read it using the </value><value>fs.readFile</value><value> function.</value><value>Inside our </value><value>fs.readFile</value><value> callback, lets start loading up the data (which we know is an html page) into cheerio so we can crawl the DOM (Document Object Model) and select the data we need.</value><value>Lets take a look at this line by line.</value><value>You might be wondering why are we assigning a </value><value>$</value><value> variable to the loading of the DOM data, we are using the </value><value>$</value><value> for no specific reason, except that its what jQuery uses and it became universal amongst most developers to represent the DOM.</value><value>We assign the </value><value>numLines</value><value> variable to </value><value>10</value><value>, because this is what we will use to figure out where our movie title is. So each time the </value><value>numLines</value><value> is reduced to </value><value>10</value><value> we know its the node of a movie title.</value><value>The </value><value>movie</value><value> variable is assigned to a new copy of the </value><value>newMovieObj</value><value> to get all the properties in that object.</value><value>This </value><value>synopsisNext</value><value> variable is to make sure that we know when the synopsis information is coming up, since the actual information and the title word </value><value>SYNOPSIS</value><value> are stored in different </value><value>&lt;p&gt;</value><value> tags.</value><value>The code above is plenty, but lets break it down as to what each part is doing.</value><value>We will start off on line 1 which loops through all the </value><value>p</value><value> tags inside of a </value><value>div</value><value> with the </value><value>id</value><value> of </value><value>content</value><value>.</value><value>On line 2 and 3 we are assigning the text of each </value><value>p</value><value> tag into a variable called </value><value>text</value><value> and a variable called </value><value>html</value><value>.</value><value>From line 4 we are then checking if the current </value><value>p</value><value> tag is situated in the first 3, as we have figured this is where the address for the cinema is located. We then append that text to the </value><value>address</value><value> property of the </value><value>movieListings</value><value> object. At this point we do some cleanup on the text with the </value><value>replace</value><value> string method.</value><value>Next we can see that the actual movie listings start from line 9 onwards, this is because we know that after 12 </value><value>p</value><value> tags we have the movie listings starting.</value><value>On line 10 to 16, we check if </value><value>html</value><value> is empty and reset </value><value>numLines</value><value> to 11, you might be wondering why 11 instead of 10, this is because we have to offset by 1 in order to get any subsequent title after the first time, now we add the current </value><value>movie</value><value> to the </value><value>movieListings.movies</value><value>. We then move on to creating a new </value><value>movie</value><value> object to make sure that our next loop is not updating an existing movie reference.</value><value>On line 19 to 56, we use multiple </value><value>if</value><value> statements to decide which piece of movie information we are currently accessing. Here you will notice we are using different methods to check the data against. When we find the information we need, we are doing some manipulation and cleanup in order to create a format we are happy with. In this particular area we created a helper function to get the showtimes, by check a string to see if it contains any of the days in the week. That helper function is the </value><value>checkDaysOfWeek</value><value> function, which looks like the below.</value><value>The rest of the code below is just working out how best to find a particular piece of movie information.</value><value>Once we hit line 57, we reduce by 1 the </value><value>numLines</value><value> left.</value><value>You can view the full source code and working copy on </value><value>Glitch</value><value>.</value><value>And this is how I went about scraping the movie data I needed from the cinema website. In the code there are a lot of places that can be refactored and simplified. I might write another post on refactoring the current codebase.</value><value>Thanks to </value><value>Wendy Smith</value><value>, </value><value>Edmond Mensah</value><value>, </value><value>Emmanuel Lartey</value><value> and </value><value>David Oddoye</value><value> for reviewing this post and giving feedback to improve it. If you need Front-end/NodeJS/PHP development done, please visit </value><value>https://www.donielsmith.com</value><value> and check out some of my work. Feel free to get in-touch with me on Twitter </value><value>@silentworks</value><value> with questions.</value><value>Originally published at </value><value>www.donielsmith.com</value><value> on August 29, 2017.</value><value>Written by</value></article></item>
<item><title>ReactJS Examples</title><article><value>I’m writing some ReactJS examples to demonstrate how we use React as the view rendering library. It also shows how we do data-view separation.</value><value>You can find the examples in the following CodePen collection:</value><value>http://codepen.io/collection/XwaeGM/</value><value>Written by</value></article></item>
<item><title>How to scrape websites with Python and BeautifulSoup</title><article><value>What do you do when you can’t download a website’s information? You do it by hand? Wow, you’re brave!</value><value>I’m a web developer, so I’m way too lazy to do things manually 🙂</value><value>If you’re about to scrape data for the first time, go ahead and read </value><value>How To Scrape A Website</value><value>. You can also read a small intro about </value><value>web scraping</value><value>.</value><value>Today, let’s say that you need to enrich your CRM with company data.</value><value>To make it interesting for you, we will scrape </value><value>Angel List</value><value>.</value><value>More specifically, we’ll scrape </value><value>Uber’s company profile</value><value>.</value><value>Please scrape responsibly!</value><value>Before starting to code, be sure to have </value><value>Python 3</value><value> installed, as we won’t cover it here. Chances are you already have it installed.</value><value>You also need </value><value>pip</value><value>, a package management tool for Python.</value><value>The full code and dependencies are </value><value>available here</value><value>.</value><value>We’ll be using BeautifulSoup, a standard Python scraping library.</value><value>You could also create a </value><value>virtual environment</value><value> and install all the dependencies inside the requirements.txt file:</value><value>Open </value><value>https://angel.co/uber</value><value> in your web browser (I recommend using Chrome).</value><value>Right-click and open your browser’s inspector.</value><value>Hover your cursor on the description.</value><value>This example is pretty straightforward: you want the </value><value>&lt;h2&gt;</value><value> tag with the </value><value>js-startup_high_concept</value><value> class.</value><value>This would be the unique location of our data thanks to the </value><value>class</value><value> tags.</value><value>Let’s dive right in with a bit of code:</value><value>Let’s get into the details:</value><value>Save this as </value><value>script.py</value><value> and run it in your shell, like this </value><value>python script.py</value><value>.</value><value>You should get the following:</value><value>Oh 🙁 What happened?</value><value>Well, it seems that AngelList has detected that we are a bot. Clever people!</value><value>Okay, so change the </value><value>headers</value><value> variable for this one:</value><value>Run the code with </value><value>python script.py</value><value>. Now it should be good:</value><value>Yeah! Our first piece of data 😀</value><value>Want to find the website? Easy:</value><value>And you get:</value><value>Ok, but how do I get the </value><value>value</value><value> of the website?</value><value>Easy. Tell the program to extract the </value><value>href</value><value>:</value><value>Make sure to use the </value><value>strip()</value><value> method, otherwise you’ll have big spaces:</value><value>I won’t cover in detail all the elements you could extract. If you’re having issues, you can always check </value><value>this amazing XPath cheatsheet</value><value>.</value><value>Pretty useless to print data, right? We should definitely save it!</value><value>The Comma-Separated Values format is really a standard for this purpose. You can import it very easily in Excel or Google Sheets.</value><value>Add the following lines:</value><value>What you get is a single line of data. Since we told the program to append every result, new lines won’t erase previous results.</value><value>The script is </value><value>available here</value><value>.</value><value>It wasn’t that hard, right?</value><value>We covered a very basic example. You could also add multiple pages and parse them inside a for loop.</value><value>Remember how we got blocked by the website’s security and resolved this by adding a custom User-Agent? We wrote a small paper about </value><value>anti-scraping</value><value> techniques. It’ll help you understand how websites try to block bots.</value><value>If you feel like web scraping is too difficult for you or you’re getting blocked, you can always </value><value>contact us</value><value>!</value><value>You can also use a more advanced version of this script on our platform</value><value>.</value><value>Originally published at </value><value>captaindata.co</value><value> on November 8, 2018.</value><value>Written by</value></article></item>
<item><title>How to Scrape Data from Web Pages for Sentiment Analysis?</title><article><value>Today, Businesses can understand their customers’ reactions with the help of many available tools. They can analyze if the customers have liked the layout or not, get the existing offers, did the services please them? The increased data volume is valuable to evaluate success as well as draw insights about the future.</value><value>At </value><value>X-Byte Enterprise Crawling</value><value>, We are a Data-as-a-Service provider, so we understand the importance of this data as well as help you get valuable insights through our Data Scraping Services. We Extract Websites and Scrape Structured Data that can be utilized to derive some insights. We provide the </value><value>best webpage data scraping</value><value> for sentiment analysis services to help your business do better with real time sentiment analysis of social media platform data.</value><value>Being a </value><value>web scraping service</value><value> provider, we make that easier to scrape data from the web. With our professional webpage data scraping services for sentiment analysis, you just need to provide us the websites list that you want to scrape for sentiment analysis with the required fields as well as the frequency that you wish the data to. With our personalized crawlers as well as progressive computing stacks, we have retrieved the data in a format you want (generally JSON, CSV, XML,). You can ask for the data through our API or even get the data provided to your AWS or FTP location.</value><value>As the data scraping is really challenging, we do replicate on how the opinion mining could help our business enterprise clients do better. Sentiment Analysis or Opinion Mining copes with automatic data scanning as well as establishing its purpose or nature. Basically, it is very important to define if the text extracted and scraped from the website is helpful or not; or whether it associates with the subject which is given in the title.</value><value>The functions of Sentiment Analysis of Twitter or Sentiment Analysis of Facebook could be to analyze records (product feedback, user reviews, services feedback forms, etc.) as well as specify feelings expressed (dissatisfaction, happiness, etc.). On the easy scale, it can be attained by creating a rating system from 1–10 where every word is usually associated with emotions. The scores of every word, as well as the entire text, is calculated to observe what the sentiments or opinions are indicated.</value><value>The added methodology is objectivity or subjectivity identification. Here, scraped data is verified for being objective or subjective. Though, this might prove to be tough as results of assessments are person-specific.</value><value>Maybe the most advanced type is Feature-Based Sentiment Analysis. Here, individuals give opinions about users that are scraped from the text about a definite service or product and then evaluate it to see if a consumer gets satisfied or not. That is where X-Byte Enterprise Crawling’s </value><value>Web Data Scraping Services</value><value> help. For instance, if you want to crawl hundreds and thousands of news, blogs, or forum websites to scrape high-level data like date, title, article URLs, content, and author, mass-scale crawls, etc. will offer the data in a well-structured format like constant feeds.</value><value>We could also filter these crawls based on a list of keywords to facilitate better sentiment analysis based on subject topic, language, and even keyword detection. Our named-entity recognition service only helps to enrich this information.</value><value>We help our clients with product sentiment analysis. The customer wanted to scrape comments about that from websites and forums, from distributors, retailers, and enthusiasts to an average customer. The customer’s use case was to get data to know how promising users found the product as well as what consumers have talked about that on the Internet.</value><value>Considering there are thousands of websites that might comprise product reviews as well as different online forums based on the consumer durables or associated topics, you get a valued collection of understandings. We set crawls to scrape reviews from highly valued websites with thousands of URLs spontaneously.</value><value>Our automated data scraping and </value><value>Monitoring Solutions</value><value> target sites as well as deliver exact results. Furthermore, with place normalization, we deliver analysis-ready well-structured data.</value><value>To get professional web data scraping for Sentiment Analysis, contact </value><value>X-Byte Enterprise Crawling</value><value> </value><value>or ask for a free quote!</value><value>Visit Us:</value><value> </value><value>www.xbyte.io</value><value>Written by</value></article></item>
<item><title>Web Crawling? eh.. What is it?</title><article><value>Hey folks! This post is for all those who have always wondered what web crawling is, how do you do it but have never been able to understand it. Lo and behold! your search stops here..:) In layman’s terms, Web crawling is the art of extracting vast amounts of </value><value>information</value><value> from the world wide web.</value><value>Hey so what.. what’s so different about it? Extracting data was done years ago(large data sheets, hand-written bank records, all people staying at a hotel, etc). Ahem!.. wait! Imagine the online register at the entrance of a hotel that was built a few years ago. Now, imagine one million such registers containing the exact same information of people who have visited the hotel since that time. You are the new supervisor and are supposed to draw out the details of all those people. That’s tough! right? A hard working guy would take a paper and a pen and manually do all the hard work. A smart guy would write a code that automatically does this for him. That’s one kind of web scraping and crawling.</value><value>Web crawlers</value><value> are also known as </value><value>spiders</value><value> because of the their very nature to walk through the </value><value>world wide</value><value> </value><value>web</value><value>. Yes, they are the soul of online search engines that help you with relevant pages in barely a fraction of a second! How? These silent warriors pack their tools and tricks up their sleeves and go around the world downloading bulk of information from the online document store(i.e. WWW). Just to generate more interest among you guys I am sharing another link </value><value>here</value><value>. Just read it and think about it. If you want to further try out something, then </value><value>y</value><value>ou can get your </value><value>feet wet here</value><value>.</value><value>Originally published at </value><value>sidlearnstocrawl.wordpress.com</value><value> on April 17, 2015.</value><value>Written by</value></article></item>
<item><title>Learn web app development while solving a real world problem</title><article><value>In this series, we will learn creating a web application from scratch, web scraping and storing the scraped data. All this while solving a real world problem.</value><value>This is going to be a fun practical series divided into 3 part:</value><value>1. Scrape the data</value><value>2. Store it</value><value>3. Create a web application</value><value>Note: I am not sure if web scraping is illegal or not. It’s a complex topic to discuss. This series does not discuss the legal aspects of web scraping. However, I believe web scraping done ethically (debatable what is ethical) should not be a problem for the websites being scrapped.</value><value>Assume that you are living in the USA and want to send some money to your friend or family in India. You would first google USD to INR rate, then you look for a money transfer service that allows you to send money from USD to INR. But, there are a lots of different services that provide different exchange rates, different service charges. First, you collect a list of such money transfer services and then you visit their websites to check what is the rate that they are providing. This takes a lot of time and effort.</value><value>In this tutorial we are going solve this problem by creating a web-application that will show the exchange rates provided by these services at one place only. So that, you have to open only one website to decide which service to use.</value><value>Lets solve the problem by breaking it into three parts:</value><value>This article is going to cover the #1.</value><value>Sounds interesting?</value><value>We have a lot to cover, so without wasting a moment, let’s get started.</value><value>1.</value><value> Python (web scraping)</value><value>2. </value><value>BeautifulSoup (Python library for webscraping)</value><value>3. </value><value>urllib.request (Python library for opening URLs)</value><value>In this part, we will cover the web scraping section.</value><value>In a typical client server scenario, client (eg. web browsers) sends a request to server. Server responds with data. For eg. When we open google.com in any web browser, the browser sends the request to google’s server to get the google search page. The google server returns the data in HTML and then browser renders the HTML and display beautiful UI to the user. The same thing happens when we open any other website. Web-Scraping is to read the HTML and get the required data/information from that HTML.</value><value>Lets understand this while solving our problem at hand. Follow the below steps:</value><value>As we are going to create an application where users can view the USD to INR exchange rate offered by various services that lets users send money from USA to India. The obvious requirement is the list of such services. We are going to use Remitly and Transferwise (randomly selected).</value><value>We learned in the basics that all websites are in HTML(Hyper Text Markup Language). Underlying HTML of the website opened in commonly used browsers (chrome, safari, edge, firefox) can be easily seen by right clicking on the page and selecting </value><value>Inspect</value><value> option.</value><value>We want to get the USD to INR, so open the first website (</value><value>Remitly</value><value>), right click on the place where it shows INR rate that you would like to scrape. In the developer console that gets opened, right click on the selected element and copy -&gt; Copy Selector.</value><value>The </value><value>selector</value><value> is copied to the clipboard, save it for now (!important), we will use it later. Similarly, open the second website (</value><value>Tranferwise</value><value>) and do the same.</value><value>Now we have the selectors (which is the CSS path to the element that displays the INR rate in the DOM). We will write Python script to </value><value>programmatically</value><value> make request (using urllib) to Remitly and Tranferwise web pages and read the HTML response (using BeautifulSoup library) and extract the INR rate using the selectors (obtained in previous step).</value><value>The above script contains all the comments to explain what each line is doing. If it needs more explanation, let me know in comments section :). In short, the above script is performing the below steps:</value><value>1. Call the page(url) that shows the exchange rate from USD to INR.</value><value>2. Get the HTML.</value><value>3. Create a BeautifulSoup object to navigate the html easily.</value><value>4. Extract the rate using the selectors we got in step2.</value><value>Copy the above script in a file and save it with </value><value>.py</value><value> extension (e.g. scrapper.py).</value><value>Run the script by executing below command in terminal/cmd (python3 should be installed already)</value><value>Now, we have the nice script that scrapes the exchange rate from two money exchange services (Remitly &amp; Transferwise). This script can be easily extended to include more services without much changes in the code. Simply create new class for a new service and include the name of the service in the MONEY_TRANSER_SERVICES array. That’s it.</value><value>In this part, we saw how to extract the information from a web-page. In the next part we will see how to structure the data and store in MongoDB for long term storage. Stay tuned!</value><value>Written by</value></article></item>
<item><title>Need to know about the scrapping a car</title><article><value>Selling a car can be such a pain if you do not explore the web and not find out what to do with the car, how to sell it and what price to demand the car itself.</value><value>Written by</value></article></item>
<item><title>How to get the next page on Beautiful Soup</title><article><value>It is easy to scrape a simple page, but how do we get the next page on Beautiful Soup? What can we do to crawl all the pages until we reach the end?</value><value>Today, we are going to learn how to fetch all the items while Web Scraping by reaching to the next pages.</value><value>As the topic of this post is what to do to crawl next pages, instead of coding a Beautiful Soup script again, we are going to take the one we did previously.</value><value>If you are a beginner, please, do the ‘</value><value>Your first Web Scraping script with Python and Beautiful Soup</value><value>‘ tutorial first.</value><value>If you know how to use Beautiful Soup, use this starting code in </value><value>repl.it.</value><value>This code fetches us the albums from the band the user asks for. All of them? No, just the first 10 ones that are displayed on the first page. By now.</value><value>Open a new repl.it file or copy-paste the code in your code editor: Now it’s time to code!</value><value>Before adding features, we need to clean the clutter by refactoring.</value><value>We are going to take blocks of code and placing them in their own functions, then calling that functions where the code was.</value><value>Go to the end of the code and take the lines where we create the table:</value><value>Cut them and create a function, for example, export_table_and_print, and put it after base_url and search_url:</value><value>We also added a ‘clean_band_name’ so the filename where we store the data doesn’t have empty spaces and it is all lowercase, so “ThE BeAtLES” search stores a ‘the_beatles_albums.csv’ file.</value><value>Now, where the old code was, call the function, just at the end of the file:</value><value>The first part is done. Run the code and check it is still working.</value><value>Go to the ‘for loop’ at around line 45. Take everything that involves in extracting values and adding them to ‘data’ (so, the whole code) and replace it with the ‘get_cd_attributes(cd)’.</value><value>After the last function, create that function and paste the code:</value><value>Again, run the code and check it is still working. If it is not, compare your code with mine:</value><value>t is working? Cool. Time to get ALL the albums!</value><value>Ok, here’s the trick to get the job done: Recursiveness.</value><value>We are going to create a “parse_page’ function. That function will fetch the 10 albums the page will have.</value><value>After the function it is done, it is going to call itself again, with the next page, to parse it, over and over again until we have everything.</value><value>Let me simplify it for you:</value><value>I hope it is clear: As we keep having a ‘next page’ to parse, we are going to call the same function again and again to fetch all the data. When there is no more, we stop. As simple as that.</value><value>Grab this code, create another function called ‘parse_page(url)’ and call that function at the last line.</value><value>The data object is going to be used in different places, take it out and put it after the search_url.</value><value>We took the main code and created a parse_page function, called it using the ‘search_url’ as parameter and took the ‘data’ object out so we can use it globally.</value><value>In case you are dizzy, here’s what your code should look like now:</value><value>Please check this line:</value><value>Now we are not fetching the ‘search_url’ (the first one) but the URL that we pass as an argument. This is very important.</value><value>Run the code again. It should fetch the 10 first albums as always.</value><value>That’s why because we haven’t used recursion. Let’s write the code that will:</value><value>Once we have fetched all the cd attributes (that’s it, after the ‘for cd in list_all_cd’ loop), add this line:</value><value>We are getting all the ‘list item’ (or ‘li’) elements inside the ‘unordered list’ with the ‘SearchBreadcrumbs’ class. That’s the pagination list.</value><value>Then, we go to the last one and get the text. Add this after the last code:</value><value>Now we check if ‘next_page_text’ has ‘Next’ as text. If it does, we take the partial url, we add it to the base to build the next_page_url. If it does not, there is no more pages, so we can create the file and print it.</value><value>That’s all we need. Run the code, and now you are getting dozens, if not hundreds of items!</value><value>But we can still improve the code. Add this 4 lines after parsing the page with Beautiful Soup:</value><value>Sometimes there is a ‘Next’ page when the numbers of albums are multiple of 10 (10, 20, 30, 40 and so on) but there is no album there. That makes the code to end without creating the file.</value><value>With this code, it is fixed.</value><value>Your coding is done! Congratulations!</value><value>Let me summarize what we have done:</value><value>Now it seems simpler, right?</value><value>I want to keep doing tutorials like this one, but I want to ask you what do you want to see:</value><value>Please, leave me a comment with what do you want to see in future posts.</value><value>And if this tutorial has been useful to you, share it with your friends, on Twitter, Facebook or where you can help others.</value><value>Final code on Repl.it</value><value>Reach to me on Twitter</value><value>My Youtube tutorial videos</value><value>My Github</value><value>Contact me: DavidMM1707@gmail.com</value><value>Keep reading </value><value>more tutorials</value><value>Written by</value></article></item>
<item><title>How Xpath Plays Vital Role In Web Scraping Part 2</title><article><value>To read the first part of this blog do read:</value><value>Here is a piece of content on Xpaths which is the follow up of </value><value>How Xpath Plays Vital Role In Web Scraping</value><value>Let’s dive into a real-world example of scraping amazon website for getting information about deals of the day. Deals of the day in amazon can be found at this . So navigate to the (deals of the day) in Firefox and find the XPath selectors. Right click on the deal you like and select “Inspect Element with Firebug”:</value><value>If you observe the image below keenly, there you can find the source of the image(deal) and the name of the deal in src, alt attribute’s respectively. So now let’s write a generic XPath which gathers the name and image source of the product(deal). //img[@role=”img”]/@src ## for image source //img[@role=”img”]/@alt ## for product name</value><value>In this post, I’ll show you some tips we found valuable when using XPath in the trenches.</value><value>If you have an interest in Python and web scraping, you may have already played with the nice </value><value>requests library </value><value>to get the content of pages from the Web. Maybe you have toyed around using </value><value>Scrapy selector </value><value>or to make the content extraction easier. Well, now I’m going to show you some tips I found valuable when using XPath in the trenches and we are going to use both and </value><value>Scrapy selector </value><value>for HTML parsing.</value><value>Avoid using expressions which contains(.//text(), ‘search text’) in your XPath conditions. Use contains(., ‘search text’) instead.</value><value>Here is why: the expression .//text() yields a collection of text elements — a node-set(collection of nodes).and when a node-set is converted to a string, which happens when it is passed as argument to a string function like contains() or starts-with(), results in the text for the first element only.</value><value>Scrapy Code:</value><value>from scrapy import Selector</value><value> html_code = “””&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;”””</value><value> sel = Selector(text=html_code)</value><value> print xp(‘//a//text()’)</value><value> xp = lambda x: sel.xpath(x).extract() # Let’s type this only once # Take a peek at the node-set</value><value> [u’Click here to go to the ‘, u’Next Page’] # output of above command</value><value> print xp(‘string(//a//text())’) # convert it to a string # output of the above command</value><value> [u’Click here to go to the ‘]</value><value>Let’s do the above one by using lxml then you can implement XPath by both lxml or Scrapy selector as XPath expression is same for both methods.</value><value>lxml code:</value><value>from lxml import html </value><value> html_code = “””&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;””” # Parse the text into a tree</value><value> parsed_body = html.fromstring(html_code) # Perform xpaths on the tree</value><value> print parsed_body(‘//a//text()’) # take a peek at the node-set</value><value> [u’Click here to go to the ‘, u’Next Page’] # output</value><value> print parsed_body(‘string(//a//text())’) # convert it to a string</value><value> [u’Click here to go to the ‘] # output</value><value>A node converted to a string, however, puts together the text of itself plus of all its descendants:</value><value>&gt;&gt;&gt; xp(‘//a[1]’) # selects the first a node</value><value> [u’&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;’]</value><value>&gt;&gt;&gt; xp(‘string(//a[1])’) # converts it to string</value><value> [u’Click here to go to the Next Page’]</value><value>Beware of the difference between //node[1] and (//node)[1]//node[1] selects all the nodes occurring first under their respective parents and (//node)[1] selects all the nodes in the document, and then gets only the first of them.</value><value>from scrapy import Selector</value><value>sel = Selector(text=html_code) </value><value> xp = lambda x: sel.xpath(x).extract()</value><value>xp(“//li[1]”) # get all first LI elements under whatever it is its parent</value><value>xp(“(//li)[1]”) # get the first LI element in the whole document</value><value>xp(“//ul/li[1]”) # get all first LI elements under an UL parent</value><value>xp(“(//ul/li)[1]”) # get the first LI element under an UL parent in the document</value><value>//a[starts-with(@href, ‘#’)][1] gets a collection of the local anchors that occur first under their respective parents and (//a[starts-with(@href, ‘#’)])[1] gets the first local anchor in the document.</value><value>When selecting by class, be as specific as necessary.</value><value>If you want to select elements by a CSS class, the XPath way to do the same job is the rather verbose:</value><value>*[contains(concat(‘ ‘, normalize-space(@class), ‘ ‘), ‘ someclass ‘)]</value><value>Let’s cook up some examples:</value><value>&gt;&gt;&gt; sel = Selector(text=’&lt;p class=”content-author”&gt;Someone&lt;/p&gt;&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’)</value><value>&gt;&gt;&gt; xp = lambda x: sel.xpath(x).extract()</value><value>BAD: because there are multiple classes in the attribute</value><value>[]</value><value>BAD: gets more content than we need</value><value>&gt;&gt;&gt; xp(“//*[contains(@class,’content’)]”)</value><value>[u’&lt;p class=”content-author”&gt;Someone&lt;/p&gt;’, u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</value><value>&gt;&gt;&gt; xp(“//*[contains(concat(‘ ‘, normalize-space(@class), ‘ ‘), ‘ content ‘)]”) </value><value> [u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</value><value>And many times, you can just use a CSS selector instead, and even combine the two of them if needed:</value><value>&gt;&gt;&gt; sel.css(“.content”).extract() </value><value> [u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</value><value>&gt;&gt;&gt; sel.css(‘.content’).xpath(‘@class’).extract() </value><value> [u’content text-wrap’]</value><value>Learn to use all the different axes.</value><value>It is handy to know how to use the axes, you can follow through these examples .</value><value>In particular, you should note that following and following-sibling are not the same thing, this is a common source of confusion. The same goes for preceding and preceding-sibling, and also ancestor and parent.</value><value>Useful trick to get text content</value><value>Here is another XPath trick that you may use to get the interesting text contents:</value><value>//*[not(self::script or self::style)]/text()[normalize-space(.)]</value><value>This excludes the content from the script and style tags and also skip whitespace-only text nodes.</value><value>Tools &amp; Libraries Used:</value><value>Firefox</value><value> Firefox inspect element with firebug</value><value> Scrapy : 1.1.1</value><value> Python : 2.7.12</value><value> Requests : 2.11.0</value><value>Have questions? Comment below. Please share if you found this helpful.</value><value>Read the original article here: </value><value>https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping-part-2/</value><value>Originally published at </value><value>https://blog.datahut.co</value><value> on August 26, 2016.</value><value>Written by</value></article></item>
<item><title>Theory vs. The World: How Retrieving Links from Google Is not t</title><article><value>Do you want to prove a bit of coding helps in the Humanities? Easy!</value><value>We</value><value> all use Google a lot in our research, what if you can store the links you get from search results? This looks like a super-easy task. It takes a second to figure out the steps you need to perform </value><value>by hand</value><value>: access Google, perform the search, get results, save data, move to the next page, iterate if needed.</value><value>Plus the “extract the link” is quite a popular feature in variaty of packages that perform webscraping, you there should be a lot of documentation annd tutorials out there. Even better: the script we want to build is helpful for some colleagues (we’ll work with </value><value>Python</value><value> here).</value><value>It looks like that’s an eay task to learn some new features of a library by putting it in practice. Further, it proves the point of </value><value>coding helps in the humanities</value><value>.</value><value>Cool, so just go. It won’t take long, right? Spoiler: it was not that easy (hence the post).</value><value>The project outline is easy to map and close to what we would do by hand:</value><value>Step 4 looks like the most scary one. We’ll have to inspect the html and get the right tag. But that’s part of the fun. Ok, there are issues lurking here like “how do I find out when I run out of results?”. But we can agree to have a fixed set of pages scraped or even stop a the first one.</value><value>Armed with </value><value>requests</value><value> and </value><value>BeautifulSoup</value><value> library (if you don’t have them, get the instruction for installation </value><value>here</value><value> and </value><value>here</value><value>, respectively) we begin our journey with some standard imports:</value><value>Next, we build our request to a search engige (Google here). To do that we note that all queries on Google have the url that goes as: ‘</value><value>https://www.google.com/search?q=</value><value>’ + ‘something to query’.</value><value>As we don’t want to keep typing our query as an input, we’ll hard code it, i.e. search ‘Goofy’. Then, we check the status of our request to make sure everything is ok when we access the page.</value><value>If you want to input a different query everytime (i.e. not to hard code it) you may go with something like this:</value><value>We have done tasks 1, 2 and 3 from our sketch. Now comes the tricky part. We need to isolate the links that Google gives us. This means we need to create a BeautifulSoup object for each page returning the search results (i.e. what we called </value><value>searchreq</value><value>) and process them with BeautifulSoup.</value><value>We follow the standard practice and call this object ‘soup’. We also specify it’s html that we want to parse. Then in ‘results’ we are going to use our soup object to return what we need and print it. That’s what we add to our code:</value><value>To scrape the links we need to tell BeautifulSoup what we need it to extract. To find this out, we call the inspector mode from our web browser on one of the search results (right click and select inspect on Chrome).</value><value>From there we play a game of:</value><value>Our first choice might be something like ‘http’, but this is going to catch a lot of extra stuff as well like links that are </value><value>not</value><value> search results.</value><value>You have to think about HTML patterns and tags. If you look at it (or Google around like crazy), you’ll find out that there’s a nice thing called </value><value>div class=“r”</value><value> that seems to have what you are looking for.</value><value>After a few extra minutes with the BeautifulSoup documentation page, we learn to get them from the soup with: </value><value>soup.select(‘.r a’)</value><value>.</value><value>So we put all together:</value><value>We are ready to try this out!</value><value>[]</value><value>Exactly, watch that again. A pair of square brackets. That’s our output.</value><value>[], i.e. </value><value>an empty list.</value><value>That’s our result. This is disappointing. Why is that? What’s happening? Let’s check what’s going on.</value><value>The first we do is try to print our soup object (if you have Ipython, use the shell). Once we have the soup object printed, we try to search our beloved “r” class, the one we are trying to select with out soup object.</value><value>It’s not there!</value><value>This is: </value><value>the world getting back on us</value><value>. In practice, theory is not enough. So, well, </value><value>now we can panic</value><value>. What’s going on? This was supposed to be an easy task.</value><value>We start googling more. I went out on Twitter and ask Al Sweigart (the author of </value><value>Automate the Boring Stuff with Python</value><value>, a book you should check if you are starting out with Python) about it. In fact, one of the programs in the book discusses the task of getting links.</value><value>Al was kind enough to let me know that’s common practice for Google to obscure its results. That’s why the soup doesn’t match what we looked at. He briefly reminded me there’s life out of Google, so there are chances to be better off searching on different search engines (he suggested duckduckgo).</value><value>That’s </value><value>reeeeally</value><value> important (hence the extra </value><value>Es</value><value>). Now we know the cause of the problem: </value><value>the HTML we see on the Google is not the same we get with our request</value><value>. And we already have a hint towards a solution: try asking to different search engines.</value><value>We can use these new knowledge to build alternative ways.</value><value>We have a new problem. The HTML that delivers our search results is partly out of our control. What can we do? Can we get it like we see? Are there ways around it? This depends on how we want to fight.</value><value>The first option is to circumvent the problem: we pick a different search engine. In practice, we go on </value><value>Wikipedia</value><value> and asks for search engines names. We then figure out how the query is asked and hope that the links extraction phase stays the same.</value><value>Assuming this, that doesn’t look as a costly option. And we hope one of the engines gives us the same html we can inspect.</value><value>We know what we want to get. Despite the HTML tags being different, we know the links are still there. What about extracting them through </value><value>regular expressions</value><value>? It will be difficult and maybe sub-optmial, but rather than risking to fight again with HTML obfuscation, etc. we can tackle the issue once and forever.</value><value>We’ll write a regular expression extracting all that </value><value>http-something</value><value>. We can predict we will:</value><value>Assuming you can identify the bad links, more links than required might be better than the [empty list] we got before.</value><value>Maybe we can get around the HTML obfuscation and get the search results in a different way. </value><value>Selenium</value><value> is another popular Python library that allows us to automate our browsing.</value><value>Selenium will open the browser for us and then we’ll have a look at the HTML. Should this fail, we may have Selenium inspect the page for us and copy and paste the inspected html.</value><value>This seems something that can work </value><value>in theory</value><value>. But requires extra efforts.</value><value>We know that obfuscation happens but we do not know how and when. Maybe we can try to download the page and save it on our desktop and operate from there.</value><value>This sounds both simple and complicated. Saving a file, easy. Still, we need to access it properly… Is request the way to go? This requires some extra efforts.</value><value>Ok, there’s still a problem but the field looks clearer:</value><value>(This is an improved and reviewed version of a previous post that appeared here: </value><value>http://www.thegui.eu/blog/scraping-links-from-google-part-1.htm</value><value>).</value><value>This work is carried out as part of a </value><value>CAS Fellowship</value><value> as </value><value>CAS-SEE Rijeka</value><value>. See more about the Fellowship </value><value>here.</value><value>Written by</value></article></item>
<item><title>Scraping Data from Website to Excel</title><article><value>You probably know how to use basic functions in Excel. It’s easy to do things like sorting, applying filters, making charts, and outlining data with Excel. You even can perform advanced data analysis using pivot and regression models. It becomes an easy job when the live data turns into a structured format. The problem is, how can we extract scalable data and put it into Excel? This can be tedious if you doing it manually by typing, searching, copying and pasting repetitively. Instead, you can achieve automated data scraping from websites to excel.</value><value>In this article, I will introduce several ways to save your time and energy to scrape web data into Excel.</value><value>Disclaimer</value><value>: There many other ways to scrape from websites using programming languages like PHP, Python, Perl, Ruby and etc. Here we just talk about how to scrape data from websites into excel for non-coders.</value><value>Except for transforming data from a web page manually by copying and pasting, Excel Web Queries is used to quickly retrieve data from a standard web page into an Excel worksheet. It can automatically detect tables embedded in the web page’s HTML. Excel Web queries can also be used in situations where a standard ODBC(Open Database Connectivity) connection gets hard to create or maintain. You can directly scrape a table from any website using Excel Web Queries.</value><value>The process boils down to several simple steps (Check out </value><value>this article</value><value>):</value><value>1. Go to Data &gt; Get External Data &gt; From Web</value><value>2. A browser window named “New Web Query” will appear</value><value>3. In the address bar, write the web address</value><value>(picture from excel-university.com)</value><value>4. The page will load and will show yellow icons against data/tables.</value><value>5. Select the appropriate one</value><value>6. Press the Import button.</value><value>Now you have the web data scraped into the Excel Worksheet — perfectly arranged in rows and columns as you like.</value><value>Most of us would use formula’s in Excel(e.g. =avg(…), =sum(…), =if(…), etc.) a lot, but less familiar with the built-in language — Visual Basic for Application a.k.a VBA. It’s commonly known as “Macros” and such Excel files are saved as a **.xlsm. Before using it, you need to first enable the Developer tab in the ribbon (right click File -&gt; Customize Ribbon -&gt; check Developer tab). Then set up your layout. In this developer interface, you can write VBA code attached to various events. Click HERE (https://msdn.microsoft.com/en-us/library/office/ee814737(v=office.14).aspx) to getting started with VBA in excel 2010.</value><value>Using Excel VBA is going to be a bit technical — this is not very friendly for non-programmers among us. VBA works by running macros, step-by-step procedures written in Excel Visual Basic. To scrape data from websites to Excel using VBA, we need to build or get some VBA script to send some request to web pages and get returned data from these web pages. It’s common to use VBA with XMLHTTP and regular expressions to parse the web pages. For Windows, you can use VBA with WinHTTP or InternetExplorer to scrape data from websites to Excel.</value><value>With some patience and some practice, you would find it worthwhile to learn some Excel VBA code and some HTML knowledge to make your web scraping into Excel much easier and more efficient for automating the repetitive work. There’s a plentiful amount of material and forums for you to learn how to write VBA code.</value><value>For someone who is looking for a quick tool to scrape data off pages to Excel and doesn’t want to set up the VBA code yourself, I strongly recommend automated web scraping tools </value><value>(https://www.octoparse.com/) </value><value>to scrape data for your Excel Worksheet directly or via API. There is no need to learn programming. You can pick one of those web scraping freeware from the list, and get started with extracting data from websites immediately and exporting the scraped data into Excel. Different web scraping tool has its pros and cons and you can choose the perfect one to fit your needs.</value><value>Check out </value><value>this post</value><value> and try out these TOP 30 free web scraping tools</value><value>If time is your most valuable asset and you want to focus on your core businesses, outsourcing such complicated web scraping work to a proficient web scraping team that has experience and expertise would be the best option. It’s difficult to scrape data from websites due to the fact that the presence of anti-scraping bots will restrain the practice of web scraping. A proficient web scraping team would help you get data from websites in a proper way and deliver structured data to you in an Excel sheet, or in any format you need.</value><value>Don’t hesitate if you have things to say. I am a passionate web scraper. </value><value>Welcome to read more articles, and learn web scraping at </value><value>Octoparse</value><value>.</value><value>Written by</value></article></item>
<item><title>None</title><article><value>If you own a business, you need to monitor your competitors’ move so as to remain ahead of the game. However, you need to do a market research so as to gather useful information that will help you determine your position in the online business. The easiest and convenient way to gather data is through </value><value>web data extraction</value><value>. This can be done manually or by using data extraction software. Most businesses employ manual methods by browsing the web in order to gather useful information. While this method is reliable, it is time consuming and expensive. For effective results it is advisable to use automated data mining software which is faster and cheaper.</value><value>Today, </value><value>data mining companies</value><value> have developed web harvesting software which you could buy and install in your computers or you can outsource the services from a qualified company. However, outsourcing the services will help you cut on costs which come with the installation, maintenance and running of the software. All you need to do is specify the type of information you require and the web scraping company will do the searching. According to your specifications, you’ll get customized data scraping services where web crawler show up data that matches your specifications. The information collected is readable, easy to understand and transferable.</value><value>There are different reasons why you should seek web data extraction services. May be you want to monitor your performance in the online market compared to your competitors. This may be in terms of sales and marketing strategies. You need to do a competitor price monitoring to know the products your competitors have and their rates. This will help you set the right prices for your products or services so as to attract more customers. Setting higher prices than your competitors will scare away customers. Competitor price monitoring software will also help you keep track on price changes. However you can only achieve this by getting the right data harvesting specialists. You need to work with a company that will give you value for your money.</value><value>Due to the fast growth in the eCommerce market, business trends keep on changing so you need to check what your competitors are doing in order to remain relevant in business. The web harvesting company should provide you with up to date information for the businesses you’re monitoring. Ensure you get the right data that you can use to formulate a good pricing strategy for your business. Clients will be prompted to order from businesses that offer the right prices for their products. As a business owner you need to ensure you remain ahead of your competition.</value><value>If you’re doing business online it’s mandatory to invest in </value><value>web data extraction services</value><value> so as to enhance business growth. Contact a reputable web data mining company and they’ll do the donkey work as you enjoy the benefits. They have a team of experts who will work closely with your company. They’ll assess your business needs and help you improve your presence in the online market.</value><value>Written by</value></article></item>
<item><title>How can Competitive Business Intelligence (BI) escalate your Success?</title><article><value>You wish to ace it, but is it so easy to do so? You recognize your market; however, do they recognize you as well? You have a massive variety of items for your customers; however, are they of real worth to your clients or consumers? How to really defeat your competition at this game?</value><value>A universal answer to all these questions is </value><value>Competitive Business Intelligence obtained from web data scraping</value><value>!!! If you typically aren’t leveraging big information to your benefit, then you are missing out on those instrumental data mining benefits that your competitors are already doing. You need to understand that there are rivals keeping a competitive eye on you.</value><value>For beginners, it could aid you to snoop your competitors. Considering that today’s economic situation has become a lot fiercer, businesses, as well as vendors have actually been aiming to ace the race. Currently, </value><value>big data</value><value> simply makes it all that easier. With the right tools in place, you not just understand exactly what your rivals do daily, weekly, monthly, or yearly; but even discover exactly what they are doing now.</value><value>Regardless of this, 75% sellers do not make use of real-time affordable analytics.</value><value>There is a remarkable quantity of understandings that big information could discover, yet to be able to utilize it to your advantage needs a framework. Below are a couple of points to watch out for:</value><value>Constantly inspect that the procedures you are preparing to use could be automated or not. In spite of the fact that today everything could be automated, it’s best to be assured. You do not wish to end up doing such a stressful job manually.</value><value>Be specific that the details you are placing for evaluation is exact and also from a qualified resource. Guarantee that you feed the appropriate kind of information for the most precise outcomes.</value><value>Produce applications that make it simpler for team members to draw out real-time details while placing the same for evaluation. When dealing with challenging clients, this could be extremely useful.</value><value>The most effective feature of competitive business intelligence is the location-specific understandings. These kinds of understandings could be several of one of the most important little bits of information you will certainly stumble upon.</value><value>Make your approaches smarter. Your approach can not be as easy as decreasing the cost each time your rival does. Your method needs to make it possible for constant growth as well as most valuable activities based on your sales technique.</value><value>Constantly think of long-term preparation for cost reduction by matching your rival’s activities. You require taking into consideration whether decreasing the cost of a provided product diminish inventory too rapidly or not.</value><value>Want Competitive Business Intelligence (BI) solutions at cost-effective or cheaper rates to leverage your company growth? See us at </value><value>3i Data Scraping</value><value>, your ultimate Big Data outsourcing partners.</value><value>Originally published at </value><value>www.3idatascraping.com</value><value> on November 16, 2017.</value><value>Written by</value></article></item>
<item><title>How do we find daily good deals online, automatically?</title><article><value>Background</value><value>As defined </value><value>here</value><value>, “a data scientist is someone who is better at statistics than any software engineer and better at software engineering than any statistician.” Therefore, this blog post focuses on the practice of web content scrapping, which is an essential skill for data scientists to acquire information outside of structured databases, and when APIs are unavailable.</value><value>When looking for good deals online, we often go on to a few eCommerce websites frequently to check the prices on the items we want. After a while, this becomes a tedious task. Inspired by </value><value>The Programmer’s Guide to Booking a Plane</value><value>, in which Zeke wrote a script in Node to automate the process of finding cheap plane tickets, we would like to replicate his method on good MacBook deals, using a few packages in R.</value><value>Objective</value><value>The objective is to receive automatic email alerts when the MacBook price drops to below a certain point.</value><value>Approach</value><value>We need to load the html structure of the website first, in order to retrieve the information we need. The R package we will be using is </value><value>rvest</value><value>.</value><value>After saving the URL html, we need to find the section of information that we need, by inspecting the page source. We will search a price to navigate to product related information, as shown below.</value><value>We noticed that product related information is under </value><value>&lt;div class=”b-content”&gt;</value><value>and therefore we will extract this part only.</value><value>An excellent Chrome add on called </value><value>SelectorGadget</value><value> can be downloaded </value><value>here</value><value>. This tool allows us to intuitively select the specific content we want.</value><value>When we select the name of the product, the content will be highlighted in green, as shown below. The tool also guesses that we also want other product names as well, and therefore it will highlight other product names in yellow. For any content that we do not need, we can click on it and it will be removed (the color will turn red).</value><value>We found that product name can be extracted using </value><value>.product-name</value><value>, as shown on the bottom of the page.</value><value>Next we will repeat the process to find price and save it in numeric format.</value><value>After we are done, we can save name and price in a dataframe.</value><value>We will also need to scrap multiple pages to extract all the information.</value><value>The final result is stored below in dataframe format.</value><value>2. Create rules to send out email alerts</value><value>Next, we will set up the rules to receive email alerts. Say we only wish to receive alerts on products with price between NT$25,000 and NT$30,000.</value><value>Next we will use the</value><value> mailR</value><value> package to send out the email, if there is at least one alert, as shown below.</value><value>3. Automate the process by scheduling the task regularly</value><value>This can be done with the </value><value>taskscheduleR</value><value> package, but currently only available in Windows. Click </value><value>here</value><value> for more details. We can schedule the Rscript to run at desired frequency and receive automatic alerts accordingly.</value><value>This sums up the short blog on how to scrap content for websites with static content, however, dynamic websites are more complicated and may require additional code to simulate real browsing behaviors, such as member login and form submits. Alternatively, similar task can also be performed in Python with </value><value>scrapy</value><value> and </value><value>BeautifulSoup</value><value>.</value><value>R Code</value><value>Q</value><value>uestions, comments, or concerns?</value><value>jchen6912@gmail.com</value><value>Written by</value></article></item>
<item><title>How to Run JavaScript in Python | Web Scraping | Web Testing</title><article><value>When we develop web application sometimes </value><value>we need to test the UX</value><value>. Most of the time we do it manually. For example, after a form submission what happen, which a person check it manually. In future, if another coder wrongly modified the form code it may creates a bug which may be skipped by manual tester.</value><value>Sometimes </value><value>we want to scrap some webpage’s information</value><value> but which is fully loaded by JavaScript framework. In normal scraping techniques it’s not possible to scrap data as the data is loaded lazily.</value><value>We </value><value>can solve both webpage testing and dynamic web page scraping</value><value> by running </value><value>JavaScript code using </value><value>Selenium</value><value> library. </value><value>Which is called automate the web browser.</value><value>In this post I will discuss about:</value><value>We have a Bangla narrated video tutorial for this solution:</value><value>We need </value><value>pipenv</value><value> to install Selenium library for this project. </value><value>If you don’t know how to install Pipenv then please </value><value>check my other tutorial</value><value>.</value><value>First in terminal go to a directory. In my case I am in this directory:</value><value>/Users/mahmud/Desktop/demo/sel1</value><value>Now open the Terminal in Mac or PowerShell in Windows and run the following commands:</value><value>pipenv install selenium</value><value>It will create 2 files, Pipfile and Pipfile.lock</value><value>Now run the following command to activate sel1 project’s virtualenv.</value><value>To automate web browser, which is done in invisible way, we need to install Google Chrome driver. Please visit </value><value>the following website</value><value> and download the latest released driver for your mac or windows or linux operating system.</value><value>Now </value><value>unzip the downloaded file</value><value>, and c</value><value>opy the chromedriver.exe file</value><value> in our project directory </value><value>sel1</value><value>.</value><value>Now in the sel1 directory, create a python script named </value><value>chapter9.py</value><value> and paste the following codes. </value><value>Github Source</value><value>In macOS terminal run the following command:</value><value>In windows 10 power shell run the following command. Just use Python instead of Python3</value><value>After successfully run the program, you will get a png file named python-github.png.</value><value>It is a very simple script. At first we import python selenium libraries in our script. Then we create a webdriver object based on some options we provided also we mentioned the google chrome browser driver location via </value><value>chrome_driver</value><value> object.</value><value>Then by </value><value>driver.get()</value><value> method we load </value><value>github.com</value><value> website.</value><value>In the </value><value>#scrap info section </value><value>we </value><value>scrap HTML h1 tag data</value><value> and </value><value>print it in the console</value><value>. </value><value>This is how we scrap</value><value> via selenium and headless web driver.</value><value>In the #scrap info section we scrap HTML h1 tag data and print it in the console. This is how we scrap via selenium and headless web driver.</value><value>We see “Built for developers” is printed in the terminal.</value><value>Finally we fill and submit the form by code. To select the search form in the webpage by javascript, we use Google Chrome Browser’s Inspect code option to check the form element name.</value><value>This is the code that automates search and submit the form:</value><value>To take screenshot of the final page we write the following code:</value><value>This is one of the way we can use selenium library in Python to execute JavaScript to test webpage or scrap dynamic or static website information.</value><value>Reference:</value><value>2. Selenium Python Docs: </value><value>http://selenium-python.readthedocs.io/getting-started.html</value><value>3. Google Chrome Web Driver: </value><value>https://sites.google.com/a/chromium.org/chromedriver/downloads</value><value>Source:</value><value> Thinkdiff.net</value><value>Written by</value></article></item>
<item><title>How to link preview like Facebook, Twitter, Slack, and WhatsApp</title><article><value>Have you ever wondered how do web applications preview a link once you’ve posted it on your timeline or send a message ?, I’ve been to the sun and back multiple times trying to figure it out.</value><value>I had many questions that needed to be answered, but it was either no one understood what I asked or I was asking the wrong questions.</value><value>Worrest answers I’ve received were “you can use a web scraper API tool to achieve it, that’s what I used in my project”.</value><value>Services like</value><value>A few more…..</value><value>Until one day I met a guardian angel and I was introduced to </value><value>open graph protocol</value><value>.</value><value>Thank you, Emma 🤗.</value><value>FYI — The correct word for what we are doing is called web scraping</value><value>The Open Graph protocol enables any web page to become a rich object in a social graph. For instance, this is used on Facebook to allow any web page to have the same functionality as any other object on Facebook.</value><value>~ Someone from </value><value>https://ogp.me/</value><value>In short, it describes a website with objects like title, description, images, and more with </value><value>&lt;meta&gt;</value><value> tags.</value><value>I’m not here to talk about open graph protocol, I’m here to show you how to fetch those data to make your own link preview, so if you want to know more about OGP, here are a couple of links.</value><value>FYI — Twitter has its own meta tag, but they use the “twitter” prefix instead of “og”</value><value>It’s a simple process and doesn’t require much work, we will fetch the web page as text in our Node.js application. Then we will select the HTML elements we need and get the data/text it holds, save it to a JSON file then send the data back.</value><value>“But how can we select the dom from the back end Adel ?”</value><value>Easy, with the help of cheerio and other modules like it, cheerio is a </value><value>Fast, flexible, and lean implementation of core jQuery designed specifically for the server.</value><value>As far as I know, you cant, this cant be done in the front end script, when you try to fetch eg my portfolio or any other site in chrome’s console, it will throw a cors (Cross-Origin Resource Sharing) error.</value><value>To bypass this issue, we will send the URL to the back end server, process the request then send back the data we scrapped.</value><value>If you want to tag along, I’ve got starter files you can clone/download, and I’ll be adding the completed files too.</value><value>1 — Get to know the front end script</value><value>In our front end script located in the public/javascript folder has a fairly small amount of code in it, we have a click event listener on our add button, which will</value><value>This function accepts an id and will add the loading preview to the unordered list,</value><value>This function will receive an object, it will get the loading list by its id which was previously added, remove the loading class name then append the data</value><value>This function removes a preview card</value><value>The UUID CDN was making the app load super slow, thanks to </value><value>broofa</value><value> who came up with this function, it will be creating our unique id for each preview card</value><value>2 — Installing modules</value><value>We need to install a few modules.</value><value>Cheerio </value><value>to</value><value> </value><value>load the source code of the webpage we want to crawl.</value><value>ExpressJS</value><value> to create our HTTP server.</value><value>Express-handlebars</value><value> a template engine that makes writing HTML code easier and renders out page.</value><value>Node-fetch </value><value>to make our HTTP request in node.js.</value><value>I’ve added these modules to the dependencies, simply install them by running </value><value>npm i</value><value> in the command line.</value><value>3 — Creating our server</value><value>Over at app.js, we have requested all our modules, set up our view engine and middlewares.</value><value>We can start by creating the home route, which will render the home temple and passing it the data in </value><value>data.json</value><value> (currently, </value><value>data,json</value><value>is empty).</value><value>Open up a command line and run npm start, then in your browser open up localhost://3000.</value><value>You should get an empty home page, with just an input filed.</value><value>Now we work on fetching the metadata we want to get from a web page, let’s create a post route to receive the id and URL from the front end once the add button is clicked.</value><value>In the request body, we are expecting a value from previewUrl and id</value><value>Let’s work on fetching the HTML page from my last medium post.</value><value>Make the anonymous function into an async/await function, and use the fetch API from the node-fetch module, then create a variable called html and give it the value of the fetch method (make sure you use the await keyword, to wait for a result from the fetch), pass it the previewUrl value from the request body, then chain a </value><value>.then(res =&gt; res.text())</value><value> to it.</value><value>Next, we use cheerio, remember cheerio is an </value><value>implementation of </value><value>core jquery for the server side.</value><value>Create a variable with the $ sign and give it the value of </value><value>cheerio.load(), </value><value>pass the html variable to the load method, you can now try and select an html element using the $ sign.</value><value>We can now start getting the meta tags we want, create a variable named </value><value>metaTagData</value><value> which will hold an object of the data,</value><value>id</value><value> — we will pass in the id from </value><value>req.body</value><value> to the object,</value><value>url</value><value> — the web site url to the url key.</value><value>domain</value><value> — For the domain we just need the domain name of the previewUrl, we can use the url module from nodejs to get the hostname.</value><value>title</value><value> — use cheerio to select the meta tag with the attribute of </value><value>name="title"</value><value>img</value><value>— use cheerio to select the meta tag with the attribute of </value><value>name="title"</value><value>description</value><value>— use cheerio to select the meta tag with the attribute of </value><value>name="description"</value><value> and get the attribute of </value><value>content</value><value> .</value><value>The meta tags have another attribute called </value><value>content</value><value> that's where the values are stored, to get the values, you need to chain the cheerio selectors with the </value><value>attr</value><value> method and pass it the string of </value><value>content</value><value> .</value><value>You should end with an object like this.</value><value>Now, this should do it, but some web pages use a basic html meta tag, some use open graph, some use twitter cards, some use the property attribute instead of the name attribute, some don't add a image meta tag, we can basically end up with missing data or no data at all.</value><value>Solution</value><value>Creating a function which will return the first thing it finds</value><value>We can now change the value of title, img and description of our metaTagData object to the getMetaTag function and pass it a the meta tag name as a string.</value><value>And what if a web page doesn't use meta tags at all ?</value><value>We add a fallback value on our title, img and description keys.</value><value>title</value><value> — will fall back to the first h1 tag on the page</value><value>img</value><value> — will fall back to an image in the public/images folder</value><value>description</value><value> — will fall back to the first paragraph tag on the page</value><value>Some descriptions can get a bit lengthy, I decided to keep all descriptions at a max of 200 character count.</value><value>Next, we push the data to the beginning of the data array, using the unshift array method, then write it to the </value><value>data.json</value><value> file using the </value><value>writeFile</value><value> from the fs (file system) nodejs module.</value><value>The first parameter of the </value><value>writeFile</value><value> method takes in the file location, the second parameter we pass in the data we want to write to the file, since its a JSON file we need to stringify the data using the</value><value>JSON.stringify</value><value> method, the third parameter takes in a call back function, where we respond back with JSON and passing it the data using the shift array method and also set the HTTP status to 201.</value><value>Test Run!</value><value>If you start your app, and past a link of any web page then click on add, you should end up with this.</value><value>Deleting the card</value><value>To remove a card, create another post route which will accept an ID from the URL parameter, create a variable named indexOfId and the value you map over the data json array and return the just the id of each object, then chaining the array method </value><value>indexOf()</value><value> to the map, will give you the exact position of the id you want to remove from the array (make sure you pass in the id from the url parameter to the indexOf method).</value><value>Next, we use the splice array method to remove the data from the data json array and passing the first parameter the </value><value>indexOfId</value><value> variable and second parameter the value </value><value>1</value><value> , indicating we want to remove just the object from the array.</value><value>Then we use the fs nodejs module to rewrite the new edited data to the </value><value>data.json</value><value> file, and respond back with a status of 200 and using the respond </value><value>end()</value><value> to end the request.</value><value>Test Run!</value><value>If you try and remove a card, then refresh the page, the removed cards will it will no longer be there.</value><value>We learned how to create a link preview by web scraping meta tags, but with the power of web scraping, you can do more than scrap meta tags.</value><value>Take Adidas as an example, they don't prove an API for their product, images, prices, etc…, and you want to create an eCommerce side project.</value><value>You can go to their web page and start scraping the products, but if a web page like adidas.com uses react, angular or vue, it can get complicated to web scrap.</value><value>You will need to use a headless browser to get around scraping that kind of web sites.</value><value>!!! Watch out though, it is illegal to scrap some web sites !!!</value><value>Got any questions ?</value><value>DM me in twitter @Adel_xoxo and I’ll answer to the best of my knowledge</value><value>~Adel ak</value><value>Written by</value></article></item>
<item><title>Asynchronous Web Scraping in Python using concurrent module.</title><article><value>Ever felt frustrated at how long your web scraping script takes to complete the task? Have you ever wished there was a faster way to do your web scraping?</value><value>Well, there is. And I’m going to show you today how you can increase the performance of your scraper in a very beginner friendly way.</value><value>In this post we will also talk about asynchronous programming in Python. And then apply that knowledge to optimize web scraping.</value><value>Let’s dive in!</value><value>If you’re a beginner in web scraping, then I assume you’ve worked with </value><value>requests</value><value> and </value><value>BeautifulSoup</value><value> modules in python. And what you generally do while writing your scraper is as follows —</value><value>Or you might use a different structure than this. But the end result is same. The way you code your scraper is in a </value><value>synchronous</value><value> fashion.</value><value>What it means is that your program goes through the target URLs one by one, in a synchronized way. You send a GET request to the server and the server takes some time to send a response. But what do you suppose is happening while your program is waiting for a response from the server?</value><value>Nothing!</value><value>That’s right. The network request is the instruction that takes the most time in your script. And when you’re doing it in a synchronous way, your script remains idle a large amount of time which is spent waiting for the server response. How would you make use of that free time?</value><value>It’s quite obvious. We do not want our program to remain idle while one of the GET requests is waiting for server’s response. We want our program to move ahead with other URLS and their processing without being blocked due to one sluggish network request.</value><value>A</value><value>synchronous programming is simply executing multiple instructions simultaneously.</value><value>So we need a way to process multiple URLs simultaneously and independent of one another. Let’s see how we can achieve this in Python.</value><value>In this section, I will discuss different strategies of asynchronous execution. If you’re just interested in the asynchronous python code, you can skip this part.</value><value>There are many ways in which asynchronous execution is implemented. Three broad categories of multi-processing can be given as —</value><value>If you have some background in Unix operating system, you would be familiar with these concepts. Still, I will do my best to explain them as concisely and cogently as possible.</value><value>In Process level multi-processing, you can achieve asynchronous execution by dividing the total work across separate processes. Each process running on a different processor core. In this way, your original task is divided into number of chunks and all of these chunks are being processed simultaneously. This level of multi-processing is in-built in an OS. So all you have to do is utilize this and let the kernel worry about process scheduling.</value><value>Thread level multi-processing is almost same as the previous one. Except in this case, we are dividing the task across multiple threads. A thread is like a process but a lightweight process. And we can add multiple threads under a single process context. So all of these thread would belong to the same process. This feature is also implemented in the OS itself. We just need to utilize this using Python and we will see how it is done.</value><value>Application level multi-processing is somewhat different than the previous two. Here the OS is under the impression that it is executing only one process with a single thread. But our application itself schedules different tasks on that thread for execution. So the asynchronous nature of execution is implemented in our application program itself.</value><value>These are the main ways to handle parallel execution on a traditional Unix system. Now we will see how we can use the </value><value>concurrent</value><value> module in Python to utilize these concepts and to boost our scraping speed.</value><value>Okay, so you must be itching to get started. Let’s start coding —</value><value>So we first import the things we require. You will observe that we imported </value><value>ProcessPoolExecutor</value><value> and </value><value>ThreadPoolExecutor.</value><value> Both of these classes correspond to Process level and Thread level multi-processing respectively. We only need to use one of these. And for our use case i.e web scraping, both of these will be effective.</value><value>So the multi-processing features in the OS are abstracted and we can directly do parallel processing using the above classes.</value><value>The </value><value>concurrent.futures</value><value> module provides a high-level interface for asynchronously executing callables.</value><value>The asynchronous execution can be performed with threads, using </value><value>ThreadPoolExecutor</value><value>, or separate processes, using </value><value>ProcessPoolExecutor</value><value>.</value><value>The way it works is that we have a </value><value>pool </value><value>of threads or processes. And we can assign some task to each of them and they will start executing independently of each other.</value><value>We can create a pool —</value><value>Now we can </value><value>submit</value><value> or </value><value>map</value><value> different tasks to each individual thread or process.</value><value>Suppose we have a list of 100 URLs and we want to download the HTML page for each URL and do some post-processing and extract data.</value><value>We have a function </value><value>download_and_extract</value><value> which will gather our data and we want to gather data from the 100 URLs previously mentioned.</value><value>If we were to do this synchronously, it would take 100 multiplied by average time for one GET request ( assuming post-processing time is trivial ). But instead if we divide the 100 URLs on 4 separate threads/processes, then the time required would be 1/4th the original time, at least theoretically.</value><value>So let us try this —</value><value>Here we have slightly modified the Pool initialization to suit our use case but it does the same thing when we initialized it previously.</value><value>executor.submit</value><value> function takes two parameters in our code. The first one is the task we want to perform Or more technically, the function we want to execute and the parameters for the execution of our function. The executor will distribute the work across 4 different processes with each process executing one instance of </value><value>download_and_extract</value><value> for the given URL.</value><value>But how do we know when the tasks are done? And what about the data that we wanted?</value><value>executor.submit</value><value> returns a </value><value>Future</value><value> object.</value><value>(Future) Encapsulates the asynchronous execution of a callable.</value><value>This object represents the asynchronous execution of a specific function. You can read more about its properties in the </value><value>documentation</value><value>. We will only focus on two main functions for this object that we will require viz. </value><value>done</value><value> and </value><value>result.</value><value>done()</value><value> function returns the bool value </value><value>True</value><value> if the function has finished executing or if there was some exception in it. And when it has finished execution, we can retrieve the result using the </value><value>result()</value><value> function.</value><value>Here’s another new thing — </value><value>as_completed().</value><value>The function </value><value>as_completed()</value><value> simply determines the order of the results that are returned by the future. Using this function, we avoid having to write a block of code where we keep checking whether a given </value><value>Future</value><value> is </value><value>done()</value><value> or not.</value><value>The function will start generating results as soon as any one of the functions being executed yields some result. And then we simply append that result to our main collection of data.</value><value>And that’s it! Using these simple concepts you can make your program multi-processing capable. Web scraping is just a simple example to illustrate the concept. You can apply this concept anywhere you want.</value><value>We will look at a fully coded and working example below —</value><value>You can now experiment using this example with URLs of your choice and different degrees of parallelization. See what conclusions you can draw from this.</value><value>In this post, I demonstrated how to divide a particular task across multiple threads and process. And we achieved asynchronous execution of a specific task in this way.</value><value>But think about this, the task we are doing i.e downloading data from the network, it is admittedly being done across multiple processes but on any one process the task is still being done synchronously.</value><value>What I mean is that we are simply performing the task in a parallel fashion. So in any one of the threads/processes, that one process or thread still remains idle for some time until server responds.</value><value>There is a way in which we can overcome this and make our scraping truly asynchronous. We would have to use Application level multi-processing to accomplish this.</value><value>We want our program to send a GET request and while the server is processing that request, we want our program to suspend that request and move on to next requests. When the server finally responds, we want that data to be mapped to the correct request. In this way, we do not allow our program to remain idle at all. It is always doing something.</value><value>This is possible in python using </value><value>asyncio</value><value> and </value><value>aiohttp</value><value> modules. I will explore both of those modules in the context of web scraping in a future post.</value><value>So stay tuned!</value><value>Written by</value></article></item>
<item><title>6 Tips on How to Do Data Scraping of Unstructured Data</title><article><value>Data scraping, data extraction or web scraping is an automatic web method to fetch or do data collection from your web. It converts unstructured data into structured one which can warehouse to the database.</value><value>6 Tips on How to Do Data Scraping of Unstructured Data</value><value>Conventional technical approaches of unstructured data scraping isolate the moving parts of the results to make that easier for the programmers resolve the issues.</value><value>They are unapproachable from the real time usage setups. However, while the non-programmatic method builds a code, this opens the chances of accepting indications regarding proposed use of the extracted data.</value><value>Any automated data scraping software and checking solution can do this, for example:</value><value>• Avoid worthless links and attain projected data quickly</value><value> • Build a responsive load footprint for the targeted websites</value><value> • Use lesser hardware resources</value><value>It will help in the data mining of unstructured data using the unstructured data scraping tools.</value><value>Besides, non-programmatic method, it will capture knowledge regarding targeted websites better and influence that to promptness of learning using multiple websites, adding up to the ability of scaling proficiently and brilliantly while extracting the unstructured data.</value><value>All the web scraping software depend on the HTML delimiters that breakdown while the main HTML changes as well as the requirement for fixing problems need to be tracked manually.</value><value>Any automated data scraping and tracing solution identify additions and changes with accuracy, offering only the ideal data using techniques of unstructured data examination.</value><value>Any automatic </value><value>web data scraping solution</value><value>, particularly for the data extraction tools for retailer, can help in rationalizing the workflows and processes at scale, smoothly generates productivity gains. They consist of:</value><value>• Automatic load handling and deployment</value><value> • Bulk operations to complete the jobs and task preparation</value><value> • Consistent testing for superior quality assurance</value><value> • Data mining techniques and tools for the unstructured data</value><value> • Shared request lists and schemas for handling different projects having dependable team practices</value><value> • Tools which effortlessly increase the mass regulation activities</value><value> • User subscriptions and agent migrations among the systems</value><value>Unstructured data can be used for the human eyes whereas well-structured can be used for computers.</value><value>A conventional data scraper as well as an automated </value><value>data scraping solution</value><value>, both can </value><value>transform the unstructured data into structured data</value><value>, offering analysis to take superior business decisions.</value><value>Nevertheless, the automated data scraping solutions integrate and use data normalization techniques to ensure that your structured data is effortlessly converted into main data insights.</value><value>Visual abstraction is the method to use machine learning for creating well-organized codes. Visual abstraction recognizes each and every web page just like a human examines a page visually.</value><value>However, an automated </value><value>data mining and extraction solution</value><value> can help you better with a superior level of visual abstraction without utilizing the HTML structures. This doesn’t break while it gets page variations.</value><value>In the existing data-obsessed business environment, many teams frequently interrelate with the data collection as well as analysis procedures.</value><value>Business organizations searching for the web scraping about unstructured data have to talk about and support all the data necessities, for different purposes.</value><value>As the business requirements are different, built-in aspects supportive to different requirements are the key for ranging higher frequencies and volumes of the data collection.</value><value>Find out more about accurate, result-oriented and better accessible data scraping solutions.</value><value>You can </value><value>contact us</value><value> to discover how the automated data intelligence and data extraction solution can improve your organization’s productivity, efficiency, and general workflow.</value><value>Originally published at </value><value>www.3idatascraping.com</value><value> on June 30, 2017.</value><value>Written by</value></article></item>
<item><title>How to Do Price Monitoring from Car Dealers Sites?</title><article><value>The automobile business is booming in all countries including the USA. According to </value><value>NADA</value><value>, since the year 2018, the USA’s 16,794 franchised dealers had sold over 8.6 million light-duty vehicles. The sale of new vehicles has touched the figure of more than $500 billion. Altogether, the dealerships had ordered 155 million repairs, whereas and services sales have reached $58 billion.</value><value>However, there are no location-wise automobile dealer directories available and mostly, the information needs to be collected either using personal contacts or a location-specific Google search. If you want to try and scrape data about </value><value>Price Monitoring</value><value> from the car dealers sites, you can use Google itself as well as use the keywords that should comprise- “car dealer”, together with the location as well as the car’s company name. You can try the initial few links which are not endorsed by Google and scrape data from them. It can be reiterated for different locations as well as car companies through an excel sheet. Although, the efficiency and scale of manually scraping car dealers data are particularly limited.</value><value>For scraping data in an automated manner, you can use professional Web Crawling Services of X-Byte Enterprise Crawling, once you get the website list ready. As a professional </value><value>Web and Data Scraping Service</value><value> provider, X-Byte Enterprise Crawling can provide this data in the plug-and-use format. Provided that you have collected the resources with required persistence, your data will be clean and dependable. However, if you are unaware of which cars are more accepted in which states or countries, you can only scrape data for getting that information and for that </value><value>X-Byte Enterprise Crawling</value><value> is the finest option.</value><value>All the car dealers around the world promote themselves heavily to get more customers. </value><value>Data scraping from social media websites</value><value> and online communities can help you collect information on all the popular auto dealers. Besides that, there are many other resources to scrape price monitoring data from car dealers sites on the web.</value><value>As the web is growing exponentially, it doesn’t matter what research you are doing or applications you are creating, the web is the finest place to collect data and the same applies to scrape data on car dealers. Whether you are creating an application that will utilize your location as well as get you your nearest car dealer or if you want to create a ranking or reviewing site for car dealers, data scraping will assist you to create your data source and fill your website or app with information.</value><value>Scraping price monitoring data</value><value> from the car dealers are extremely difficult and that’s where X-Byte Enterprise Crawling has an important role to play.</value><value>Many dealerships work in both new and used cars and they provide vehicles that fit everyone’s requirements. They offer wonderful customer service with the help of friendly salespeople. They have a lot of used cars to select from. These dealers offer cars of different brands.</value><value>Scraping data from all these car dealers is difficult and that’s what </value><value>X-Byte Enterprise Crawling</value><value> does easily! At X-Byte Enterprise Crawling, we scrape price monitoring data from car dealers’ sites as well as do car inventory scraping and used cars inventory scraping.</value><value>You can also get addition car information like:</value><value>The dealers usually have online inventories that are amongst the key reasons why these dealerships are a brilliant source for different car companies. All the drivers can approach their sales associates and let their customer service specify how they make used or new car procedure, hassle-free!</value><value>Contact </value><value>X-Byte Enterprise Crawling</value><value> for all your car dealer site price monitoring services requirements or ask for a free quote!</value><value>Visit Our Site :</value><value> </value><value>www.xbyte.io</value><value>Written by</value></article></item>
<item><title>Web scraping with Python(using BeautifulSoup)</title><article><value>As usual the first set of questions always go like this, what is web scraping? What is the usefulness? And how do I do it? Now, to answer the first two questions with the simplest of words, web scraping is simply the collection of specific data or information from a web site or a simple web page, to which this information or data could be used for analysis or whatever the web scraper needs such information for. Several programming languages can be used for web scraping, but as stated above we would be using the python programming language to scrape a web site. How do I do it? lets get right to it with a simple example. First, it would be a good thing to note that one of the languages used in building a website is the Hyper Text Mark-up Language(HTML). HTML contains large amount of data in text form. To scrape data from a web site, we would use the beautifulsoup4 from the bs4 python library and the lxml parser( there are other types of parsers but we would be using ‘lxml’ for this example). These tools are way more preferable, very helpful and easy to use when it comes to web scraping.</value><value>Getting Started:</value><value>You would need to create a folder, after which you create a virtual environment in that folder, then install these tools and libraries in the virtual environment. All these steps would be done in the command prompt using pip.</value><value>I will assume that most of the readers have an idea even if it’s a little knowledge on HTML, but if you have none, you could always skim through a good free source website, in which I will recommend “w3schools.com”.</value><value>Now if you are using sublime text, all you have to do is drag your folder named “work’’ into the sublime text application and you are ready to code. If you have a well downloaded anaconda application, jupyter notebook has all these installed, so you would not have to go through the ‘Getting started’ phase.</value><value>Our task is really simple, we are to get the name of movies from ‘</value><value>http://toxicwap.com/New_Movies/</value><value>’ and their links. In your already set compiler, you import the libraries.</value><value>Importing Libraries:</value><value>Getting the raw data:</value><value>After importing the libraries, you get the information in text format from the web page using requests.get().text</value><value>Parsing:</value><value>Now, you have gotten the information needed, so you parse through the text using lxml, you make it clean and readable using prettify().</value><value>Inspecting the web page:</value><value>Go back to the web page and inspect it( you right click and and select inspect), you then navigate the web page from the source code seen, you navigate to the point you are able to highlight the part you want to scrape. When you have gotten all you need, you go back to your code then look through your “prettified” text. When going through your cleaned up data(prettified text) you would see the code you highlighted from the web page, depending on the site you are scraping you may have to dig a lot deeper before getting to what you want.</value><value>Digging(navigating) through the text data:</value><value>To explain the code. First, if you are doing exactly what I am doing, when you inspect the web page you would notice the tags are mostly “div”, now line 1 selects the particular div that holds the content you want to scrape. Line 2 digs deeper into the div to the ul(unordered list) and line 3 digs into the ul to the li(list). Well, We all know what line 4 does( it displays the list).</value><value>Note: some sources online use ‘class_=()’ when trying to get to a particular div, but you would notice that in this particular case the prettified text did not display the class of the div, hence resulting to the use of ‘attrs ={}’.</value><value>Now, the first line says put in the variable named title the text which can be found in the ‘a’ tag(&lt;a&gt;: link tag in HTML), which is also found in the ‘li’ tag. It is literally just digging from ‘li’ into ‘a’ to the ‘text’. From what I have said, you should be able to interpret the third line. Basically, you are already done, but this written code will get you just the first title and the first link, to get all the titles and links you use a for loop.</value><value>Displaying the whole output:</value><value>Notice how .find() changed to .find_all()? That’s what you do when you want to get all the data needed. It is good practice to use .find() first when trying to navigate, then when the code format is gotten you use the .find_all() to get the data remaining. So now the whole code should look like this.</value><value>Complete code:</value><value>You are done, but if you want to save the scrapped data into a text file or csv file, you can. I’ll be saving this into a csv file.</value><value>Saving in a format:</value><value>I would like to add that some websites make it really hard to scrape their page and for some it is illegal to scrape their page.</value><value>That is it. It is all done. I guess I could say you just learnt how to scrape a website.</value><value>Written by</value></article></item>
<item><title>Natural Language Processing</title><article><value>I recently became familiar with the process of using website API’s and/or how to do web scraping, to extract words or tables from websites, to become a source of data for machine learning purposes. That in itself was pretty interesting, but what you can do with all that information, particularly with words, is fascinating. Welcome to the world of Natural Language Processing (NLP).</value><value>NLP has become on of my favorite subjects I have learned in my Data Science learning. Being able to take a post from Reddit, or comments from Amazon, or an article from a webpage, and to create a predictive model from that blows my mind. I mean, how can words be treated like numerical values?! But if start thinking about it, certain words can define who wrote/said in a statement, and if you can identify that, you now have a feature you can help predict on. Finding the occurence of the number of times a word shows up can hold a lot of power.</value><value>For instance, the phrase “Make America Great Again” is President Trump’s slogan. If I am trying to predict if an article or post is written by a democrat or republican, and those words show up in that record, with some tuning, the model would probably predict if a republican wrote that post, or if there are ties to the Republican party, or even Trump himself.</value><value>At the time I was learning this, there was the big news of someone close in Trump’s cabinet that wrote a very incriminating letter of Trump’s alleged missteps as President. Nobody knew who wrote it, but I came across several news reports and articles, where Data Scientists were using NLP to try and find who wrote the article, comparing how certain words were used, compared to a number of other published articles from Trump’s cabinet over the years. Talk about relevant and an exciting use of techniques! It is like being a data detective! That just increased my excitement more and more to dive into the NLP process further.</value><value>I also had an opportunity to talk with a Data Science company, and they had just finished a project using NLP to look for gender discrimination in employee reviews, and they were successful at building a model that helped find these type of discriminatory reviews. Words hold power and can have equal weight, if applied right, to predicting outcomes. The old saying “Sticks and stones can break my bones but words will never hurt me” is something we all know is not true, but apply the concepts of NLP to words, and words might actually be more of a threat than a feature of just rocks and sticks!</value><value>Written by</value></article></item>
<item><title>EZ Web Scraping</title><article><value>Before I actually learned how to do it the concept of web scraping seemed like something extremely complicated and advanced. Using a few simple packages in python, however, it turns out its something that can be learned in an afternoon.</value><value>To accomplish some basic web scraping tasks, all you really need is requests and BeautifulSoup.</value><value>From there, you can simply request a url (after checking that the website allows you to do so, of course) to get the html:</value><value>From there you can use BeautifulSoup to organize the content in a manageable way and search through it by element.</value><value>The above will return all of the ‘td’ elements with the class ‘title’ which you can then further refine.</value><value>Using the above basic skills and something like Chrome’s developer tools, you can learn to scrape almost any basic information off of a website.</value><value>Written by</value></article></item>
<item><title>How to web scrape with Puppeteer in Google Cloud Functions</title><article><value>In this article, I will use </value><value>Javascript</value><value> (</value><value>Node.js</value><value>) for the code, </value><value>Yarn</value><value> as a package manager for Node, and </value><value>apt-get</value><value> for OS dependencies.</value><value>When you need data from a source that doesn’t provide an API, you have to do web scraping. That’s why you can consider using Puppeteer combined with Google Cloud Functions. Puppeteer is a library that uses Chromium to automate browser interactions. However, this is a time-consuming process, heavy for CPU and memory. So in order to keep your app light, you may want to execute this code into a cloud environment like Google Cloud Functions (the equivalent of AWS Lambda).</value><value>Let’s start by initializing a node project:</value><value>Then, </value><value>cd</value><value> to your new project and install Puppeteer:</value><value>This will download the most recent stable version of Chromium on your machine, about ~200MB depending on your OS.</value><value>In order to test and deploy your functions, you will need to install the Google Cloud SDK and the Google Cloud Functions Emulator. To get the SDK, run the following command (on </value><value>Ubuntu</value><value>):</value><value>This SDK will allow you to deploy your functions. But before that, you will need to test them locally with the functions emulator:</value><value>The </value><value>--ignore-engines</value><value> option will very likely be required. Currently, the Google Cloud Functions Emulator is fully compatible with Node 6. If your Node version is higher than that, the dependency won’t work unless you choose to ignore it with this option.</value><value>So basically, your project only needs two files:</value><value>Here, </value><value>package.json</value><value> contains the basic scripts to test your function locally and deploy it:</value><value>This file contains the main dependency of this project, </value><value>puppeteer</value><value>, and two scripts to test and deploy your function. Both scripts rely on </value><value>scrapingExample</value><value>, the name used in the example below with </value><value>exports.scrapingExample</value><value>.</value><value>The following code is a basic configuration for </value><value>index.js</value><value>:</value><value>There is a lot of boilerplate here: the only important lines are lines 38-41! However, we’ll go through the rest of the code to understand what happens.</value><value>First, we import </value><value>puppeteer</value><value> and declare its options:</value><value>Then finally comes the code, split into 3 functions:</value><value>At some point, you may need to have persistent data. To do that, you cannot use the execution environment of your Google Cloud Function. A storage in fact exists, but it is temporary and very limited. To store a large number of files, you can use a cloud storage service like Google Cloud Storage or AWS S3. Just know that with the Google Cloud’s Free Plan, </value><value>you cannot send data to another IP, so in this case, forget about Amazon S3, and go for Google Cloud Storage.</value><value>There are several ways to upload files to a cloud storage. The most elegant one (not always possible), is to download your file (through </value><value>axios</value><value> for example), and pipe it to your remote bucket. This way, you never store anything in your Cloud Function environment, and avoid a lot of potential problems, like available storage or file naming. You can see an example of this method </value><value>here</value><value>.</value><value>But sometimes, piping directly is not possible so you need to store your files in a temporary directory before uploading them. There is a simple way to initialize and use Google Cloud Storage with Puppeteer:</value><value>Here, we do several things:</value><value>As a programmer, it’s a common thing to say it’s someone else’s fault. And when you do web scraping… this may be true! In fact, a website can be very poorly designed at several levels, making it difficult to scrape.</value><value>One problem you may encounter is related to page loading. Puppeteer provides several functions to wait for events. For example, if you need to navigate to a page and get an element from it, you can use the following function: </value><value>await page.waitForNavigation({ waitUntil: 'load' })</value><value>. However, bad website design can make this instruction crash if you try to get an unexisting HTML element on the new page. Some websites trigger the </value><value>load</value><value> event when the new page is loaded, but it only contains a loader element. You have to be careful, and it’s sometimes preferable to use </value><value>await page.waitForSelector('.mySelector')</value><value>. The good thing about these two functions is that they have an optional </value><value>timeout</value><value> argument. This can be useful on websites with a long loading time: the default timeout is 500ms.</value><value>You also need to be careful with navigation links. Sometimes the information you want to scrape won’t be on a page directly accessible by URL. Some websites load data as you navigate, and you may need to reproduce a full “human” browsing to get the information you need.</value><value>Finally, be very precise with your CSS selectors! Some websites use the same id on several elements. This can make you select the wrong element in your code. When possible, use the </value><value>&gt;</value><value> selector (or other selectors) to prevent any ambiguity.</value><value>Your Google Cloud Function can run out of memory if you are not careful. Puppeteer launches Chromium, and you need to instantiate big objects (like </value><value>browser</value><value> or </value><value>page</value><value>) to use it. In the example above titled </value><value>Basic configuration</value><value>, you can see that </value><value>closeConnection</value><value> is called in the </value><value>finally</value><value> block. This is to destroy the objects and clean up the memory as you exit the function. In many Puppeteer examples, you don’t destroy anything in case of error. After several executions, your environment memory can then become full, and the first instruction </value><value>puppeteer.launch(PUPPETEER_OPTIONS)</value><value> will crash.</value><value>In the Google Cloud Management Console, you have access to logs that give you information about the remote execution of your functions. But for your local logs, you can use:</value><value>To clear them, just execute (</value><value>sudo</value><value> may be required here):</value><value>In order to get information on DOM elements, you can use the Puppeteer function </value><value>page.evaluate()</value><value>. Inside its callback, you have access to DOM elements (through CSS selectors for example), but the rest of your code is not accessible. As a second argument after the callback, you can pass it a serializable object. This means that </value><value>a function defined outside </value><value>evaluate()</value><value> cannot be used inside of it.</value><value>Another problem with </value><value>page.evaluate()</value><value> is that it’s hard to debug. In fact, if you try to use </value><value>console.log</value><value> inside of it, you won’t see anything in your local logs. To solve this issue, add the following instruction just after you initialize the </value><value>page</value><value> object:</value><value>When you test your function locally, you almost always put the </value><value>headless</value><value> option to </value><value>false</value><value> to see what happens in your browser. But when you deploy your function, you want the </value><value>headless</value><value> option to be set to </value><value>true</value><value> (otherwise it won’t work). So here is the perfect place to use an environment variable as the value of </value><value>headless</value><value>.</value><value>Finally, a very easy way to reduce the execution time of your cloud function is to parallelize text inputs in forms. If you have forms to fill, instead of doing several </value><value>await page.type('.selector', fieldValue)</value><value>, parallelize them in a </value><value>Promise.all</value><value>. Of course, the submitting of the form must be done outside of this </value><value>Promise.all</value><value> to have valid field values.</value><value>I hope you found this article useful! Feel free to give me your feedback and ask any questions :)</value><value>Written by</value></article></item>
<item><title>How to do data scraping EFFICIENTLY?</title><article><value>For many people, data scraping or web scraping is to write some programs that click websites and copy information from them. While this is a legitimate way to do scraping, it is the least efficient method.</value><value>In this article, I will list a few methods I used in scraping data from less efficient ones to more efficient ones.</value><value>Put it in a very simple way, the web is a place where a lot of requests and responses happening. A client (user) requests information from a server (which is just another computer). And the server serves the information (response) back to the client.</value><value>What you see as a client on a browser is just a bunch of data parsed and formatted in a pretty and presentable way, thanks to HTML, CSS, and JavaScript. The key here is the line: “just a bunch of data” which means we may be able to scrape it.</value><value>There is a subtle detail which may affect the technique to scrape data. Sometimes, the whole webpage is rendered in the server side which means the server sends the complete HTML, CSS and JavaScript to the client and the client’s browser displays it. The other way is that the server returns the data and the client browser is responsible for parsing it. For the websites which is rendered in server side, we can only scrape by HTML elements . For the websites which is rendered in the client side, we can scrape by a more efficient method.</value><value>Actually, the key to efficient scraping is to find a place where data is rendered in the client side. For example, you may find that in a website, the data is rendered in server. But you also find that the company provides an android app. It is very likely that for an android app, the data is rendered in the client side. This enable us to use a more efficient method.</value><value>Don’t underestimate the difference. Scraping HTML elements are usually very slow!</value><value>A script will send a request to the server. Server responses with the HTML file. Then the script find the location of information and extract it.</value><value>A script will send a request to the server. Server responses with data, usually JSON, XML. Then the script extracts the useful information or store the responded data directly.</value><value>Usually people we look at the Network Tab in the developer tool (Firefox) to inspect the traffic. </value><value>Mitmproxy </value><value>may also be used for more detail analysis.</value><value>This is a special tricks I used to scrape webpage that renders in server side. But it only works if the company provides an Android app (Haven’t tried it on IOS app).</value><value>The steps go like this:</value><value>For example, I want to scrape Centaline Property Website. After inspecting the webpage, I found that it seems like data is rendered in the server side.</value><value>But, they provided an android app. After using the above steps, I managed to find the data which is requested (POST) using</value><value>https://hkapi.centanet.com/api/FindProperty/MapV2.json?postType=s&amp;order=desc&amp;page=1&amp;pageSize=20&amp;pixelHeight=2220&amp;pixelWidth=1080&amp;points[0].lat=22.705635288642362&amp;points[0].lng=113.85844465345144&amp;points[1].lat=22.705635288642362&amp;points[1].lng=114.38281349837781&amp;points[2].lat=21.993328259196705&amp;points[2].lng=114.38281349837781&amp;points[3].lat=21.993328259196705&amp;points[3].lng=113.85844465345144&amp;sort=score&amp;zoom=9.745128631591797&amp;platform=android</value><value>I cannot go into very detail in the steps. The article is quite long already. The message here is that try not to use HTML element in the first place. Always look for client side render service. This will improve your scraping efficiency greatly.</value><value>However, something that is working is always better than nothing. If you really can’t find a better way at this moment, just use HTML element and keep looking for better solution!</value><value>Written by</value></article></item>
<item><title>Coupling Web Scraping with Functional programming in R for Scale</title><article><value>In this article, we will see how to do web scraping with R while doing so, we’ll leverage functional programming in R to scale it up. The nature of the article is more like a cookbook-format rather than a documentation/tutorial-type, because the objective here is to explain how effectively web scraping can be coupled with Functional Programming</value><value>Web scraping needs no introduction among Data enthusiasts. It’s one of the most viable and most essential ways of collecting Data when the data itself isn’t available.</value><value>Knowing web scraping comes very handy when you are in shortage of data or in need of Macroeconomics indicators or simply no data available for a particular project like a Word2vec / Language with a custom text dataset.</value><value>rvest</value><value> a beautiful (like BeautifulSoup in Python) package in R for web scraping. It also goes very well with the universe of </value><value>tidyverse</value><value> and the super-handy </value><value>%&gt;%</value><value> pipe operator.</value><value>Text Analysis of how customers feel about Etsy.com. For this, we are going to extract reviews data from </value><value>trustpilot.com</value><value>.</value><value>Below is the R code for scraping reviews from the first page of Trustpilot’s Etsy page. </value><value>URL: https://www.trustpilot.com/review/www.etsy.com?page=1</value><value>This is fairly a straightforward code where we pass on the URL to read the html content. Once the content is read, we use </value><value>html_nodes</value><value> function to get the reviews text based on its </value><value>css selector property</value><value> and finally just taking the text out of it </value><value>html_text()</value><value> and assigning it to the R object </value><value>reviews</value><value> .</value><value>Below is the sample output of </value><value>reviews</value><value>:</value><value>Well and Good. We’ve successfully scraped the reviews we wanted for our Analysis.</value><value>But the catch is the amount of reviews we’ve got is just 20 reviews — in that as we can see in the screenshot we’ve already got a non-English review that we might have to exclude in the data cleaning process.</value><value>This all puts us in a situation to collect more data to compensate the above mentioned data loss and make the analysis more effective.</value><value>With the above code, we had scraped only from the first page (which is the most recent). So, Due to the need for more data, we have to expand our search to further pages, let’s say 10 other pages which will give us 200 raw reviews to work with before data processing.</value><value>The very conventional way of doing this is to use a loop — typically </value><value>for</value><value>loop to iterate the URL from 1 to 20 to create 20 different URLs (String Concatenation at work) based on a base url. As we all know that’s more computationally intensive and the code wouldn’t be compact either.</value><value>This is where we are going to use R’s functional programming support from the package </value><value>purrr</value><value> to perform the same iteration but quite in R’s </value><value>tidy</value><value> way within the same data pipeline as the above code. We’re going to use two functions from </value><value>purrr</value><value> ,</value><value>Below is our Functional Programming Code</value><value>As you can see, this code is very similar to the above single-page code and hence it makes it easier for anyone who understand the previous code to read this through with minimal prior knowledge.</value><value>The additional operations in this code is that we build 20 new URLs (by changing the query value of the URL) and pass on those 20 URLs one-by-one for web scraping and finally as we’d get a list in return, we use </value><value>unlist</value><value> to save all the reviews whose count must be 200 (20 reviews per page x 10 pages).</value><value>Let’s check how the output looks:</value><value>Yes, 200 reviews it is. That fulfills our goal of collecting (fairly) sufficient data for performing the text analysis use-case we mentioned above.</value><value>But the point of this article is to introduce you to the world of functional programming in R and to show how easily it fits in with the existing data pipeline / workflow and how compact it is and with a pinch of doubt, how efficient it is (than a typical for-loop). Hope, the article served its purpose.</value><value>Thanks: This entire article and code was inspired by the Session that Saurav Ghosh took in the Bengaluru R user group meetup</value><value>Written by</value></article></item>
<item><title>Hands-On Web Scraping With Python</title><article><value>W</value><value>eb scraping is inherently useful for many people, in particular those who do not know how to do it. I have written many web scraping scripts for friends. None of them had any programming or computer science related background. This tutorial is for all the Sociologists, Business Analysts, Literature Researcher and all other people sometimes need to automatically collect data from the web.</value><value>At the end of this tutorial we will have a little script, which if you run it automatically collects an article from medium.com which you could for instance store in a .csv file. You…</value><value>Written by</value></article></item>
<item><title>How I took a break from Job Searching and let Python do it for me.</title><article><value>Another day in the Winter break. The ever looming blade of getting a full time job compels me to go to Indeed.com. I use the Advanced Search set my preferences, location, radius around the location, job title, entry level, full time, how long ago the jobs was posted (I prioritize applying to jobs no older than 15 days) the usual stuff. Hit search and the nightmare begins.</value><value>Now, before I complain about Indeed for no reason. I consider it to be one of the best websites for job search — in fact, I got my summer internship through Indeed. It has a great collection of relevant jobs and a nice set of filtering options. But finding full time jobs is kind of a mess because of the wider range of job type that all fall under the same umbrella.</value><value>If only I could do this entire chore of going through each and every job title, job description and eliminating unsuitable ones and consolidating the right ones in one place. Well, yes I can, with Python. I consider Python to be a great automation tool, with its rich set of libraries and intuitive syntax, be it data cleaning or arranging my desktop, it never lets me down.</value><value>First, task is to find out if at all the job title matches my need. But, how do I know where is what on the webpage and how will my code know that. Lucky for me, Indeed has a very well defined html page structure and I can leverage the semantic class and id tags. View the page source or simply “inspect element” on your choice of page element. And Voila!</value><value>If you look closer. All results have the class tag “result”. That’s great, something to begin coding. Lets head over to </value><value>atom. </value><value>BeautifulSoup</value><value> is a great tool for all things HTML. Plus, it is great for nested search of elements and attributes. Grab the URL,</value><value>url_base = “</value><value>https://www.indeed.com/jobs?q=software+engineer</value><value>…”</value><value>load the page into a soup</value><value>and lets search for that job. </value><value>(pgno is what lets me go through all the pages, but I’m leaving out the details.)</value><value>Now, having seen the html layout. I can get the </value><value>div </value><value>holding the “result”.</value><value>for job in soup.find_all(class_=’result’):</value><value>I have access to the title, company, location, short description, salary if listed, and the URL for the job.</value><value>All kinds of checks can be applied on these, like ignore anything with the work “intern” or “senior”. After doing some more filtering from this information I use the URL to get the job description and get </value><value>regex </value><value>to find out if its good for me. The regex here checks for 2 things :</value><value>If I find either of these in a description, it is thrown out as not suitable.</value><value>As evident, I may have played it fast and loose with the regular expressions. But based on the job descriptions I have encountered before (hundreds). This seemed to be enough to eliminate jobs which I did not fit for as well as prevent me loosing out on suitable postings.</value><value>So there it is a </value><value>s</value><value>ample of the process I followed. Although, I did a lot more tweaking and I was more specific about what job titles I targeted.</value><value>Here’s the github link to the code: </value><value>https://github.com/umangkshah/job-scraping-python/blob/master/job_scraper.ipynb</value><value>All that’s left now is to go and apply. I always make sure that I am a good fit for the job description and that my resume and cover letter has all the relevant details. Currently, I am exploring roles in Self Driving Car teams dealing with Perception, Localization, Mapping or Motion Planning and tricks like this are helping me find specific titles.</value><value>This is my first post on Medium (or ever ). I got a lot of help from other Medium articles on writing and formatting. Look forward to write more. I plan to cover some topics in AI/Self Driving Cars too. So thanks for reading, let me know how you optimize your job hunt?</value><value>Written by</value></article></item>
<item><title>Scrapping the content of single-page application (SPA) with headless Chrome and puppeteer</title><article><value>All the code examples from this articles you can find on a GitHub repository </value><value>https://github.com/AndrejsAbrickis/axios-cheerio-puppeteer</value><value>Axios and cheerio is a great toolset to fetch and scrape the content of a static web page. But nowadays when many of the websites are built as a single page application and gets rendered dynamically on the client it might not be possible to get the content.</value><value>Just because it’s rendered asynchronously and the content is not backed into the HTML received over the wire, doesn’t mean you cannot access it. You just need a different toolset which allows waiting for the content to appear.</value><value>Let’s have a quick look on the source HTML of a SPA application and the rendered result.</value><value>In the screenshot above, on the left, you can see a fully rendered standings table. But look at the source the browser downloaded all we can notice is a single </value><value>&lt;div id="#app"&gt;&lt;/div&gt;</value><value> and a couple of JavaScript files and NO content. So let’s try to get the HTML content of the body.</value><value>axios</value><value> is a “Promise based HTTP client for the browser and node.js”. Because it’s an HTTP client we can use it to fetch an HTTP endpoint and receive the response with the body. We can use the HTTP client to fetch not only HTML endpoint but also JSON, images, etc. And hence we are responsible to handle the plain text response.</value><value>That’s where the cheerio comes to help. </value><value>Cheerio</value><value> is a “Fast, flexible &amp; lean implementation of core jQuery designed specifically for the server”. Basically, it loads and parses the HTML markup as plain text and returns a DOM model we can then access and traverse in jQuery style.</value><value>And because cheerio doesn’t interpret the markup as a browser does. It won’t apply the CSS styles and won’t run the JavaScript and the dynamically rendered content won’t be added to the DOM.</value><value>As an example let’s try to get the content of the body tag using axios and cheerio. In the following gist you can see that we are firing a GET request (L6), then parse the response data into a DOM using cheerio (L7) and finally search for the </value><value>&lt;body&gt;</value><value> element (L9) to output its HTML content.</value><value>When executed this node script we get the web apps placeholder element </value><value>&lt;div id="app"&gt;</value><value> without the dynamically rendered content.</value><value>Because of what we received over the wire was a plain text and the JavaScripts included in the HTML were not executed and this is where a headless browser comes to rescue.</value><value>Let me shortly explain what a </value><value>headless</value><value> browser is. In a nutshell headless means it’s a browser without graphical user interface (GUI) which can be controlled programmatically. Mostly it’s useful for E2E testing as it will apply all styles, and run JavaScript to generate the DOM. And because of that, it’s a perfect tool to scrape Single Page Applications.</value><value>And as I mentioned that it’s controlled programmatically. And for that reason, we can use puppeteer to control the browser over the </value><value>DevTools</value><value> protocol. Let’s get hands-on and see how to get the dynamically rendered HTML.</value><value>In the example above we are using single dependancy pf puppeteer package. First, we initialize a browser instance (L5) and create a new browser page (L6). Afterward, we instruct the browser page to visit an URL (L7) and wait for an element to appear on the page (L8) before to continue. Notice that one can set the timeout in milliseconds how long the browser should wait for the element.</value><value>After we have awaited the element we are using page’s evaluate method to execute a JavaScript within the web page’s context (L10 — L12). This allows us to access the HTML document using vanilla DOM API. From this, we return the HTML of body element and output. And finally, we close the browser which kills the headless Chrome’s process.</value><value>And now the result of running this script includes the content of dynamically rendered HTML.</value><value>This short post demonstrated two solutions how to scrape a website. One can use a combination of axios and cheerio to get the content of a statically rendered website. And use puppeteer to get a dynamical content which is rendered by a fully-powered and invisible (headless) browser.</value><value>I hope this article will help you to start to utilize the mentioned tools as they can be used not only to scrape the websites but also for testing your web apps (E2E or snapshot tests) or taking screenshots.</value><value>If you found this post useful and would like to read more about random web development topics, just clap for this article or drop a comment here. And as always you can find me on </value><value>Twitter@andrejsabrickis</value><value>This article, the content, and opinions expressed on Medium are my own. But as I work for one of the</value><value> leading P2P loans marketplaces Mintos.com</value><value> I would like to use this last line to promote that we are hiring. Including the Growth Engineering team, I am leading at the moment.</value><value>You can see all list of the </value><value>open positions on our Workable board</value><value>. And feel free to contact me directly if you find something interesting in the list or would like to recommend a person you know.</value><value>Cheers!</value><value>Written by</value></article></item>
<item><title>A Serverless Pipeline to retrieve, validate, and immerse the data to Azure SQL Server from Twitter.</title><article><value>Learning how to do data science is like learning to ski. You have to do it.</value><value>Given a twitter ID, get a minimum of 100 followers (Modified this to keep in Azure function 5–10 min timeout…</value><value>Written by</value></article></item>
</items>