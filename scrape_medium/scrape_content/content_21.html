<p id="4b1f" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">The first rule of web crawling is you do not harm the website. The second rule of web crawling is you do <strong class="gs he">NOT</strong> harm the website. We’re supporters of the democratization of web data, but not at the expense of the website’s owners.</p><p id="fcc7" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">In this post we’re sharing a few tips for <a href="https://scrapy.org/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy</a> users (Scrapy is a 100% open source web crawling framework) who want polite and considerate web crawlers.</p><p id="ff18" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Whether you call them spiders, crawlers, or robots, let’s work together to create a world of Baymaxs, WALL-Es, and R2-D2s rather than an apocalyptic wasteland of HAL 9000s, T-1000s, and Megatrons.</p><figure class="hk hl hm hn ho hp do dp paragraph-image"><div class="do dp hj"><div class="hv r hw hx"><div class="hy r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*YSO3AbxfWQ6Bc8McB5-dGA.png?q=20" width="310" height="171" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="310" height="171" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/620/1*YSO3AbxfWQ6Bc8McB5-dGA.png" width="310" height="171" role="presentation"></noscript></div></div></div><figcaption class="ax fi id ie if dq do dp ig ih as cx">Embrace the lovable bots</figcaption></figure><h1 id="c209" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">What Makes a Crawler Polite?</h1><blockquote class="iu iv iw"><p id="6701" class="gq gr ef ix gs b gt gu gv gw gx gy gz ha hb hc hd dx">A polite crawler respects robots.txt<br>A polite crawler never degrades a website’s performance<br>A polite crawler identifies its creator with contact information<br>A polite crawler is not a pain in the buttocks of system administrators</p></blockquote><h1 id="7722" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">robots.txt</h1><p id="1637" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Always make sure that your crawler follows the rules defined in the website’s robots.txt file. This file is usually available at the root of a website (www.example.com/robots.txt) and it describes what a crawler should or shouldn’t crawl according to the <a href="https://support.google.com/webmasters/answer/6062608?hl=en" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Robots Exclusion Standard</a>. Some websites even use the crawlers’ user agent to specify separate rules for different web crawlers:</p><pre class="hk hl hm hn ho jd je cm"><span id="1949" class="jf ij ef at jg b fi jh ji r jj">User-agent: Some_Annoying_Bot<br>Disallow: /</span><span id="09ee" class="jf ij ef at jg b fi jk jl jm jn jo ji r jj">User-Agent: *<br>Disallow: /*.json<br>Disallow: /api<br>Disallow: /post<br>Disallow: /submit<br>Allow: /</span></pre><h1 id="6211" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Crawl-Delay</h1><p id="079d" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Mission critical to having a polite crawler is making sure your crawler doesn’t hit a website too hard. Respect the delay that crawlers should wait between requests by following the robots.txt Crawl-Delay directive.</p><p id="73d1" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">When a website gets overloaded with more requests that the web server can handle, they might become unresponsive. Don’t be that guy or girl that causes a headache for the website administrators.</p><h1 id="b89d" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">User-Agent</h1><p id="a962" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">However, if you have ignored the cardinal rules above (or your crawler has achieved aggressive sentience), there needs to be a way for the website owners to contact you. You can do this by including your company name and an email address or website in the request’s User-Agent header. For example, Google’s crawler user agent is “Googlebot”.</p><h1 id="445a" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">How to be Polite using Scrapy</h1><p id="735c" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx"><a href="https://scrapy.org/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy</a> is a bit like Optimus Prime: friendly, fast, and capable of getting the job done no matter what. However, much like Optimus Prime and his fellow Autobots, Scrapy occasionally needs to be <a href="https://youtu.be/DgQHgy7Nmkk?t=8s" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">kept in check</a>. So here’s the nitty gritty for ensuring that Scrapy is as polite as can be.</p><figure class="hk hl hm hn ho hp do dp paragraph-image"><a href="https://scrapy.org/"><div class="do dp jp"><div class="hv r hw hx"><div class="jq r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*FsyqhfN3evDrckB5IH_h8w.png?q=20" width="256" height="256" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="256" height="256" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/512/1*FsyqhfN3evDrckB5IH_h8w.png" width="256" height="256" role="presentation"></noscript></div></div></div></a></figure><h1 id="d39a" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Robots.txt</h1><p id="a0e9" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Crawlers created using Scrapy 1.1+ already respect robots.txt by default. If your crawlers have been generated using a previous version of Scrapy, you can enable this feature by adding this in the project’s settings.py:</p><pre class="hk hl hm hn ho jd je cm"><span id="4b9d" class="jf ij ef at jg b fi jh ji r jj">ROBOTSTXT_OBEY = True</span></pre><p id="e091" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Then, every time your crawler tries to download a page from a disallowed URL, you’ll see a message like this:</p><pre class="hk hl hm hn ho jd je cm"><span id="8c55" class="jf ij ef at jg b fi jh ji r jj">2016-08-19 16:12:56 [scrapy] DEBUG: Forbidden by robots.txt: &lt;GET http://website.com/login&gt;</span></pre><h1 id="eae6" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Identifying your Crawler</h1><p id="6cd1" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">It’s important to provide a way for sysadmins to easily contact you if they have any trouble with your crawler. If you don’t, they’ll have to dig into their logs and look for the offending IPs.</p><p id="eeef" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Be nice to the friendly sysadmins in your life and identify your crawler via the Scrapy USER_AGENT setting. Share your crawler name, company name and a contact email:</p><pre class="hk hl hm hn ho jd je cm"><span id="829d" class="jf ij ef at jg b fi jh ji r jj">USER_AGENT = 'MyCompany-MyCrawler (bot@mycompany.com)'</span></pre><h1 id="940d" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Introducing Delays</h1><p id="0c5b" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Scrapy spiders are blazingly fast. They can handle many concurrent requests and they make the most of your bandwidth and computing power. However, with great power comes great responsibility.</p><p id="772d" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">To avoid hitting the web servers too frequently, you need to use the <a href="http://doc.scrapy.org/en/latest/topics/settings.html?highlight=download_delay#download-delay" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">DOWNLOAD_DELAY</a> setting in your project (or in your spiders). Scrapy will then introduce a random delay ranging from 0.5 * DOWNLOAD_DELAY to 1.5 * DOWNLOAD_DELAY seconds between consecutive requests to the same domain. If you want to stick to the exact DOWNLOAD_DELAY that you defined, you have to disable <a href="http://doc.scrapy.org/en/latest/topics/settings.html?highlight=download_delay#randomize-download-delay" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">RANDOMIZE_DOWNLOAD_DELAY</a>.</p><p id="1fcb" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">By default, DOWNLOAD_DELAY is set to 0. To introduce a 5 second delay between requests from your crawler, add this to your settings.py:</p><pre class="hk hl hm hn ho jd je cm"><span id="6324" class="jf ij ef at jg b fi jh ji r jj">DOWNLOAD_DELAY = 5.0</span></pre><p id="be82" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">If you have a multi-spider project crawling multiple sites, you can define a different delay for each spider with the download_delay (yes, it’s lowercase) spider attribute:</p><pre class="hk hl hm hn ho jd je cm"><span id="676b" class="jf ij ef at jg b fi jh ji r jj">class MySpider(scrapy.Spider):<br>    name = 'myspider'<br>    download_delay = 5.0<br>    ...</span></pre><h1 id="ef64" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Concurrent Requests Per Domain</h1><p id="2c65" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Another setting you might want to tweak to make your spider more polite is the number of concurrent requests it will do for each domain. By default, Scrapy will dispatch at most 8 requests simultaneously to any given domain, but you can change this value by updating the <a href="http://doc.scrapy.org/en/latest/topics/settings.html#concurrent-requests-per-domain" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">CONCURRENT_REQUESTS_PER_DOMAIN</a> setting.</p><p id="56a8" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Heads up, the <a href="http://doc.scrapy.org/en/latest/topics/settings.html?highlight=download_delay#concurrent-requests" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">CONCURRENT_REQUESTS</a> setting defines the maximum amount of simultaneous requests that Scrapy’s downloader will do for all your spiders. Tweaking this setting is more about your own server performance / bandwidth than your target’s when you’re crawling multiple domains at the same time.</p><h1 id="2058" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">AutoThrottle to Save the Day</h1><p id="bf9b" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Websites vary drastically in the number of requests they can handle. Adjusting this manually for every website that you are crawling is about as much fun as watching paint dry. To save your sanity, Scrapy provides an extension called <a href="http://doc.scrapy.org/en/latest/topics/autothrottle.html" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">AutoThrottle</a>.</p><p id="ae78" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">AutoThrottle automatically adjusts the delays between requests according to the current web server load. It first calculates the latency from one request. Then it will adjust the delay between requests for the same domain in a way that no more than <a href="http://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">AUTOTHROTTLE_TARGET_CONCURRENCY</a> requests will be simultaneously active. It also ensures that requests are evenly distributed in a given time span.</p><p id="ad9f" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">To enable AutoThrottle, just include this in your project’s settings.py:</p><pre class="hk hl hm hn ho jd je cm"><span id="f050" class="jf ij ef at jg b fi jh ji r jj">AUTOTHROTTLE_ENABLED = True</span></pre><p id="66cb" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx"><a href="https://scrapinghub.com/scrapy-cloud/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy Cloud</a> users don’t have to worry about enabling it because it’s already enabled by default.</p><p id="716f" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">There’s a <a href="http://doc.scrapy.org/en/latest/topics/autothrottle.html#settings" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">wide range of settings</a> to help you tweak the throttle mechanism, so have fun playing around!</p><h1 id="7d4e" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Use an HTTP Cache for Development</h1><p id="a014" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Developing a web crawler is an iterative process. However, running a crawler to check if it’s working means hitting the server multiple times for each test. To help you to avoid this impolite activity, Scrapy provides a built-in middleware called <a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">HttpCacheMiddleware</a>. You can enable it by including this in your project’s settings.py:</p><pre class="hk hl hm hn ho jd je cm"><span id="c044" class="jf ij ef at jg b fi jh ji r jj">HTTPCACHE_ENABLED = True</span></pre><p id="9d39" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Once enabled, it caches every request made by your spider along with the related response. So the next time you run your spider, it will not hit the server for requests already done. It’s a win-win: your tests will run much faster and the website will save resources.</p><h1 id="94bb" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Don’t Crawl, use the API</h1><p id="2c4d" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Many websites provide HTTP APIs so that third parties can consume their data without having to crawl their web pages. Before building a web scraper, check if the target website already provides an HTTP API that you can use. If it does, go with the API. Again, it’s a win-win: you avoid digging into the page’s HTML and your crawler gets more robust because it doesn’t need to depend on the website’s layout.</p><h1 id="1ea4" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Scrapinghub Abuse Report Form</h1><p id="8d37" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Hey folks using our <a href="https://scrapinghub.com/scrapy-cloud/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy Cloud</a> platform! We trust you will crawl responsibly, but to support website administrators, we provide an <a href="https://scrapinghub.com/abuse-report/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">abuse report form</a> where they can report any misbehaviour from crawlers running on our platform. We’ll kindly pass the message along so that you can modify your crawls and avoid ruining a sysadmin’s day. If your crawler’s are turning into Skynet and <a href="https://scrapinghub.com/tos/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">running roughshod over human law</a>, we reserve the right to halt their crawling activities and thus avert the robot apocalypse.</p><h1 id="cd84" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Wrap Up</h1><p id="9729" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Let’s all do our part to keep the peace between sysadmins, website owners, and developers by making sure that our web crawling projects are as noninvasive as possible. Remember, we need to band together to delay the rise of our robot overlords, so let’s keep our crawlers, spiders, and bots polite.</p><figure class="hk hl hm hn ho hp do dp paragraph-image"><div class="do dp jr"><div class="hv r hw hx"><div class="js r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*kgoZTFROt7PpugiqcGHWLQ.jpeg?q=20" width="300" height="225" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="300" height="225" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/600/1*kgoZTFROt7PpugiqcGHWLQ.jpeg" width="300" height="225" role="presentation"></noscript></div></div></div></figure><p id="c026" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">To all website owners, help a crawler out and ensure your site has an HTTP API.</p><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><p id="1c01" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx"><a href="https://scrapinghub.com/platform/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy Cloud is forever free</a> and is the peanut butter to Scrapy’s jelly. Hopefully you learned a few tips for how to both speed up your crawls and prevent abuse complaints.</p></div></div></section><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><figure class="hk hl hm kd ke hp ic kf bv kg kh ki kj kk bg kl km kn ko kp kq paragraph-image"><div class="do dp kc"><div class="hv r hw hx"><div class="jq r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*eB6mGeLhPSP3hXrcrtRuqQ.jpeg?q=20" width="217" height="217" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="217" height="217" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/434/1*eB6mGeLhPSP3hXrcrtRuqQ.jpeg" width="217" height="217" role="presentation"></noscript></div></div></div></figure><p id="7090" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">This post was written by Valdir Stumm( <a href="https://twitter.com/stummjr" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">@stummjr</a>), a developer at Scrapinghub.</p><p id="7927" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Please heart the “Recommend” so that others can learn more about how to use Scrapy politely.</p><p id="220a" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx"><a href="https://scrapinghub.com/data-services/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow"><strong class="gs he">Learn more about what web scraping and web data can do for you</strong></a><strong class="gs he">.</strong></p><p id="2a93" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Originally published on the <a href="https://blog.scrapinghub.com/2016/08/25/how-to-crawl-the-web-politely-with-scrapy/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapinghub blog</a>.</p></div></div><div class="hp"><div class="n p"><div class="kr ks kt ku kv kw ag kx ah ky aj ak"><div class="hk hl hm hn ho n kz"><figure class="la hp lb lc ld le lf paragraph-image"><a href="http://bit.ly/HackernoonFB"><div class="hv r hw hx"><div class="lg r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*0hqOaABQ7XGPT-OYNgiUBg.png?q=20" width="1136" height="572" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="1136" height="572" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/2272/1*0hqOaABQ7XGPT-OYNgiUBg.png" width="1136" height="572" role="presentation"></noscript></div></div></a></figure><figure class="la hp lb lc ld le lf paragraph-image"><a href="https://goo.gl/k7XYbx"><div class="hv r hw hx"><div class="lg r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*Vgw1jkA6hgnvwzTsfMlnpg.png?q=20" width="1136" height="572" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="1136" height="572" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/2272/1*Vgw1jkA6hgnvwzTsfMlnpg.png" width="1136" height="572" role="presentation"></noscript></div></div></a></figure><figure class="la hp lb lc ld le lf paragraph-image"><a href="https://goo.gl/4ofytp"><div class="hv r hw hx"><div class="lg r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*gKBpq1ruUi0FVK2UM_I4tQ.png?q=20" width="1136" height="572" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="1136" height="572" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/2272/1*gKBpq1ruUi0FVK2UM_I4tQ.png" width="1136" height="572" role="presentation"></noscript></div></div></a></figure></div></div></div></div><div class="n p"><div class="ac ae af ag ah ec aj ak"><blockquote class="iu iv iw"><p id="f922" class="gq gr ef ix gs b gt gu gv gw gx gy gz ha hb hc hd dx"><a href="http://bit.ly/Hackernoon" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Hacker Noon</a> is how hackers start their afternoons. We’re a part of the <a href="http://bit.ly/atAMIatAMI" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">@AMI</a> family. We are now <a href="http://bit.ly/hackernoonsubmission" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">accepting submissions</a> and happy to <a href="mailto:partners@amipublications.com" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">discuss advertising &amp; sponsorship</a> opportunities.</p><p id="708a" class="gq gr ef ix gs b gt gu gv gw gx gy gz ha hb hc hd dx">If you enjoyed this story, we recommend reading our <a href="http://bit.ly/hackernoonlatestt" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">latest tech stories</a> and <a href="https://hackernoon.com/trending" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">trending tech stories</a>. Until next time, don’t take the realities of the world for granted!</p></blockquote></div></div><div class="hp ak"><figure class="hk hl hm hn ho hp ak paragraph-image"><a href="https://goo.gl/Ahtev1"><div class="hv r hw hx"><div class="lh r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*35tCjoPcvq6LbB3I6Wegqw.jpeg?q=20" width="15000" height="1800" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="15000" height="1800" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/30000/1*35tCjoPcvq6LbB3I6Wegqw.jpeg" width="15000" height="1800" role="presentation"></noscript></div></div></a></figure></div></section>