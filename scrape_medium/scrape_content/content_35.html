<figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="cl cm fm"><div class="fy r fz ga"><div class="gb r"><div class="ft fu cp t u fv ak eh fw fx"><img class="cp t u fv ak gc gd ge" src="https://miro.medium.com/max/60/1*9hUizS9cheTAYEH1NhkUDA.png?q=20" width="700" height="400" role="presentation"></div><img class="ft fu cp t u fv ak gf" width="700" height="400" role="presentation"><noscript><img class="cp t u fv ak" src="https://miro.medium.com/max/1400/1*9hUizS9cheTAYEH1NhkUDA.png" width="700" height="400" role="presentation"></noscript></div></div></div></figure><p id="a0db" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Ever felt frustrated at how long your web scraping script takes to complete the task? Have you ever wished there was a faster way to do your web scraping?</p><p id="a5c7" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Well, there is. And I’m going to show you today how you can increase the performance of your scraper in a very beginner friendly way.</p><p id="cc6f" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">In this post we will also talk about asynchronous programming in Python. And then apply that knowledge to optimize web scraping.</p><p id="9656" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Let’s dive in!</p><h2 id="b269" class="gu gv dc bk bj gw gx gy gz ha hb hc hd he hf hg hh"><strong class="az">What is Asynchronous execution? And why would you want it?</strong></h2><p id="6512" class="gg gh dc bk gi b gj hi gl hj gn hk gp hl gr hm gt cu">If you’re a beginner in web scraping, then I assume you’ve worked with <code class="ga hn ho hp hq b">requests</code> and <code class="ga hn ho hp hq b">BeautifulSoup</code> modules in python. And what you generally do while writing your scraper is as follows —</p><pre class="fn fo fp fq fr hr hs ht"><span id="37fe" class="gu gv dc bk hq b eg hu hv r hw">def parse(soup):<br>    # Extract data<br>    # return data</span><span id="3bf5" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">urls = [...]</span><span id="daaa" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">results = []<br>for url in urls:<br>    r = requests.get(url)<br>    soup = BeautifulSoup(r.content, 'lxml')<br>    results.append(parse(soup))</span></pre><p id="1946" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Or you might use a different structure than this. But the end result is same. The way you code your scraper is in a <em class="ic">synchronous</em> fashion.</p><p id="f4f6" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">What it means is that your program goes through the target URLs one by one, in a synchronized way. You send a GET request to the server and the server takes some time to send a response. But what do you suppose is happening while your program is waiting for a response from the server?</p><p id="ffde" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu"><strong class="gi id">Nothing!</strong></p><p id="3097" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">That’s right. The network request is the instruction that takes the most time in your script. And when you’re doing it in a synchronous way, your script remains idle a large amount of time which is spent waiting for the server response. How would you make use of that free time?</p><p id="7301" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">It’s quite obvious. We do not want our program to remain idle while one of the GET requests is waiting for server’s response. We want our program to move ahead with other URLS and their processing without being blocked due to one sluggish network request.</p><blockquote class="ie if ig"><p id="5109" class="gg gh dc ic gi b gj gk gl gm gn go gp gq gr gs gt ih cu">A<!-- -->synchronous programming is simply executing multiple instructions simultaneously.</p></blockquote><p id="61c9" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So we need a way to process multiple URLs simultaneously and independent of one another. Let’s see how we can achieve this in Python.</p><h2 id="4872" class="gu gv dc bk bj gw gx gy gz ha hb hc hd he hf hg hh">How asynchronous execution is achieved?</h2><p id="ff62" class="gg gh dc bk gi b gj hi gl hj gn hk gp hl gr hm gt cu">In this section, I will discuss different strategies of asynchronous execution. If you’re just interested in the asynchronous python code, you can skip this part.</p><p id="da97" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">There are many ways in which asynchronous execution is implemented. Three broad categories of multi-processing can be given as —</p><ul class=""><li id="bee7" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt ii ij ik">Process level multi-processing.</li><li id="1448" class="gg gh dc bk gi b gj il gl im gn in gp io gr ip gt ii ij ik">Thread level multi-processing.</li><li id="8cab" class="gg gh dc bk gi b gj il gl im gn in gp io gr ip gt ii ij ik">Application level multi-processing.</li></ul><p id="5ca6" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">If you have some background in Unix operating system, you would be familiar with these concepts. Still, I will do my best to explain them as concisely and cogently as possible.</p><p id="05ab" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">In Process level multi-processing, you can achieve asynchronous execution by dividing the total work across separate processes. Each process running on a different processor core. In this way, your original task is divided into number of chunks and all of these chunks are being processed simultaneously. This level of multi-processing is in-built in an OS. So all you have to do is utilize this and let the kernel worry about process scheduling.</p><p id="3571" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Thread level multi-processing is almost same as the previous one. Except in this case, we are dividing the task across multiple threads. A thread is like a process but a lightweight process. And we can add multiple threads under a single process context. So all of these thread would belong to the same process. This feature is also implemented in the OS itself. We just need to utilize this using Python and we will see how it is done.</p><p id="e433" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Application level multi-processing is somewhat different than the previous two. Here the OS is under the impression that it is executing only one process with a single thread. But our application itself schedules different tasks on that thread for execution. So the asynchronous nature of execution is implemented in our application program itself.</p><p id="5a97" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">These are the main ways to handle parallel execution on a traditional Unix system. Now we will see how we can use the <code class="ga hn ho hp hq b">concurrent</code> module in Python to utilize these concepts and to boost our scraping speed.</p><h2 id="bb08" class="gu gv dc bk bj gw gx gy gz ha hb hc hd he hf hg hh">Implementing asynchronous execution:</h2><p id="130d" class="gg gh dc bk gi b gj hi gl hj gn hk gp hl gr hm gt cu">Okay, so you must be itching to get started. Let’s start coding —</p><pre class="fn fo fp fq fr hr hs ht"><span id="ddeb" class="gu gv dc bk hq b eg hu hv r hw">from concurrent.futures import ProcessPoolExecutor<br>from concurrent.futures import ThreadPoolExecutor<br>from concurrent.futures import Future </span></pre><p id="f4e2" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So we first import the things we require. You will observe that we imported <code class="ga hn ho hp hq b">ProcessPoolExecutor</code> and <code class="ga hn ho hp hq b">ThreadPoolExecutor.</code> Both of these classes correspond to Process level and Thread level multi-processing respectively. We only need to use one of these. And for our use case i.e web scraping, both of these will be effective.</p><p id="7da5" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So the multi-processing features in the OS are abstracted and we can directly do parallel processing using the above classes.</p><blockquote class="ie if ig"><p id="deef" class="gg gh dc ic gi b gj gk gl gm gn go gp gq gr gs gt cu">The <code class="ga hn ho hp hq b"><a href="https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures" class="at cg iq ir is it" target="_blank" rel="noopener nofollow">concurrent.futures</a></code> module provides a high-level interface for asynchronously executing callables.</p><p id="99bb" class="gg gh dc ic gi b gj gk gl gm gn go gp gq gr gs gt cu">The asynchronous execution can be performed with threads, using <code class="ga hn ho hp hq b"><a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor" class="at cg iq ir is it" target="_blank" rel="noopener nofollow">ThreadPoolExecutor</a></code>, or separate processes, using <code class="ga hn ho hp hq b"><a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor" class="at cg iq ir is it" target="_blank" rel="noopener nofollow">ProcessPoolExecutor</a></code>.</p></blockquote><p id="0d75" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">The way it works is that we have a <strong class="gi id">pool </strong>of threads or processes. And we can assign some task to each of them and they will start executing independently of each other.</p><p id="5c05" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">We can create a pool —</p><pre class="fn fo fp fq fr hr hs ht"><span id="580a" class="gu gv dc bk hq b eg hu hv r hw">pool = ThreadPoolExecutor(3) # This means a pool of 3 threads<br>            OR<br>pool = ProcessPoolExecutor(3) # This means a pool of 3 processes </span></pre><p id="0031" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Now we can <code class="ga hn ho hp hq b">submit</code> or <code class="ga hn ho hp hq b">map</code> different tasks to each individual thread or process.</p><p id="3519" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Suppose we have a list of 100 URLs and we want to download the HTML page for each URL and do some post-processing and extract data.</p><pre class="fn fo fp fq fr hr hs ht"><span id="92a3" class="gu gv dc bk hq b eg hu hv r hw">def download_and_extract(url):<br>    r = requests.get(url)<br>    soup = BeautifulSoup(r.content, 'lxml')<br>    # Some data extraction logic<br>    return data</span></pre><p id="a4a3" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">We have a function <code class="ga hn ho hp hq b">download_and_extract</code> which will gather our data and we want to gather data from the 100 URLs previously mentioned.</p><p id="ae52" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">If we were to do this synchronously, it would take 100 multiplied by average time for one GET request ( assuming post-processing time is trivial ). But instead if we divide the 100 URLs on 4 separate threads/processes, then the time required would be 1/4th the original time, at least theoretically.</p><p id="4480" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So let us try this —</p><pre class="fn fo fp fq fr hr hs ht"><span id="022c" class="gu gv dc bk hq b eg hu hv r hw">URLs = [...]</span><span id="041e" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">def d_and_e(url): # Our download and extract function<br>    ....</span><span id="3abd" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">with ProcessPoolExecutor(max_workers=4) as executor:<br>    futures = [ executor.submit(d_and_e, url) for url in URLs]</span></pre><p id="9bd7" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Here we have slightly modified the Pool initialization to suit our use case but it does the same thing when we initialized it previously.</p><p id="dc42" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu"><code class="ga hn ho hp hq b">executor.submit</code> function takes two parameters in our code. The first one is the task we want to perform Or more technically, the function we want to execute and the parameters for the execution of our function. The executor will distribute the work across 4 different processes with each process executing one instance of <code class="ga hn ho hp hq b">download_and_extract</code> for the given URL.</p><p id="e554" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">But how do we know when the tasks are done? And what about the data that we wanted?</p><p id="76b9" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu"><code class="ga hn ho hp hq b">executor.submit</code> returns a <code class="ga hn ho hp hq b">Future</code> object.</p><blockquote class="ie if ig"><p id="70a9" class="gg gh dc ic gi b gj gk gl gm gn go gp gq gr gs gt cu">(Future) Encapsulates the asynchronous execution of a callable.</p></blockquote><p id="2f7a" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">This object represents the asynchronous execution of a specific function. You can read more about its properties in the <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Future" class="at cg iq ir is it" target="_blank" rel="noopener nofollow">documentation</a>. We will only focus on two main functions for this object that we will require viz. <code class="ga hn ho hp hq b">done</code> and <code class="ga hn ho hp hq b">result.</code></p><p id="4e3c" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu"><code class="ga hn ho hp hq b">done()</code> function returns the bool value <code class="ga hn ho hp hq b">True</code> if the function has finished executing or if there was some exception in it. And when it has finished execution, we can retrieve the result using the <code class="ga hn ho hp hq b">result()</code> function.</p><pre class="fn fo fp fq fr hr hs ht"><span id="b25f" class="gu gv dc bk hq b eg hu hv r hw">URLs = [...]</span><span id="37cf" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">def d_and_e(url): # Our download and extract function<br>    ....</span><span id="f7f0" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">with ProcessPoolExecutor(max_workers=4) as executor:<br>    futures = [ executor.submit(d_and_e, url) for url in URLs]</span><span id="8b5a" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">    results = []<br>    for result in concurrent.futures.as_completed(futures):<br>        results.append(result)</span></pre><p id="a20e" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Here’s another new thing — <code class="ga hn ho hp hq b">as_completed().</code></p><p id="df86" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">The function <code class="ga hn ho hp hq b">as_completed()</code> simply determines the order of the results that are returned by the future. Using this function, we avoid having to write a block of code where we keep checking whether a given <code class="ga hn ho hp hq b">Future</code> is <code class="ga hn ho hp hq b">done()</code> or not.</p><p id="8111" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">The function will start generating results as soon as any one of the functions being executed yields some result. And then we simply append that result to our main collection of data.</p><p id="a681" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">And that’s it! Using these simple concepts you can make your program multi-processing capable. Web scraping is just a simple example to illustrate the concept. You can apply this concept anywhere you want.</p><p id="aded" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">We will look at a fully coded and working example below —</p><pre class="fn fo fp fq fr hr hs ht"><span id="f9fd" class="gu gv dc bk hq b eg hu hv r hw">import time<br>import requests<br>from bs4 import BeautifulSoup<br>from concurrent.futures import ProcessPoolExecutor, as_completed</span><span id="907a" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">URLs = [ ... ] # A long list of URLs.</span><span id="8fa1" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">def parse(url):<br>    r = requests.get(url)<br>    soup = BeautifulSoup(r.content, 'lxml')<br>    return soup.find_all('a')</span><span id="d4c0" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">with ProcessPoolExecutor(max_workers=4) as executor:<br>    start = time.time()<br>    futures = [ executor.submit(parse, url) for url in URLs ]<br>    results = []<br>    for result in as_completed(futures):<br>        results.append(result)<br>    end = time.time()<br>    print("Time Taken: {:.6f}s".format(end-start))</span></pre><p id="a48c" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">You can now experiment using this example with URLs of your choice and different degrees of parallelization. See what conclusions you can draw from this.</p><h2 id="9dad" class="gu gv dc bk bj gw gx gy gz ha hb hc hd he hf hg hh">What next?</h2><p id="d77c" class="gg gh dc bk gi b gj hi gl hj gn hk gp hl gr hm gt cu">In this post, I demonstrated how to divide a particular task across multiple threads and process. And we achieved asynchronous execution of a specific task in this way.</p><p id="5b04" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">But think about this, the task we are doing i.e downloading data from the network, it is admittedly being done across multiple processes but on any one process the task is still being done synchronously.</p><p id="93b5" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">What I mean is that we are simply performing the task in a parallel fashion. So in any one of the threads/processes, that one process or thread still remains idle for some time until server responds.</p><p id="0ae2" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">There is a way in which we can overcome this and make our scraping truly asynchronous. We would have to use Application level multi-processing to accomplish this.</p><p id="a889" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">We want our program to send a GET request and while the server is processing that request, we want our program to suspend that request and move on to next requests. When the server finally responds, we want that data to be mapped to the correct request. In this way, we do not allow our program to remain idle at all. It is always doing something.</p><p id="c11b" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">This is possible in python using <code class="ga hn ho hp hq b">asyncio</code> and <code class="ga hn ho hp hq b">aiohttp</code> modules. I will explore both of those modules in the context of web scraping in a future post.</p><p id="2048" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So stay tuned!</p>