<figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm fm"><div class="gc r fv gd"><div class="ge r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*a4xWpmTJ14YsMVDg0Wwlbw.jpeg?q=20" width="720" height="566" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="720" height="566" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/1440/1*a4xWpmTJ14YsMVDg0Wwlbw.jpeg" width="720" height="566" role="presentation"></noscript></div></div></div></div></figure><p id="c5a4" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">As usual the first set of questions always go like this, what is web scraping? What is the usefulness? And how do I do it? Now, to answer the first two questions with the simplest of words, web scraping is simply the collection of specific data or information from a web site or a simple web page, to which this information or data could be used for analysis or whatever the web scraper needs such information for. Several programming languages can be used for web scraping, but as stated above we would be using the python programming language to scrape a web site. How do I do it? lets get right to it with a simple example. First, it would be a good thing to note that one of the languages used in building a website is the Hyper Text Mark-up Language(HTML). HTML contains large amount of data in text form. To scrape data from a web site, we would use the beautifulsoup4 from the bs4 python library and the lxml parser( there are other types of parsers but we would be using ‘lxml’ for this example). These tools are way more preferable, very helpful and easy to use when it comes to web scraping.</p><p id="ee7d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Getting Started:</strong></p><p id="f761" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You would need to create a folder, after which you create a virtual environment in that folder, then install these tools and libraries in the virtual environment. All these steps would be done in the command prompt using pip.</p><pre class="fn fo fp fq fr gy gz ha"><span id="9890" class="hb hc dc bk hd b eg he hf r hg">#on the command prompt<br>1) mkdir work<br>2) cd work <br>3) virtualenv work_flow_env <br>4) work_flow_env\Scripts\activate<br>5) pip install beautifulsoup4<br>6) pip install lxml<br>7) pip install html5lib #optional<br>8) pip install requests</span><span id="c3fd" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg"># explanation of code lines:</span><span id="89b1" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">line 1 creates a folder named work<br>line 2 opens the folder<br>line 3 creates a virtual environment named work_flow_env in that folder<br>line 4 activates the virtual environment(this code is for a windows os)<br>line 5,6,7 and 8 installs the tools and libraries needed for web scraping.</span></pre><p id="0ab1" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">I will assume that most of the readers have an idea even if it’s a little knowledge on HTML, but if you have none, you could always skim through a good free source website, in which I will recommend “w3schools.com”.</p><p id="5f6c" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now if you are using sublime text, all you have to do is drag your folder named “work’’ into the sublime text application and you are ready to code. If you have a well downloaded anaconda application, jupyter notebook has all these installed, so you would not have to go through the ‘Getting started’ phase.</p><p id="ef37" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Our task is really simple, we are to get the name of movies from ‘<a href="http://toxicwap.com/New_Movies/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow">http://toxicwap.com/New_Movies/</a>’ and their links. In your already set compiler, you import the libraries.</p><p id="8bce" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Importing Libraries:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="970c" class="hb hc dc bk hd b eg he hf r hg">from bs4 import BeautifulSoup<br>import requests<br></span></pre><p id="5d38" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Getting the raw data:</strong></p><p id="26ed" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">After importing the libraries, you get the information in text format from the web page using requests.get().text</p><pre class="fn fo fp fq fr gy gz ha"><span id="209b" class="hb hc dc bk hd b eg he hf r hg">source = requests.get('<a href="http://toxicwap.com/New_Movies/').text" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow">http://toxicwap.com/New_Movies/').text</a> </span></pre><p id="d459" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Parsing:</strong></p><p id="2132" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now, you have gotten the information needed, so you parse through the text using lxml, you make it clean and readable using prettify().</p><pre class="fn fo fp fq fr gy gz ha"><span id="1e7e" class="hb hc dc bk hd b eg he hf r hg">soup = BeautifulSoup(source, 'lxml')<br>print(soup.prettify())</span></pre><p id="2c73" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Inspecting the web page:</strong></p><p id="d321" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Go back to the web page and inspect it( you right click and and select inspect), you then navigate the web page from the source code seen, you navigate to the point you are able to highlight the part you want to scrape. When you have gotten all you need, you go back to your code then look through your “prettified” text. When going through your cleaned up data(prettified text) you would see the code you highlighted from the web page, depending on the site you are scraping you may have to dig a lot deeper before getting to what you want.</p><p id="9183" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Digging(navigating) through the text data:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="e272" class="hb hc dc bk hd b eg he hf r hg">div = soup.find('div', attrs={'data-role':'content'})<br>ul = div.find('ul')<br>li = ul.find('li')<br>print(li)</span></pre><p id="88f4" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">To explain the code. First, if you are doing exactly what I am doing, when you inspect the web page you would notice the tags are mostly “div”, now line 1 selects the particular div that holds the content you want to scrape. Line 2 digs deeper into the div to the ul(unordered list) and line 3 digs into the ul to the li(list). Well, We all know what line 4 does( it displays the list).</p><p id="e4c1" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Note: some sources online use ‘class_=()’ when trying to get to a particular div, but you would notice that in this particular case the prettified text did not display the class of the div, hence resulting to the use of ‘attrs ={}’.</p><pre class="fn fo fp fq fr gy gz ha"><span id="ac83" class="hb hc dc bk hd b eg he hf r hg">title = li.a.text<br>print(title)<br>link = li.a<br>print(link['href'])</span></pre><p id="ac17" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now, the first line says put in the variable named title the text which can be found in the ‘a’ tag(&lt;a&gt;: link tag in HTML), which is also found in the ‘li’ tag. It is literally just digging from ‘li’ into ‘a’ to the ‘text’. From what I have said, you should be able to interpret the third line. Basically, you are already done, but this written code will get you just the first title and the first link, to get all the titles and links you use a for loop.</p><p id="c1d2" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Displaying the whole output:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="ec8f" class="hb hc dc bk hd b eg he hf r hg">for li in ul.find_all('li'):<br>    title = li.a.text<br>    print(title)<br>    <br>    link =li.a<br>    print(link['href'])</span></pre><p id="4df0" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Notice how .find() changed to .find_all()? That’s what you do when you want to get all the data needed. It is good practice to use .find() first when trying to navigate, then when the code format is gotten you use the .find_all() to get the data remaining. So now the whole code should look like this.</p><p id="7fa8" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Complete code:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="6ee6" class="hb hc dc bk hd b eg he hf r hg">from bs4 import BeautifulSoup<br>import requests</span><span id="8d5b" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">source = requests.get('<a href="http://toxicwap.com/New_Movies/').text" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow">http://toxicwap.com/New_Movies/').text</a><br>soup = BeautifulSoup(source, 'lxml')</span><span id="28cb" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">div = soup.find('div', attrs={'data-role':'content'})<br>ul = div.find('ul')<br>li = ul.find('li')</span><span id="4bdb" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">for li in ul.find_all('li'):<br>    title = li.a.text<br>    print(title)<br>    <br>    link = li.a<br>    print(link['href'])<br>    print()<br>    <br> </span></pre><p id="a6be" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You are done, but if you want to save the scrapped data into a text file or csv file, you can. I’ll be saving this into a csv file.</p><p id="bd7b" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Saving in a format:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="f4a3" class="hb hc dc bk hd b eg he hf r hg">from bs4 import BeautifulSoup<br>import requests<br>import csv</span><span id="9fe5" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">source = requests.get('<a href="http://toxicwap.com/New_Movies/').text" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow">http://toxicwap.com/New_Movies/').text</a><br>soup = BeautifulSoup(source, 'lxml')</span><span id="971e" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">div = soup.find('div', attrs={'data-role':'content'})<br>ul = div.find('ul')<br>li = ul.find('li')</span><span id="0c7c" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">file = open('s_data.csv', 'w')<br>file_writer = csv.writer(file)<br>file_writer.writerow(['Titles','Links'])</span><span id="d0a7" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">for li in ul.find_all('li'):<br>    title = li.a.text<br>    print(title)<br>    <br>    link = li.a<br>    print(link['href'])<br>    print()<br>    <br>    file_writer.writerow([title, link ])</span><span id="1f55" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">file.close()</span></pre><p id="0274" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">I would like to add that some websites make it really hard to scrape their page and for some it is illegal to scrape their page.</p><p id="885c" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">That is it. It is all done. I guess I could say you just learnt how to scrape a website.</p>