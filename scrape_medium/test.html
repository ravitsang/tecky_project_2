<div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="0fac" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How Xpath Plays Vital Role In Web Scraping Part 2</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@sandra_21783?source=post_page-----fd32e6c45c65----------------------"><img alt="Sandra K" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/2*mIH5vdwhAOewjJjvRqoZ1Q.png" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@sandra_21783?source=post_page-----fd32e6c45c65----------------------">Sandra K</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@sandra_21783/how-xpath-plays-vital-role-in-web-scraping-part-2-fd32e6c45c65?source=post_page-----fd32e6c45c65----------------------">Aug 26, 2016</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/fd32e6c45c65/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/fd32e6c45c65/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40sandra_21783%2Fhow-xpath-plays-vital-role-in-web-scraping-part-2-fd32e6c45c65&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="3f21" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">To read the first part of this blog do read:</p><div class="ga gb gc gd ge gf"><a href="https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping/" rel="noopener nofollow"><div class="gi n ar"><div class="gj n co p gk gl"><h2 class="bj gm gn bl dc"><div class="eh gg ej ek gh em">How Xpath Plays Vital Role In Web Scraping - Data hut</div></h2><div class="go r"><h3 class="bj ef eg bl bo"><div class="eh gg ej ek gh em">XPath is a language for finding information in structured documents like XML or HTML. You can say that XPath is (sort…</div></h3></div><div class="gp r"><h4 class="bj ef fb bl bo"><div class="eh gg ej ek gh em">blog.datahut.co</div></h4></div></div><div class="gq r"><div class="gr r gs gt gu gq gv gw gx"></div></div></div></a></div><p id="f801" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Here is a piece of content on Xpaths which is the follow up of <a href="https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping/" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">How Xpath Plays Vital Role In Web Scraping</a></p><p id="f410" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Let’s dive into a real-world example of scraping amazon website for getting information about deals of the day. Deals of the day in amazon can be found at this . So navigate to the (deals of the day) in Firefox and find the XPath selectors. Right click on the deal you like and select “Inspect Element with Firebug”:</p><p id="7188" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">If you observe the image below keenly, there you can find the source of the image(deal) and the name of the deal in src, alt attribute’s respectively. So now let’s write a generic XPath which gathers the name and image source of the product(deal). //img[@role=”img”]/@src ## for image source //img[@role=”img”]/@alt ## for product name</p><p id="87f2" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">In this post, I’ll show you some tips we found valuable when using XPath in the trenches.</p><p id="094b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">If you have an interest in Python and web scraping, you may have already played with the nice <a href="http://docs.python-requests.org/" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">requests library </a>to get the content of pages from the Web. Maybe you have toyed around using <a href="http://doc.scrapy.org/en/latest/topics/selectors.html" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">Scrapy selector </a>or to make the content extraction easier. Well, now I’m going to show you some tips I found valuable when using XPath in the trenches and we are going to use both and <a href="http://doc.scrapy.org/en/latest/topics/selectors.html" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">Scrapy selector </a>for HTML parsing.</p><p id="20df" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Avoid using expressions which contains(.//text(), ‘search text’) in your XPath conditions. Use contains(., ‘search text’) instead.</p><p id="4019" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Here is why: the expression .//text() yields a collection of text elements — a node-set(collection of nodes).and when a node-set is converted to a string, which happens when it is passed as argument to a string function like contains() or starts-with(), results in the text for the first element only.</p><p id="d30f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo hc">Scrapy Code:</strong></p><p id="4a87" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">from scrapy import Selector<br> html_code = “””&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;”””<br> sel = Selector(text=html_code)<br> print xp(‘//a//text()’)<br> xp = lambda x: sel.xpath(x).extract() # Let’s type this only once # Take a peek at the node-set<br> [u’Click here to go to the ‘, u’Next Page’] # output of above command<br> print xp(‘string(//a//text())’) # convert it to a string # output of the above command<br> [u’Click here to go to the ‘]</p><p id="7c9f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Let’s do the above one by using lxml then you can implement XPath by both lxml or Scrapy selector as XPath expression is same for both methods.</p><p id="8134" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo hc">lxml code:</strong></p><p id="c85b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">from lxml import html <br> html_code = “””&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;””” # Parse the text into a tree<br> parsed_body = html.fromstring(html_code) # Perform xpaths on the tree<br> print parsed_body(‘//a//text()’) # take a peek at the node-set<br> [u’Click here to go to the ‘, u’Next Page’] # output<br> print parsed_body(‘string(//a//text())’) # convert it to a string<br> [u’Click here to go to the ‘] # output</p><p id="93f2" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">A node converted to a string, however, puts together the text of itself plus of all its descendants:</p><p id="c04d" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp(‘//a[1]’) # selects the first a node<br> [u’&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;’]</p><p id="fdb6" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp(‘string(//a[1])’) # converts it to string<br> [u’Click here to go to the Next Page’]</p><p id="e8e3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Beware of the difference between //node[1] and (//node)[1]//node[1] selects all the nodes occurring first under their respective parents and (//node)[1] selects all the nodes in the document, and then gets only the first of them.</p><p id="5985" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">from scrapy import Selector</p><p id="e668" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">sel = Selector(text=html_code) <br> xp = lambda x: sel.xpath(x).extract()</p><p id="ed97" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">xp(“//li[1]”) # get all first LI elements under whatever it is its parent</p><p id="6e2a" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">xp(“(//li)[1]”) # get the first LI element in the whole document</p><p id="1c09" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">xp(“//ul/li[1]”) # get all first LI elements under an UL parent</p><p id="c35c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">xp(“(//ul/li)[1]”) # get the first LI element under an UL parent in the document</p><p id="0f99" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">//a[starts-with(@href, ‘#’)][1] gets a collection of the local anchors that occur first under their respective parents and (//a[starts-with(@href, ‘#’)])[1] gets the first local anchor in the document.</p><p id="2399" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">When selecting by class, be as specific as necessary.</p><p id="f85c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">If you want to select elements by a CSS class, the XPath way to do the same job is the rather verbose:</p><p id="dd8e" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo hc">*[contains(concat(‘ ‘, normalize-space(@class), ‘ ‘), ‘ someclass ‘)]</strong></p><p id="7744" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Let’s cook up some examples:</p><p id="b2d6" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; sel = Selector(text=’&lt;p class=”content-author”&gt;Someone&lt;/p&gt;&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’)</p><p id="855f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp = lambda x: sel.xpath(x).extract()</p><p id="6015" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">BAD: because there are multiple classes in the attribute</p><p id="350f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">[]</p><p id="497f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">BAD: gets more content than we need</p><p id="3836" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp(“//*[contains(@class,’content’)]”)</p><p id="5613" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">[u’&lt;p class=”content-author”&gt;Someone&lt;/p&gt;’, u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</p><p id="af4a" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp(“//*[contains(concat(‘ ‘, normalize-space(@class), ‘ ‘), ‘ content ‘)]”) <br> [u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</p><p id="7636" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">And many times, you can just use a CSS selector instead, and even combine the two of them if needed:</p><p id="1a5d" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; sel.css(“.content”).extract() <br> [u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</p><p id="783c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; sel.css(‘.content’).xpath(‘@class’).extract() <br> [u’content text-wrap’]</p><p id="b107" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Learn to use all the different axes.</p><p id="ef7b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">It is handy to know how to use the axes, you can follow through these examples .</p><p id="f297" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">In particular, you should note that following and following-sibling are not the same thing, this is a common source of confusion. The same goes for preceding and preceding-sibling, and also ancestor and parent.</p><p id="c9ad" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo hc">Useful trick to get text content</strong></p><p id="cc03" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Here is another XPath trick that you may use to get the interesting text contents:</p><p id="8287" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">//*[not(self::script or self::style)]/text()[normalize-space(.)]</p><p id="f0a3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">This excludes the content from the script and style tags and also skip whitespace-only text nodes.</p><p id="0eb1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Tools &amp; Libraries Used:</p><p id="f0d3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Firefox<br> Firefox inspect element with firebug<br> Scrapy : 1.1.1<br> Python : 2.7.12<br> Requests : 2.11.0</p><p id="4d3c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Have questions? Comment below. Please share if you found this helpful.</p><p id="7c95" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Read the original article here: <a href="https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping-part-2/" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping-part-2/</a></p></div></div></section><hr class="hd ef he hf hg hh hi hj hk hl hm"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="1b20" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><em class="hn">Originally published at </em><a href="https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping-part-2/" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow"><em class="hn">https://blog.datahut.co</em></a><em class="hn"> on August 26, 2016.</em></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="207f" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">100 Days of Code — Day 5 of 100</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@victoria2666?source=post_page-----c368583a1b2c----------------------"><img alt="Victoria Lo" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/2*ukM4UJE841N2TQ4YoDkTWQ.png" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@victoria2666?source=post_page-----c368583a1b2c----------------------">Victoria Lo</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@victoria2666/100-days-of-code-day-5-of-100-c368583a1b2c?source=post_page-----c368583a1b2c----------------------">Feb 7</a> <!-- -->·<!-- --> <!-- -->2<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewbox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/c368583a1b2c/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/c368583a1b2c/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40victoria2666%2F100-days-of-code-day-5-of-100-c368583a1b2c&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="a5b3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Today’s project is a web scraper! I have always been curious about what web scraping is about and how to do it.</p><p id="6a07" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Apparently, after hours of researching, there is an easy way to do it using Puppeteer. I followed a youtube tutorial closely and I got it done in less than an hour! Yay! Or so I thought…</p><p id="5101" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">My initial plan was to scrape some data then display it on a HTML page. So as usual, I attached &lt;script&gt; to my HTML but something went very wrong…</p><p id="66ae" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">ERROR: ‘require is not defined’. Oh boy, I thought. So I researched what this error is about and apparently the keyword require cannot be used for client-side execution. In other words, no browsers. Boo.</p><p id="562e" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">It took me another 2 hours and more to figure out what to do from here. Am I satisfied with just this back-end but completed web scraper? Or do I want a page too? After browsing and reading about browserify, I decided to have a page! But… Oh dear. Issues after issues that I don’t understand. After researching more, I’m back to square one — which is having no page because apparently, Browserify and Puppeteer don’t like each other…</p><p id="9d2a" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Ok, so fine, I thought. Let’s just push this to gitHub without the front-end… ERROR! File exceeded 100MB! *slaps face* Nothing seems to be going right today… It turns out that the “node_modules” folder which contains the Puppeteer module is over 145MB and I honestly have no idea why it is so large so I deleted it and put it in the README.md. The long day seems to finally come to an end.</p><p id="97d6" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">But wait! The front-end is not complete. All I have now is some data scraped by scraper.js. I can’t let it go to waste! So I save them to the JSON file while learning about File System in Nodejs. Very handy! After saving the JSON, I load it up to a HTML page into a table dynamically (learned from <a class="at cg ga gb gc gd" target="_blank" rel="noopener" href="/@victoria2666/100-days-of-code-day-3-of-100-d2141c4e7932">Day 3</a>)! BAM! Front and back now all covered and this noob feels accomplished for the day.</p><p id="1c7f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo ge">The Project: </strong><a href="https://victoria-lo.github.io/GameScraper/" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">GameScraper</a></p><p id="90bd" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo ge">What I Learn:</strong></p><ul class=""><li id="54f3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gf gg gh">WEB SCRAPING!</li><li id="c030" class="fm fn dc bk fo b fp gi fr gj ft gk fv gl fx gm fz gf gg gh">Save data to local JSON using File System module</li><li id="1c79" class="fm fn dc bk fo b fp gi fr gj ft gk fv gl fx gm fz gf gg gh">Using Puppeteer and how large it is in memory</li><li id="beec" class="fm fn dc bk fo b fp gi fr gj ft gk fv gl fx gm fz gf gg gh">GitHub’s size limit is 100MB (good to know)</li><li id="11cd" class="fm fn dc bk fo b fp gi fr gj ft gk fv gl fx gm fz gf gg gh">There’s too much StackOverflow forums addressing the same issues/problems that my brain got overloaded</li></ul><p id="fe66" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo ge">What I Did Not Learn:</strong></p><ul class=""><li id="2db1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gf gg gh">Browserify</li><li id="a61d" class="fm fn dc bk fo b fp gi fr gj ft gk fv gl fx gm fz gf gg gh">How to make require() work for client-side</li><li id="3eda" class="fm fn dc bk fo b fp gi fr gj ft gk fv gl fx gm fz gf gg gh">Why Puppeteer is so large</li></ul><p id="08b3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo ge">Thoughts:</strong></p><p id="a6cd" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Today was exhausting because it felt like I made no progress ever since I got the scraper running. The scraper is the main topic I want to learn today so actually, I could have been done within an hour but I just had to be all ambitious and research stuff. But it all ended in vain so it felt exhausting to me. Overall, I am still glad that I learnt how to scrape data from other sites.</p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="7fb7" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Scrapping the content of single-page application (SPA) with headless Chrome and puppeteer</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@andrejsabrickis?source=post_page-----d040025f752b----------------------"><img alt="Andrejs Abrickis" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/2*0kyh9TAgRWsvoE9U4BOFnw.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@andrejsabrickis?source=post_page-----d040025f752b----------------------">Andrejs Abrickis</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@andrejsabrickis/scrapping-the-content-of-single-page-application-spa-with-headless-chrome-and-puppeteer-d040025f752b?source=post_page-----d040025f752b----------------------">Jan 14, 2019</a> <!-- -->·<!-- --> <!-- -->5<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/d040025f752b/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/d040025f752b/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40andrejsabrickis%2Fscrapping-the-content-of-single-page-application-spa-with-headless-chrome-and-puppeteer-d040025f752b&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm fm"><div class="gc r fv gd"><div class="ge r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/0*Nm6YR258nleQ3_PN?q=20" width="6000" height="4000" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="6000" height="4000" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/12000/0*Nm6YR258nleQ3_PN" width="6000" height="4000" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg gj gk gl cn cl cm gm gn bj ef">Photo by <a href="https://unsplash.com/@pankajpatel?utm_source=medium&amp;utm_medium=referral" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">Pankaj Patel</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">Unsplash</a></figcaption></figure><h1 id="808f" class="gs gt dc bk bj gu de gv dg gw gx gy gz ha hb hc hd">TL;DR</h1><p id="79a7" class="he hf dc bk hg b hh hi hj hk hl hm hn ho hp hq hr cu">All the code examples from this articles you can find on a GitHub repository <a href="https://github.com/AndrejsAbrickis/axios-cheerio-puppeteer" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">https://github.com/AndrejsAbrickis/axios-cheerio-puppeteer</a></p></div></div></section><hr class="hs ef ht hu hv gl hw hx hy hz ia"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="466a" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">Axios and cheerio is a great toolset to fetch and scrape the content of a static web page. But nowadays when many of the websites are built as a single page application and gets rendered dynamically on the client it might not be possible to get the content.</p><p id="a056" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">Just because it’s rendered asynchronously and the content is not backed into the HTML received over the wire, doesn’t mean you cannot access it. You just need a different toolset which allows waiting for the content to appear.</p><p id="7d3c" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">Let’s have a quick look on the source HTML of a SPA application and the rendered result.</p></div></div><div class="fs"><div class="n p"><div class="ig ih ii ij ik il ag im ah in aj ak"><figure class="fn fo fp fq fr fs ip iq paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm io"><div class="gc r fv gd"><div class="ir r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*rf9VXhBcni3ub-I_UHO9Ug.png?q=20" width="2792" height="1538" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="2792" height="1538" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/5584/1*rf9VXhBcni3ub-I_UHO9Ug.png" width="2792" height="1538" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg gj gk gl cn cl cm gm gn bj ef">Screenshot of the beach volleyball standing on <a href="https://bvopen.abrickis.me/#/standings" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">https://bvopen.abrickis.me/#/standings</a></figcaption></figure></div></div></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="855e" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">In the screenshot above, on the left, you can see a fully rendered standings table. But look at the source the browser downloaded all we can notice is a single <code class="gd is it iu iv b">&lt;div id="#app"&gt;&lt;/div&gt;</code> and a couple of JavaScript files and NO content. So let’s try to get the HTML content of the body.</p><h1 id="b334" class="gs gt dc bk bj gu de gv dg gw gx gy gz ha hb hc hd">Start with axios + cheerio</h1><p id="6320" class="he hf dc bk hg b hh hi hj hk hl hm hn ho hp hq hr cu"><a href="https://github.com/axios/axios" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">axios</a> is a “Promise based HTTP client for the browser and node.js”. Because it’s an HTTP client we can use it to fetch an HTTP endpoint and receive the response with the body. We can use the HTTP client to fetch not only HTML endpoint but also JSON, images, etc. And hence we are responsible to handle the plain text response.</p><p id="1c30" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">That’s where the cheerio comes to help. <a href="https://github.com/cheeriojs/cheerio" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">Cheerio</a> is a “Fast, flexible &amp; lean implementation of core jQuery designed specifically for the server”. Basically, it loads and parses the HTML markup as plain text and returns a DOM model we can then access and traverse in jQuery style.</p><p id="7046" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">And because cheerio doesn’t interpret the markup as a browser does. It won’t apply the CSS styles and won’t run the JavaScript and the dynamically rendered content won’t be added to the DOM.</p><p id="0140" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">As an example let’s try to get the content of the body tag using axios and cheerio. In the following gist you can see that we are firing a GET request (L6), then parse the response data into a DOM using cheerio (L7) and finally search for the <code class="gd is it iu iv b">&lt;body&gt;</code> element (L9) to output its HTML content.</p><figure class="fn fo fp fq fr fs"><div class="gc r fv"><div class="iw r"><iframe src="https://medium.com/media/75ee615b6e9d6ed396433a075f0b74a8" allowfullscreen="" frameborder="0" height="0" width="0" title="axios-cheerio.js" class="cp t u fz ak" scrolling="auto"></iframe></div></div></figure><p id="1b8b" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">When executed this node script we get the web apps placeholder element <code class="gd is it iu iv b">&lt;div id="app"&gt;</code> without the dynamically rendered content.</p><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm ix"><div class="gc r fv gd"><div class="iy r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*DcGa07rn0f0HPFs0tP_nHg.png?q=20" width="1024" height="277" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="1024" height="277" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/2048/1*DcGa07rn0f0HPFs0tP_nHg.png" width="1024" height="277" role="presentation"></noscript></div></div></div></div></figure><p id="06cb" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">Because of what we received over the wire was a plain text and the JavaScripts included in the HTML were not executed and this is where a headless browser comes to rescue.</p><h1 id="3801" class="gs gt dc bk bj gu de gv dg gw gx gy gz ha hb hc hd">Switch to puppeteer and headless Chrome</h1><p id="ebfa" class="he hf dc bk hg b hh hi hj hk hl hm hn ho hp hq hr cu">Let me shortly explain what a <a href="https://developers.google.com/web/updates/2017/04/headless-chrome" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">headless</a> browser is. In a nutshell headless means it’s a browser without graphical user interface (GUI) which can be controlled programmatically. Mostly it’s useful for E2E testing as it will apply all styles, and run JavaScript to generate the DOM. And because of that, it’s a perfect tool to scrape Single Page Applications.</p><p id="9667" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">And as I mentioned that it’s controlled programmatically. And for that reason, we can use puppeteer to control the browser over the <a href="https://chromedevtools.github.io/devtools-protocol/" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">DevTools</a> protocol. Let’s get hands-on and see how to get the dynamically rendered HTML.</p><figure class="fn fo fp fq fr fs"><div class="gc r fv"><div class="iw r"><iframe src="https://medium.com/media/ee850ac5668b06ea1200310ac2e4b991" allowfullscreen="" frameborder="0" height="0" width="0" title="puppeteer.js" class="cp t u fz ak" scrolling="auto"></iframe></div></div></figure><p id="e898" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">In the example above we are using single dependancy pf puppeteer package. First, we initialize a browser instance (L5) and create a new browser page (L6). Afterward, we instruct the browser page to visit an URL (L7) and wait for an element to appear on the page (L8) before to continue. Notice that one can set the timeout in milliseconds how long the browser should wait for the element.</p><p id="c071" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">After we have awaited the element we are using page’s evaluate method to execute a JavaScript within the web page’s context (L10 — L12). This allows us to access the HTML document using vanilla DOM API. From this, we return the HTML of body element and output. And finally, we close the browser which kills the headless Chrome’s process.</p><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm ix"><div class="gc r fv gd"><div class="iy r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*sIEBwWRmcf3544YxRTiu3w.png?q=20" width="1024" height="277" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="1024" height="277" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/2048/1*sIEBwWRmcf3544YxRTiu3w.png" width="1024" height="277" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg gj gk gl cn cl cm gm gn bj ef">A part of the output</figcaption></figure><p id="1c09" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">And now the result of running this script includes the content of dynamically rendered HTML.</p><h1 id="21ab" class="gs gt dc bk bj gu de gv dg gw gx gy gz ha hb hc hd">Conclusion</h1><p id="7699" class="he hf dc bk hg b hh hi hj hk hl hm hn ho hp hq hr cu">This short post demonstrated two solutions how to scrape a website. One can use a combination of axios and cheerio to get the content of a statically rendered website. And use puppeteer to get a dynamical content which is rendered by a fully-powered and invisible (headless) browser.</p><p id="363e" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">I hope this article will help you to start to utilize the mentioned tools as they can be used not only to scrape the websites but also for testing your web apps (E2E or snapshot tests) or taking screenshots.</p></div></div></section><hr class="hs ef ht hu hv gl hw hx hy hz ia"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="475a" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">If you found this post useful and would like to read more about random web development topics, just clap for this article or drop a comment here. And as always you can find me on <a href="https://twitter.com/andrejsabrickis" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">Twitter@andrejsabrickis</a></p></div></div></section><hr class="hs ef ht hu hv gl hw hx hy hz ia"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="3f88" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">This article, the content, and opinions expressed on Medium are my own. But as I work for one of the<a href="https://www.mintos.com/en/" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow"> leading P2P loans marketplaces Mintos.com</a> I would like to use this last line to promote that we are hiring. Including the Growth Engineering team, I am leading at the moment.</p><p id="46d0" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">You can see all list of the <a href="https://mintos.workable.com" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">open positions on our Workable board</a>. And feel free to contact me directly if you find something interesting in the list or would like to recommend a person you know.</p><p id="52ec" class="he hf dc bk hg b hh ib hj ic hl id hn ie hp if hr cu">Cheers!</p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="1d90" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How to Scrape Data from Web Pages for Sentiment Analysis?</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@xbytecrawling?source=post_page-----927b31b08f36----------------------"><img alt="X-Byte Enterprise Crawling" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/0*58raBoA0H70LX-hG.jpg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@xbytecrawling?source=post_page-----927b31b08f36----------------------">X-Byte Enterprise Crawling</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@xbytecrawling/how-to-scrape-data-from-web-pages-for-sentiment-analysis-927b31b08f36?source=post_page-----927b31b08f36----------------------">Nov 27, 2019</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/927b31b08f36/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/927b31b08f36/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40xbytecrawling%2Fhow-to-scrape-data-from-web-pages-for-sentiment-analysis-927b31b08f36&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div><div class="fm"><div class="n p"><div class="fn fo fp fq fr fs ag ft ah fu aj ak"><figure class="fv fw fx fy fz fm ga gb paragraph-image"><div class="gc gd ge gf ak"><div class="cl cm ai"><div class="gl r ge gm"><div class="gn r"><div class="gg gh cp t u gi ak eh gj gk"><img alt="HOW TO SCRAPE DATA FROM WEB PAGES FOR SENTIMENT ANALYSIS?" class="cp t u gi ak go gp gq" src="https://miro.medium.com/max/60/1*oa1PS-Ldgr4M5RByT-9udg.png?q=20" width="1171" height="510"></div><img alt="HOW TO SCRAPE DATA FROM WEB PAGES FOR SENTIMENT ANALYSIS?" class="gg gh cp t u gi ak gr" width="1171" height="510"><noscript><img alt="HOW TO SCRAPE DATA FROM WEB PAGES FOR SENTIMENT ANALYSIS?" class="cp t u gi ak" src="https://miro.medium.com/max/2342/1*oa1PS-Ldgr4M5RByT-9udg.png" width="1171" height="510"></noscript></div></div></div></div><figcaption class="bo eg gs gt gu cn cl cm gv gw bj ef">Scrape Data from Web Pages for Sentiment Analysis Image</figcaption></figure></div></div></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="f275" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">Today, Businesses can understand their customers’ reactions with the help of many available tools. They can analyze if the customers have liked the layout or not, get the existing offers, did the services please them? The increased data volume is valuable to evaluate success as well as draw insights about the future.</p><figure class="fv fw fx fy fz fm cl cm paragraph-image"><div class="gc gd ge gf ak"><div class="cl cm hl"><div class="gl r ge gm"><div class="gn r"><div class="gg gh cp t u gi ak eh gj gk"><img class="cp t u gi ak go gp gq" src="https://miro.medium.com/max/60/0*nERODEcq6J1r5E3Z.png?q=20" width="1171" height="510" role="presentation"></div><img class="gg gh cp t u gi ak gr" width="1171" height="510" role="presentation"><noscript><img class="cp t u gi ak" src="https://miro.medium.com/max/2342/0*nERODEcq6J1r5E3Z.png" width="1171" height="510" role="presentation"></noscript></div></div></div></div></figure><p id="4dbf" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">At <a href="https://www.xbyte.io/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow"><strong class="gz hq"><em class="hr">X-Byte Enterprise Crawling</em></strong></a>, We are a Data-as-a-Service provider, so we understand the importance of this data as well as help you get valuable insights through our Data Scraping Services. We Extract Websites and Scrape Structured Data that can be utilized to derive some insights. We provide the <a href="https://www.xbyte.io/service/web-scraping-service/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow"><strong class="gz hq"><em class="hr">best webpage data scraping</em></strong></a> for sentiment analysis services to help your business do better with real time sentiment analysis of social media platform data.</p><h1 id="b9ea" class="hs ht dc bk bj hu de hv dg hw hx hy hz ia ib ic id">We Help Extract Products’ User Reviews</h1><p id="3875" class="gx gy dc bk gz b ha ie hc if he ig hg ih hi ii hk cu">Being a <a href="https://www.xbyte.io/service/web-scraping-service/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow"><strong class="gz hq"><em class="hr">web scraping service</em></strong></a> provider, we make that easier to scrape data from the web. With our professional webpage data scraping services for sentiment analysis, you just need to provide us the websites list that you want to scrape for sentiment analysis with the required fields as well as the frequency that you wish the data to. With our personalized crawlers as well as progressive computing stacks, we have retrieved the data in a format you want (generally JSON, CSV, XML,). You can ask for the data through our API or even get the data provided to your AWS or FTP location.</p><h1 id="b492" class="hs ht dc bk bj hu de hv dg hw hx hy hz ia ib ic id">How Important the Sentiment Analysis Is?</h1><figure class="fv fw fx fy fz fm cl cm paragraph-image"><div class="gc gd ge gf ak"><div class="cl cm hl"><div class="gl r ge gm"><div class="gn r"><div class="gg gh cp t u gi ak eh gj gk"><img alt="How Important the Sentiment Analysis Is?" class="cp t u gi ak go gp gq" src="https://miro.medium.com/max/60/0*hwyQV4JiYG87AoYj.png?q=20" width="1171" height="510"></div><img alt="How Important the Sentiment Analysis Is?" class="gg gh cp t u gi ak gr" width="1171" height="510"><noscript><img alt="How Important the Sentiment Analysis Is?" class="cp t u gi ak" src="https://miro.medium.com/max/2342/0*hwyQV4JiYG87AoYj.png" width="1171" height="510"></noscript></div></div></div></div></figure><p id="2045" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">As the data scraping is really challenging, we do replicate on how the opinion mining could help our business enterprise clients do better. Sentiment Analysis or Opinion Mining copes with automatic data scanning as well as establishing its purpose or nature. Basically, it is very important to define if the text extracted and scraped from the website is helpful or not; or whether it associates with the subject which is given in the title.</p><h1 id="c3e6" class="hs ht dc bk bj hu de hv dg hw hx hy hz ia ib ic id">Study of Sentiment Analysis Functions</h1><figure class="fv fw fx fy fz fm cl cm paragraph-image"><div class="gc gd ge gf ak"><div class="cl cm hl"><div class="gl r ge gm"><div class="gn r"><div class="gg gh cp t u gi ak eh gj gk"><img alt="Study of Sentiment Analysis Functions" class="cp t u gi ak go gp gq" src="https://miro.medium.com/max/60/0*sCoU-BjYVtG2bmmC.png?q=20" width="1171" height="510"></div><img alt="Study of Sentiment Analysis Functions" class="gg gh cp t u gi ak gr" width="1171" height="510"><noscript><img alt="Study of Sentiment Analysis Functions" class="cp t u gi ak" src="https://miro.medium.com/max/2342/0*sCoU-BjYVtG2bmmC.png" width="1171" height="510"></noscript></div></div></div></div></figure><p id="863f" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">The functions of Sentiment Analysis of Twitter or Sentiment Analysis of Facebook could be to analyze records (product feedback, user reviews, services feedback forms, etc.) as well as specify feelings expressed (dissatisfaction, happiness, etc.). On the easy scale, it can be attained by creating a rating system from 1–10 where every word is usually associated with emotions. The scores of every word, as well as the entire text, is calculated to observe what the sentiments or opinions are indicated.</p><p id="f81b" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">The added methodology is objectivity or subjectivity identification. Here, scraped data is verified for being objective or subjective. Though, this might prove to be tough as results of assessments are person-specific.</p><p id="bc36" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">Maybe the most advanced type is Feature-Based Sentiment Analysis. Here, individuals give opinions about users that are scraped from the text about a definite service or product and then evaluate it to see if a consumer gets satisfied or not. That is where X-Byte Enterprise Crawling’s <a href="https://www.xbyte.io/service/web-scraping-service/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow"><strong class="gz hq"><em class="hr">Web Data Scraping Services</em></strong></a> help. For instance, if you want to crawl hundreds and thousands of news, blogs, or forum websites to scrape high-level data like date, title, article URLs, content, and author, mass-scale crawls, etc. will offer the data in a well-structured format like constant feeds.</p><p id="88af" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">We could also filter these crawls based on a list of keywords to facilitate better sentiment analysis based on subject topic, language, and even keyword detection. Our named-entity recognition service only helps to enrich this information.</p><p id="96e1" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">We help our clients with product sentiment analysis. The customer wanted to scrape comments about that from websites and forums, from distributors, retailers, and enthusiasts to an average customer. The customer’s use case was to get data to know how promising users found the product as well as what consumers have talked about that on the Internet.</p><p id="b6b1" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">Considering there are thousands of websites that might comprise product reviews as well as different online forums based on the consumer durables or associated topics, you get a valued collection of understandings. We set crawls to scrape reviews from highly valued websites with thousands of URLs spontaneously.</p><p id="fe93" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">Our automated data scraping and <a href="https://www.xbyte.io/solutions/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow"><strong class="gz hq"><em class="hr">Monitoring Solutions</em></strong></a> target sites as well as deliver exact results. Furthermore, with place normalization, we deliver analysis-ready well-structured data.</p><h1 id="f282" class="hs ht dc bk bj hu de hv dg hw hx hy hz ia ib ic id">The X-Byte Enterprise Crawling Advantage</h1><ul class=""><li id="0f3e" class="gx gy dc bk gz b ha ie hc if he ig hg ih hi ii hk ij ik il">Our process is simple and efficient to make the crawls running.</li><li id="0457" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Our site maintenance and monitoring record all the changes in structure to offer constant data coverage.</li><li id="fcb3" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">With our <a href="https://www.xbyte.io/service/web-scraping-service/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow"><strong class="gz hq"><em class="hr">Web Data Scraping Services</em></strong></a><strong class="gz hq">,</strong> you will get the data you want.</li><li id="f4cf" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">You will get complete and easy access.</li><li id="ff28" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">You will get regular data feed alerts on uploads as well as a collaborative API system to request data from.</li></ul><p id="6a25" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">To get professional web data scraping for Sentiment Analysis, contact <a href="https://www.xbyte.io/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow"><strong class="gz hq"><em class="hr">X-Byte Enterprise Crawling</em></strong></a><strong class="gz hq"><em class="hr"> </em></strong>or ask for a free quote!</p></div></div></section><hr class="ir ef is it iu gu iv iw ix iy iz"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="bb9b" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu"><strong class="gz hq">Visit Us:</strong> <a href="https://www.xbyte.io/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow"><em class="hr">www.xbyte.io</em></a></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="4a9a" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Scalable do-it-yourself scraping — How to build and run scrapers on a large scale</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@ScrapeHero?source=post_page-----79eb1ab85482----------------------"><div class="dz ea eb"><div class="bs n ec o p cp ed ee ef eg eh ct"><svg width="57" height="57" viewbox="0 0 57 57"><path fill-rule="evenodd" clip-rule="evenodd" d="M28.5 1.2A27.45 27.45 0 0 0 4.06 15.82L3 15.27A28.65 28.65 0 0 1 28.5 0C39.64 0 49.29 6.2 54 15.27l-1.06.55A27.45 27.45 0 0 0 28.5 1.2zM4.06 41.18A27.45 27.45 0 0 0 28.5 55.8a27.45 27.45 0 0 0 24.44-14.62l1.06.55A28.65 28.65 0 0 1 28.5 57 28.65 28.65 0 0 1 3 41.73l1.06-.55z"></path></svg></div><img alt="ScrapeHero" class="r ei eb ea" src="https://miro.medium.com/fit/c/96/96/1*7F4A2PkgQ5Rd0vZKGHR9bw.png" width="48" height="48"></div></a></div><div class="ej ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ek n o el"><span class="bj em en bl eo ep eq er es et dc"><a class="at au av aw ax ay az ba bb bc eu bf bg bh bi" rel="noopener" href="/@ScrapeHero?source=post_page-----79eb1ab85482----------------------">ScrapeHero</a></span><div class="ev r ar h"><button class="ew dc q by ex ey ez fa bc bh fb fc fd fe ff fg cb bj b bk fh fi bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj em en bl eo ep eq er es et bo"><div><a class="at au av aw ax ay az ba bb bc eu bf bg bh bi" rel="noopener" href="/@ScrapeHero/scalable-do-it-yourself-scraping-how-to-build-and-run-scrapers-on-a-large-scale-79eb1ab85482?source=post_page-----79eb1ab85482----------------------">Dec 1, 2015</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n fj fk fl fm fn fo fp fq ab"><div class="n o"><div class="fr r ar"><a href="//medium.com/p/79eb1ab85482/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fr r ar"><a href="//medium.com/p/79eb1ab85482/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fs r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40ScrapeHero%2Fscalable-do-it-yourself-scraping-how-to-build-and-run-scrapers-on-a-large-scale-79eb1ab85482&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fu fv fw fx fy fz ga gb cd gc gd ge gf gg ba gh gi gj gk gl gm paragraph-image"><div class="gn go dz gp ak"><div class="cl cm ft"><div class="gv r dz gw"><div class="gx r"><div class="gq gr cp t u gs ak eo gt gu"><img class="cp t u gs ak gy gz ha" src="https://miro.medium.com/max/60/0*HWswa5tQM1c6O3TY.jpg?q=20" width="640" height="426" role="presentation"></div><img class="gq gr cp t u gs ak ga" width="640" height="426" role="presentation"><noscript><img class="cp t u gs ak" src="https://miro.medium.com/max/1280/0*HWswa5tQM1c6O3TY.jpg" width="640" height="426" role="presentation"></noscript></div></div></div></div><figcaption class="bo en hb hc hd cn cl cm he hf bj em">Scraping data for your business</figcaption></figure><p id="ed08" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">Businesses that don’t rely on data have a very low chance of success in a data driven world.</p><p id="8d38" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">One of the best sources of data is the data available publicly online on various websites and to get this data you have to employ the technique called Web Scraping or Data Scraping.</p><p id="861d" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">You can use full service professionals such <a href="http://scrapehero.com/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">ScrapeHero </a>to do all this for you or if you feel brave enough, you can tackle this yourself.</p><p id="2bf4" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">The purpose of this article is to walk you through some of the things you need to do and the issues you need to be cognizant of when you do decide to do it yourself.</p><p id="c2ec" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">When you decide to do this yourself, you will most likely be hiring a few developers who know how to build scrapers and setting up some servers and related infrastructure to run these scrapers without interruption, and integrating the data you extract into your business process.</p><p id="c559" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">Building and maintaining a large number of web scrapers is a very complex process so proceed with caution.</p><p id="b3af" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">Here are the high level steps involved in this process and we will go through each of these in detail in this article.</p><ol class=""><li id="404d" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht hy hz ia">Build Scrapers</li><li id="8cb4" class="hg hh dc bk hi b hj ib hl ic hn id hp ie hr if ht hy hz ia">Run the Scrapers</li><li id="9689" class="hg hh dc bk hi b hj ib hl ic hn id hp ie hr if ht hy hz ia">Store the data</li><li id="280f" class="hg hh dc bk hi b hj ib hl ic hn id hp ie hr if ht hy hz ia">IP Rotation, Proxies and Blacklisting</li><li id="4dda" class="hg hh dc bk hi b hj ib hl ic hn id hp ie hr if ht hy hz ia">Quality Checks on Data</li><li id="e92b" class="hg hh dc bk hi b hj ib hl ic hn id hp ie hr if ht hy hz ia">Maintenance</li></ol><h1 id="0ca2" class="ig ih dc bk bj ii de ij dg ik il im in io ip iq ir">Build Scrapers and Set up the servers</h1><p id="2cc2" class="hg hh dc bk hi b hj is hl it hn iu hp iv hr iw ht cu">The first thing to do is build the scrapers.</p><p id="4116" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">It may be best to choose an open-source framework for building your scrapers, like Scrapy or PySpider. These are excellent frameworks with a large community of developers. Both these frameworks are based on Python. You won’t run into the risk of your developer(s) disappearing in a day, and no one to maintain your scrapers because Python is popular and the community is really supportive.</p><p id="0b25" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">There is also a massive difference between writing and running one scraper that scrapes 100 pages to a large scale distributed scraping infrastructure that can scrape thousands of websites and millions of pages a day.</p><p id="5b87" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">If you are scraping a large number of big websites, you might need lot of servers to get the data in a reasonable time frame. We would suggest using Scrapy Redis or Run PySpider in scaled mode, across multiple servers.</p><p id="e828" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">Once you have chosen a framework, hire some good developers to build these scrapers, and set up the servers required to run them and to store the data.</p><h1 id="1f63" class="ig ih dc bk bj ii de ij dg ik il im in io ip iq ir">Run Scrapers</h1><p id="cd43" class="hg hh dc bk hi b hj is hl it hn iu hp iv hr iw ht cu">If you need the data to be refreshed periodically, you’ll either have to <strong class="hi ix">run it manually or automate</strong> it using some tool or process.</p><p id="d786" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">If you are using Scrapy,scrapyd + cron can schedule the spiders for you, and it will update the data the way you need it. PySpider also has a UI to do that</p><h1 id="edec" class="ig ih dc bk bj ii de ij dg ik il im in io ip iq ir">Store your data</h1><p id="ff0a" class="hg hh dc bk hi b hj is hl it hn iu hp iv hr iw ht cu">Once you have this massive data trove, you need a place to store it. We would suggest using a NoSQL database like MongoDB, Cassandra or HBase to store this data, depending upon the frequency and speed of scraping.</p><p id="3f9d" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">You can then extract this data from this database/datastore and integrate it with your business process. But before you do that, you should setup some Quality Assurance tests for your data (more on that later)</p><h1 id="2a8d" class="ig ih dc bk bj ii de ij dg ik il im in io ip iq ir">IP Rotation, Proxies and Blacklisting</h1><p id="3c14" class="hg hh dc bk hi b hj is hl it hn iu hp iv hr iw ht cu">Large scale scraping comes with a multitude of problems and one of the big ones is anti-scraping measures by the websites that you are trying to scrape.</p><p id="5d5f" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">If any of the target websites has any kind of <strong class="hi ix">IP based blocking</strong> involved, your servers’ IP address will be black listed in no time and the site won’t respond to requests from your servers. You’ll be left with very few options after getting blacklisted.</p><p id="4ea5" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">So, how do you bypass that? You’ll have to get some <strong class="hi ix">Proxies or Rotating IP solutions</strong> to use these for making requests from the scraper.</p><p id="08ff" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu"><a href="http://learn.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">Here are few tips to prevent getting blacklisted</a></p><h1 id="6fb7" class="ig ih dc bk bj ii de ij dg ik il im in io ip iq ir">Quality Assurance</h1><p id="2e31" class="hg hh dc bk hi b hj is hl it hn iu hp iv hr iw ht cu">The data you scrape is only as good as its quality. To ensure the data that you scraped in accurate and complete, you need to run a variety of QA tests on it right after it is scraped.</p><p id="9b23" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">Having a set of <strong class="hi ix">Tests </strong>for the integrity of the data is essential. Some of it can be automated by using Regular Expressions, to check if the data follows a predefined pattern and if it doesn’t then generate some alerts so that it can be manually inspected.</p><h1 id="8876" class="ig ih dc bk bj ii de ij dg ik il im in io ip iq ir">Maintenance</h1><h2 id="b8d7" class="iy ih dc bk bj ii iz ja jb jc jd je jf jg jh ji jj">Scrapers</h2><p id="4b02" class="hg hh dc bk hi b hj is hl it hn iu hp iv hr iw ht cu"><strong class="hi ix">Every website will change</strong> their structure now and then, and so should your scrapers. Scrapers usually need adjustments every few weeks, as a minor change in the target website affecting the fields you scrape might either give you incomplete data or even crash the scraper, depending on the logic of the scraper.</p><p id="f423" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">You have to be smart and detect this change and fix it before this ruins the data you are collecting.</p><h2 id="6867" class="iy ih dc bk bj ii iz ja jb jc jd je jf jg jh ji jj">Database &amp; Servers</h2><p id="3b75" class="hg hh dc bk hi b hj is hl it hn iu hp iv hr iw ht cu">Depending upon the size of data, you will have to clean up your database of outdated data to save space and money. You might also have to scale up your systems if you still need the old data. Sharding and Replication of databases can be of help.</p><p id="2c31" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">Server logs should also be cleaned periodically.</p><h1 id="f076" class="ig ih dc bk bj ii de ij dg ik il im in io ip iq ir">Know when to ask for help</h1><p id="1960" class="hg hh dc bk hi b hj is hl it hn iu hp iv hr iw ht cu">This whole process is expensive and time consuming and you need to be ready to take on this challenge.</p><p id="4771" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu">You also need to know when to stop and ask for help. <a href="http://scrapehero.com/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">ScrapeHero </a>has been doing all this and more for many years now, so let us know if you need any help.</p><h1 id="259c" class="ig ih dc bk bj ii de ij dg ik il im in io ip iq ir">Related</h1></div></div></section><hr class="jk em jl jm jn hd jo jp jq jr js"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="2fb6" class="hg hh dc bk hi b hj hk hl hm hn ho hp hq hr hs ht cu"><em class="jt">Originally published at </em><a href="http://learn.scrapehero.com/scalable-do-it-yourself-scraping-how-to-build-and-run-scrapers-on-a-large-scale/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow"><em class="jt">learn.scrapehero.com</em></a><em class="jt"> on December 1, 2015.</em></p></div></div></section></div><div><div class="dq u dr ds dt du"></div><section class="dv dw dx dy dz"><div class="n p"><div class="ae af ag ah ai ea ak al"><div><div id="2606" class="eb ec ed bc ee b ef eg eh ei ej ek el em en eo ep"><h1 class="ee b ef eq eh er ej es el et en eu ed">How to web scrape with Puppeteer in Google Cloud Functions</h1></div><div class="ev"><div class="n ew ex ey ez"><div class="o n"><div><a rel="noopener" href="/@benjaminmorali4?source=post_page-----59ad86d3abdc----------------------"><img alt="Benjamin Morali" class="r fa fb fc" src="https://miro.medium.com/fit/c/96/96/0*BOVMH1ZPEe_ObyJl.jpg" width="48" height="48"></a></div><div class="fd al r"><div class="n"><div style="flex:1"><span class="bb b bc bd be bf r ed q"><div class="fe n o ff"><span class="bb fg fh bd cu fi fj fk fl fm ed"><a class="fn fo bk bl bm bn bo bp bq br fp bu bv fq fr" rel="noopener" href="/@benjaminmorali4?source=post_page-----59ad86d3abdc----------------------">Benjamin Morali</a></span><div class="fs r ay h"><button class="ft ed q bz fu fv fw fx br fq fy fz ga gb gc gd cc bb b bc ge gf bf cd ce cf cg ch bu">Follow</button></div></div></span></div></div><span class="bb b bc bd be bf r ck cl"><span class="bb fg fh bd cu fi fj fk fl fm ck"><div><a class="fn fo bk bl bm bn bo bp bq br fp bu bv fq fr" rel="noopener" href="/benextcompany/how-to-web-scrape-with-puppeteer-in-google-cloud-functions-59ad86d3abdc?source=post_page-----59ad86d3abdc----------------------">Feb 21, 2019</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div><div class="n gg gh gi gj gk gl gm gn ab"><div class="n o"><div class="go r ay"><a href="//medium.com/p/59ad86d3abdc/share/twitter?source=post_actions_header---------------------------" class="fn fo bk bl bm bn bo bp bq br gp gq bu bv fq fr" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="go r ay"><a href="//medium.com/p/59ad86d3abdc/share/facebook?source=post_actions_header---------------------------" class="fn fo bk bl bm bn bo bp bq br gp gq bu bv fq fr" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gr r ap"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fbenextcompany%2Fhow-to-web-scrape-with-puppeteer-in-google-cloud-functions-59ad86d3abdc&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="fn fo bk bl bm bn bo bp bq br gp gq bu bv fq fr" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><blockquote class="gs gt gu"><p id="b145" class="gv gw ed gx gy b gz ha hb hc hd he hf hg hh hi hj dv">In this article, I will use <em class="bc">Javascript</em> (<em class="bc">Node.js</em>) for the code, <em class="bc">Yarn</em> as a package manager for Node, and <em class="bc">apt-get</em> for OS dependencies.</p></blockquote><p id="b1e2" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">When you need data from a source that doesn’t provide an API, you have to do web scraping. That’s why you can consider using Puppeteer combined with Google Cloud Functions. Puppeteer is a library that uses Chromium to automate browser interactions. However, this is a time-consuming process, heavy for CPU and memory. So in order to keep your app light, you may want to execute this code into a cloud environment like Google Cloud Functions (the equivalent of AWS Lambda).</p><figure class="hl hm hn ho hp hq dm dn paragraph-image"><div class="hr hs ht hu al"><div class="dm dn hk"><div class="ia r ht ib"><div class="ic r"><div class="hv hw dq t u hx al cu hy hz"><img class="dq t u hx al id ie if" src="https://miro.medium.com/max/60/1*yVVJToJSlbCH0m63Kzwf1g.png?q=20" width="1024" height="694" role="presentation"></div><img class="hv hw dq t u hx al ig" width="1024" height="694" role="presentation"><noscript><img class="dq t u hx al" src="https://miro.medium.com/max/2048/1*yVVJToJSlbCH0m63Kzwf1g.png" width="1024" height="694" role="presentation"></noscript></div></div></div></div></figure><h1 id="8b2a" class="ih ii ed bc bb ij ef ik eh il im in io ip iq ir is">Basic configuration</h1><p id="3629" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj dv">Let’s start by initializing a node project:</p><pre class="hl hm hn ho hp iy iz dc"><span id="218f" class="ja ii ed bc jb b fh jc jd r je">$ yarn init -y</span></pre><p id="c1e0" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">Then, <code class="ib jf jg jh jb b">cd</code> to your new project and install Puppeteer:</p><pre class="hl hm hn ho hp iy iz dc"><span id="9f2c" class="ja ii ed bc jb b fh jc jd r je">$ yarn add puppeteer</span></pre><p id="19f1" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">This will download the most recent stable version of Chromium on your machine, about ~200MB depending on your OS.</p><p id="6600" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">In order to test and deploy your functions, you will need to install the Google Cloud SDK and the Google Cloud Functions Emulator. To get the SDK, run the following command (on <em class="gx">Ubuntu</em>):</p><pre class="hl hm hn ho hp iy iz dc"><span id="54e5" class="ja ii ed bc jb b fh jc jd r je">$ sudo apt-get install google-cloud-sdk</span></pre><p id="e988" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">This SDK will allow you to deploy your functions. But before that, you will need to test them locally with the functions emulator:</p><pre class="hl hm hn ho hp iy iz dc"><span id="a794" class="ja ii ed bc jb b fh jc jd r je">$ yarn global add @google-cloud/functions-emulator --ignore-engines</span></pre><p id="def2" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">The <code class="ib jf jg jh jb b">--ignore-engines</code> option will very likely be required. Currently, the Google Cloud Functions Emulator is fully compatible with Node 6. If your Node version is higher than that, the dependency won’t work unless you choose to ignore it with this option.</p><p id="2f62" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">So basically, your project only needs two files:</p><ul class=""><li id="f57c" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj ji jj jk"><code class="ib jf jg jh jb b">index.js</code> for your Javascript code</li><li id="3737" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><code class="ib jf jg jh jb b">package.json</code> for the Puppeteer dependency and your scripts</li></ul><p id="cc3f" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">Here, <code class="ib jf jg jh jb b">package.json</code> contains the basic scripts to test your function locally and deploy it:</p><figure class="hl hm hn ho hp hq"><div class="ia r ht"><div class="jq r"><iframe src="https://medium.com/media/251b94d6b5cd4c6fb127cc26144ff755" allowfullscreen="" frameborder="0" height="0" width="0" title="" class="dq t u hx al" scrolling="auto"></iframe></div></div></figure><p id="3672" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">This file contains the main dependency of this project, <code class="ib jf jg jh jb b">puppeteer</code>, and two scripts to test and deploy your function. Both scripts rely on <code class="ib jf jg jh jb b">scrapingExample</code>, the name used in the example below with <code class="ib jf jg jh jb b">exports.scrapingExample</code>.</p><ul class=""><li id="c9ed" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj ji jj jk">The <code class="ib jf jg jh jb b">deploy</code> script is used to put your function on a remote cloud environment. <code class="ib jf jg jh jb b">--trigger-http</code> associates an HTTP verb (by default POST) to our function. <code class="ib jf jg jh jb b">--runtime</code> is the runtime used here (others are available like Node 6, Go and Python). The complete list of options is available <a href="https://cloud.google.com/sdk/gcloud/reference/functions/deploy" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">here</a>.</li><li id="a83a" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk">The <code class="ib jf jg jh jb b">start</code> script launches the functions emulator and locally deploys the function with the same <code class="ib jf jg jh jb b">--trigger-http</code> flag described above.</li></ul><p id="83c0" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">The following code is a basic configuration for <code class="ib jf jg jh jb b">index.js</code>:</p><figure class="hl hm hn ho hp hq"><div class="ia r ht"><div class="jq r"><iframe src="https://medium.com/media/a534ed45ae972a46f9b63b0f79b9fc33" allowfullscreen="" frameborder="0" height="0" width="0" title="" class="dq t u hx al" scrolling="auto"></iframe></div></div></figure><p id="537e" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">There is a lot of boilerplate here: the only important lines are lines 38-41! However, we’ll go through the rest of the code to understand what happens.</p><p id="ff25" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">First, we import <code class="ib jf jg jh jb b">puppeteer</code> and declare its options:</p><ul class=""><li id="e090" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj ji jj jk"><code class="ib jf jg jh jb b">headless</code> is one of the most important options. When you test your function locally, put it to <code class="ib jf jg jh jb b">false</code> to see what happens in your browser. Every action of your script will be visible. Nevertheless,<strong class="gy jv"> you must put it to </strong><code class="ib jf jg jh jb b"><strong class="gy jv">true</strong></code><strong class="gy jv"> before deploying it to Google Cloud Functions</strong>. Otherwise, the execution will crash because the service cannot execute the GUI of Chromium.</li><li id="40af" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><code class="ib jf jg jh jb b">args</code> contains a list of useful options. Some of them are pretty explicit like <code class="ib jf jg jh jb b">--disable-gpu</code> or <code class="ib jf jg jh jb b">--timeout=30000</code> and some others like<code class="ib jf jg jh jb b">--no-sandbox</code> are here to prevent crashes in some environments. The complete list of arguments can be found <a href="https://peter.sh/experiments/chromium-command-line-switches/" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">here</a>.</li></ul><p id="451a" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">Then finally comes the code, split into 3 functions:</p><ul class=""><li id="6faa" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj ji jj jk"><code class="ib jf jg jh jb b">openConnection</code> initializes all the necessary objects to browse with Puppeteer. It also sets a few parameters like the user agent and the viewport, necessary for some websites.</li><li id="22e8" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><code class="ib jf jg jh jb b">closeConnection</code> destroys the objects initialized before and must be called at the end of every execution, regardless of the results of the execution. I’ll explain why in the <em class="gx">Tips and tricks </em>section.</li><li id="50f9" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><code class="ib jf jg jh jb b">scrapingExample</code> is the main function, which is going to be called by the functions emulator and deployed in Google Cloud. The <code class="ib jf jg jh jb b">exports.</code> before the function name makes it available for Google Cloud Functions. In order to keep this example simple, it only does a simple thing: go to the Medium homepage, get its first article title, and return it.</li></ul><h1 id="682c" class="ih ii ed bc bb ij ef ik eh il im in io ip iq ir is">Interactions with Google Cloud Storage</h1><p id="fc7e" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj dv">At some point, you may need to have persistent data. To do that, you cannot use the execution environment of your Google Cloud Function. A storage in fact exists, but it is temporary and very limited. To store a large number of files, you can use a cloud storage service like Google Cloud Storage or AWS S3. Just know that with the Google Cloud’s Free Plan, <strong class="gy jv">you cannot send data to another IP, so in this case, forget about Amazon S3, and go for Google Cloud Storage.</strong></p><p id="2278" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">There are several ways to upload files to a cloud storage. The most elegant one (not always possible), is to download your file (through <em class="gx">axios</em> for example), and pipe it to your remote bucket. This way, you never store anything in your Cloud Function environment, and avoid a lot of potential problems, like available storage or file naming. You can see an example of this method <a href="https://stackoverflow.com/questions/44945376/how-to-upload-an-in-memory-file-data-to-google-cloud-storage-using-nodejs?rq=1" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">here</a>.</p><p id="7deb" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">But sometimes, piping directly is not possible so you need to store your files in a temporary directory before uploading them. There is a simple way to initialize and use Google Cloud Storage with Puppeteer:</p><figure class="hl hm hn ho hp hq"><div class="ia r ht"><div class="jq r"><iframe src="https://medium.com/media/3bb91d5d6a0032bc3b9fe2c11da4a2a3" allowfullscreen="" frameborder="0" height="0" width="0" title="" class="dq t u hx al" scrolling="auto"></iframe></div></div></figure><p id="2d99" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">Here, we do several things:</p><ol class=""><li id="24f2" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj jw jj jk">We import Puppeteer and Google Cloud Storage.</li><li id="6ae6" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj jw jj jk">We initialize our bucket and Puppeteer.</li><li id="9bc3" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj jw jj jk">We allow Puppeteer to download files and we define the storage location. In the context of a Google Cloud Function, you would only be able to write in the <code class="ib jf jg jh jb b">/tmp/</code> directory.</li><li id="c973" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj jw jj jk">We scrape our file: Puppeteer goes to the page, clicks the link (which will download the file to <code class="ib jf jg jh jb b">/tmp/</code>) and upload it to Google Cloud Storage.</li></ol><h1 id="1d29" class="ih ii ed bc bb ij ef ik eh il im in io ip iq ir is">Handling bad website design</h1><p id="2748" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj dv">As a programmer, it’s a common thing to say it’s someone else’s fault. And when you do web scraping… this may be true! In fact, a website can be very poorly designed at several levels, making it difficult to scrape.</p><figure class="hl hm hn ho hp hq dm dn paragraph-image"><div class="hr hs ht hu al"><div class="dm dn jx"><div class="ia r ht ib"><div class="jy r"><div class="hv hw dq t u hx al cu hy hz"><img class="dq t u hx al id ie if" src="https://miro.medium.com/max/46/1*gXq-DCVPmArBK1qHtawTqQ.jpeg?q=20" width="914" height="1200" role="presentation"></div><img class="hv hw dq t u hx al ig" width="914" height="1200" role="presentation"><noscript><img class="dq t u hx al" src="https://miro.medium.com/max/1828/1*gXq-DCVPmArBK1qHtawTqQ.jpeg" width="914" height="1200" role="presentation"></noscript></div></div></div></div></figure><p id="1dc1" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">One problem you may encounter is related to page loading. Puppeteer provides several functions to wait for events. For example, if you need to navigate to a page and get an element from it, you can use the following function: <code class="ib jf jg jh jb b">await page.waitForNavigation({ waitUntil: 'load' })</code>. However, bad website design can make this instruction crash if you try to get an unexisting HTML element on the new page. Some websites trigger the <code class="ib jf jg jh jb b">load</code> event when the new page is loaded, but it only contains a loader element. You have to be careful, and it’s sometimes preferable to use <code class="ib jf jg jh jb b">await page.waitForSelector('.mySelector')</code>. The good thing about these two functions is that they have an optional <code class="ib jf jg jh jb b">timeout</code> argument. This can be useful on websites with a long loading time: the default timeout is 500ms.</p><p id="9a66" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">You also need to be careful with navigation links. Sometimes the information you want to scrape won’t be on a page directly accessible by URL. Some websites load data as you navigate, and you may need to reproduce a full “human” browsing to get the information you need.</p><p id="2a9e" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">Finally, be very precise with your CSS selectors! Some websites use the same id on several elements. This can make you select the wrong element in your code. When possible, use the <code class="ib jf jg jh jb b">&gt;</code> selector (or other selectors) to prevent any ambiguity.</p><h1 id="ed00" class="ih ii ed bc bb ij ef ik eh il im in io ip iq ir is">Tips and tricks</h1><h2 id="e9f7" class="ja ii ed bc bb ij jz ka kb kc kd ke kf kg kh ki kj">Memory management</h2><p id="2c51" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj dv">Your Google Cloud Function can run out of memory if you are not careful. Puppeteer launches Chromium, and you need to instantiate big objects (like <code class="ib jf jg jh jb b">browser</code> or <code class="ib jf jg jh jb b">page</code>) to use it. In the example above titled <em class="gx">Basic configuration</em>, you can see that <code class="ib jf jg jh jb b">closeConnection</code> is called in the <code class="ib jf jg jh jb b">finally</code> block. This is to destroy the objects and clean up the memory as you exit the function. In many Puppeteer examples, you don’t destroy anything in case of error. After several executions, your environment memory can then become full, and the first instruction <code class="ib jf jg jh jb b">puppeteer.launch(PUPPETEER_OPTIONS)</code> will crash.</p><h2 id="9984" class="ja ii ed bc bb ij jz ka kb kc kd ke kf kg kh ki kj">Debugging</h2><p id="5b44" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj dv">In the Google Cloud Management Console, you have access to logs that give you information about the remote execution of your functions. But for your local logs, you can use:</p><pre class="hl hm hn ho hp iy iz dc"><span id="fce3" class="ja ii ed bc jb b fh jc jd r je">$ functions logs read</span></pre><p id="58ed" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">To clear them, just execute (<code class="ib jf jg jh jb b">sudo</code> may be required here):</p><pre class="hl hm hn ho hp iy iz dc"><span id="6469" class="ja ii ed bc jb b fh jc jd r je">$ functions logs clear</span></pre><h2 id="1bfa" class="ja ii ed bc bb ij jz ka kb kc kd ke kf kg kh ki kj">DOM interactions</h2><p id="8967" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj dv">In order to get information on DOM elements, you can use the Puppeteer function <code class="ib jf jg jh jb b">page.evaluate()</code>. Inside its callback, you have access to DOM elements (through CSS selectors for example), but the rest of your code is not accessible. As a second argument after the callback, you can pass it a serializable object. This means that <strong class="gy jv">a function defined outside </strong><code class="ib jf jg jh jb b"><strong class="gy jv">evaluate()</strong></code><strong class="gy jv"> cannot be used inside of it.</strong></p><p id="b002" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">Another problem with <code class="ib jf jg jh jb b">page.evaluate()</code> is that it’s hard to debug. In fact, if you try to use <code class="ib jf jg jh jb b">console.log</code> inside of it, you won’t see anything in your local logs. To solve this issue, add the following instruction just after you initialize the <code class="ib jf jg jh jb b">page</code> object:</p><pre class="hl hm hn ho hp iy iz dc"><span id="a65f" class="ja ii ed bc jb b fh jc jd r je">await page.on(‘console’, obj =&gt; console.log(obj.text()));</span></pre><h2 id="a58f" class="ja ii ed bc bb ij jz ka kb kc kd ke kf kg kh ki kj">Using headless</h2><p id="bffd" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj dv">When you test your function locally, you almost always put the <code class="ib jf jg jh jb b">headless</code> option to <code class="ib jf jg jh jb b">false</code> to see what happens in your browser. But when you deploy your function, you want the <code class="ib jf jg jh jb b">headless</code> option to be set to <code class="ib jf jg jh jb b">true</code> (otherwise it won’t work). So here is the perfect place to use an environment variable as the value of <code class="ib jf jg jh jb b">headless</code>.</p><h2 id="66a2" class="ja ii ed bc bb ij jz ka kb kc kd ke kf kg kh ki kj">Optimization</h2><p id="3331" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj dv">Finally, a very easy way to reduce the execution time of your cloud function is to parallelize text inputs in forms. If you have forms to fill, instead of doing several <code class="ib jf jg jh jb b">await page.type('.selector', fieldValue)</code>, parallelize them in a <code class="ib jf jg jh jb b">Promise.all</code>. Of course, the submitting of the form must be done outside of this <code class="ib jf jg jh jb b">Promise.all</code> to have valid field values.</p><h1 id="ab8b" class="ih ii ed bc bb ij ef ik eh il im in io ip iq ir is">Sources</h1><ul class=""><li id="00ca" class="gv gw ed bc gy b gz it hb iu hd iv hf iw hh ix hj ji jj jk"><a href="https://pptr.dev/" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">Puppeteer documentation</a></li><li id="bb39" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><a href="https://cloud.google.com/sdk/docs/" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">Google Cloud SDK documentation</a></li><li id="094e" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><a href="https://cloud.google.com/functions/docs/quickstart" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">Google Cloud Functions Quickstart</a></li><li id="5fab" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><a href="https://github.com/GoogleChrome/puppeteer/issues" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">Github Puppeteer issues</a>: sometimes better than the documentation!</li><li id="46f4" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><a href="https://code.tutsplus.com/tutorials/the-30-css-selectors-you-must-memorize--net-16048" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">A list of 30 useful CSS selectors</a>, good to have precise DOM selectors</li><li id="70a7" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><a href="https://yarnpkg.com/en/docs/install" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">Yarn documentation</a></li><li id="95a9" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><a href="https://nodejs.org/dist/latest-v11.x/docs/api/" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">Node documentation</a></li><li id="4787" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk">Two other great articles about the Puppeteer and Google Cloud Functions : <a href="https://rominirani.com/using-puppeteer-in-google-cloud-functions-809a14856e14" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">here</a> and <a class="fn ch jr js jt ju" target="_blank" rel="noopener" href="/@ebidel/puppeteering-in-firebase-google-cloud-functions-76145c7662bd">here</a></li><li id="eb2c" class="gv gw ed bc gy b gz jl hb jm hd jn hf jo hh jp hj ji jj jk"><a href="https://gist.github.com/Alezco" class="fn ch jr js jt ju" target="_blank" rel="noopener nofollow">My personal gist</a>, containing the code examples of this article</li></ul><p id="1902" class="gv gw ed bc gy b gz ha hb hc hd he hf hg hh hi hj dv">I hope you found this article useful! Feel free to give me your feedback and ask any questions :)</p></div></div></section></div><div><div class="ds u dt du dv dw"></div><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><div><div id="e7b7" class="ed ee ef at eg b eh ei ej ek el em en eo ep eq er"><h1 class="eg b eh es ej et el eu en ev ep ew ef">How to scrape websites with Python and BeautifulSoup</h1></div><div class="ex"><div class="n ey ez fa fb"><div class="o n"><div><a rel="noopener" href="/@GuillaumeOdier?source=post_page-----c9f0dac5e928----------------------"><img alt="Guillaume Odier" class="r fc fd fe" src="https://miro.medium.com/fit/c/96/96/0*BdTFzdZCu482BaXl.png" width="48" height="48"></a></div><div class="ff ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r ef q"><div class="fg n o fh"><span class="as cx fi au cd fj fk fl fm fn ef"><a class="dc dd bb bc bd be bf bg bh bi fo bl bm fp fq" rel="noopener" href="/@GuillaumeOdier?source=post_page-----c9f0dac5e928----------------------">Guillaume Odier</a></span><div class="fr r ap h"><button class="fs ef q bq ft fu fv fw bi fp fx fy fz ga gb gc bt as b at gd cy aw bu bv bw bx by bl">Follow</button></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cx fi au cd fj fk fl fm fn ax"><div><a class="dc dd bb bc bd be bf bg bh bi fo bl bm fp fq" rel="noopener" href="/captain-data-blog/how-to-scrape-websites-with-python-and-beautifulsoup-c9f0dac5e928?source=post_page-----c9f0dac5e928----------------------">Nov 8, 2018</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n ge gf gg gh gi gj gk gl ab"><div class="n o"><div class="gm r ap"><a href="//medium.com/p/c9f0dac5e928/share/twitter?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gm r ap"><a href="//medium.com/p/c9f0dac5e928/share/facebook?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gp r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fcaptain-data-blog%2Fhow-to-scrape-websites-with-python-and-beautifulsoup-c9f0dac5e928&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="gr gs gt gu gv gw do dp paragraph-image"><div class="gx gy gz ha ak"><div class="do dp gq"><div class="hg r gz hh"><div class="hi r"><div class="hb hc ds t u hd ak cd he hf"><img class="ds t u hd ak hj hk hl" src="https://miro.medium.com/max/60/1*9yn_stb9a0OWJK_Qog6urw.png?q=20" width="1281" height="714" role="presentation"></div><img class="hb hc ds t u hd ak hm" width="1281" height="714" role="presentation"><noscript><img class="ds t u hd ak" src="https://miro.medium.com/max/2562/1*9yn_stb9a0OWJK_Qog6urw.png" width="1281" height="714" role="presentation"></noscript></div></div></div></div></figure><p id="57b2" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">What do you do when you can’t download a website’s information? You do it by hand? Wow, you’re brave!</p><p id="e325" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">I’m a web developer, so I’m way too lazy to do things manually 🙂</p><p id="2c64" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">If you’re about to scrape data for the first time, go ahead and read <a href="https://captaindata.co/blog/how-scrape-website/" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">How To Scrape A Website</a>. You can also read a small intro about <a href="https://captaindata.co/web-scraping" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">web scraping</a>.</p><p id="e2fa" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Today, let’s say that you need to enrich your CRM with company data.</p><p id="9c0e" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">To make it interesting for you, we will scrape <a href="https://angel.co/" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">Angel List</a>.</p><p id="e566" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">More specifically, we’ll scrape <a href="https://angel.co/uber" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">Uber’s company profile</a>.</p><blockquote class="if ig ih"><p id="3cfc" class="hn ho ef ii hp b hq hr hs ht hu hv hw hx hy hz ia dx"><em class="at">Please scrape responsibly!</em></p></blockquote><h1 id="9e90" class="ij ik ef at as il eh im ej in io ip iq ir is it iu">Getting started</h1><p id="98e9" class="hn ho ef at hp b hq iv hs iw hu ix hw iy hy iz ia dx">Before starting to code, be sure to have <strong class="hp ja">Python 3</strong> installed, as we won’t cover it here. Chances are you already have it installed.</p><p id="6ed7" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">You also need <code class="hh jb jc jd je b">pip</code>, a package management tool for Python.</p><pre class="gr gs gt gu gv jf jg cm"><span id="d3a6" class="jh ik ef at je b fi ji jj r jk">easy_install pip</span></pre><p id="9a42" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">The full code and dependencies are <a href="https://github.com/captaindatatech/scraping-examples/blob/master/scripts/Angel%20List%20Company%20Info.py" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">available here</a>.</p><p id="4f5a" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">We’ll be using BeautifulSoup, a standard Python scraping library.</p><pre class="gr gs gt gu gv jf jg cm"><span id="c896" class="jh ik ef at je b fi ji jj r jk">pip install BeautifulSoup4</span></pre><p id="3579" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">You could also create a <a href="https://realpython.com/python-virtual-environments-a-primer/" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">virtual environment</a> and install all the dependencies inside the requirements.txt file:</p><pre class="gr gs gt gu gv jf jg cm"><span id="1c32" class="jh ik ef at je b fi ji jj r jk">pip install -r requirements.txt</span></pre><h1 id="3be1" class="ij ik ef at as il eh im ej in io ip iq ir is it iu">Inspecting Content</h1><p id="9900" class="hn ho ef at hp b hq iv hs iw hu ix hw iy hy iz ia dx">Open <a href="https://angel.co/uber" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">https://angel.co/uber</a> in your web browser (I recommend using Chrome).</p><p id="8f78" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Right-click and open your browser’s inspector.</p><figure class="gr gs gt gu gv gw do dp paragraph-image"><div class="do dp jl"><div class="hg r gz hh"><div class="jm r"><div class="hb hc ds t u hd ak cd he hf"><img class="ds t u hd ak hj hk hl" src="https://miro.medium.com/max/60/1*iVZ3ZDXCq7cJ3VeRZnw-1w.png?q=20" width="335" height="244" role="presentation"></div><img class="hb hc ds t u hd ak hm" width="335" height="244" role="presentation"><noscript><img class="ds t u hd ak" src="https://miro.medium.com/max/670/1*iVZ3ZDXCq7cJ3VeRZnw-1w.png" width="335" height="244" role="presentation"></noscript></div></div></div><figcaption class="ax fi jn jo jp dq do dp jq jr as cx">Sorry, it’s in French!</figcaption></figure><p id="38d5" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Hover your cursor on the description.</p><figure class="gr gs gt gu gv gw do dp paragraph-image"><div class="gx gy gz ha ak"><div class="do dp js"><div class="hg r gz hh"><div class="jt r"><div class="hb hc ds t u hd ak cd he hf"><img class="ds t u hd ak hj hk hl" src="https://miro.medium.com/max/60/1*9hXctTThkIj6AiB7HNXMRA.png?q=20" width="2508" height="796" role="presentation"></div><img class="hb hc ds t u hd ak hm" width="2508" height="796" role="presentation"><noscript><img class="ds t u hd ak" src="https://miro.medium.com/max/5016/1*9hXctTThkIj6AiB7HNXMRA.png" width="2508" height="796" role="presentation"></noscript></div></div></div></div></figure><p id="e2a0" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">This example is pretty straightforward: you want the <code class="hh jb jc jd je b"><strong class="hp ja">&lt;h2&gt;</strong></code> tag with the <strong class="hp ja">js-startup_high_concept</strong> class.</p><p id="7ee7" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">This would be the unique location of our data thanks to the <code class="hh jb jc jd je b">class</code> tags.</p><h1 id="7993" class="ij ik ef at as il eh im ej in io ip iq ir is it iu">Extracting Data</h1><p id="2618" class="hn ho ef at hp b hq iv hs iw hu ix hw iy hy iz ia dx">Let’s dive right in with a bit of code:</p><pre class="gr gs gt gu gv jf jg cm"><span id="9259" class="jh ik ef at je b fi ji jj r jk"># we'll get back to this <br>headers = {} </span><span id="f5eb" class="jh ik ef at je b fi ju jv jw jx jy jj r jk"># the Uber company page you're about to scrape! <br>company_page = '&lt;https://angel.co/uber&gt;' </span><span id="eb0e" class="jh ik ef at je b fi ju jv jw jx jy jj r jk"># open the page <br>page_request = request.Request(company_page, headers=headers) <br>page = request.urlopen(page_request) </span><span id="b34f" class="jh ik ef at je b fi ju jv jw jx jy jj r jk"># parse the html using beautifulsoup <br>html_content = BeautifulSoup(page, 'html.parser') <br>description = html_content.find('h2', attrs={'class': 'js-startup_high_concept'}) <br>print(description)</span></pre><p id="2334" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Let’s get into the details:</p><ul class=""><li id="c310" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia jz ka kb">We create a variable <strong class="hp ja"><em class="ii">headers</em></strong> (more on this very soon)</li><li id="cce6" class="hn ho ef at hp b hq kc hs kd hu ke hw kf hy kg ia jz ka kb">The <strong class="hp ja"><em class="ii">company_page</em></strong> variable is the page we’re targeting</li><li id="20ee" class="hn ho ef at hp b hq kc hs kd hu ke hw kf hy kg ia jz ka kb">Then we build our request. We inject the <strong class="hp ja"><em class="ii">company_page</em></strong> and <strong class="hp ja"><em class="ii">headers</em></strong> variable inside the <strong class="hp ja">Request</strong> object. Then we open the url with the parameterized request.</li><li id="3246" class="hn ho ef at hp b hq kc hs kd hu ke hw kf hy kg ia jz ka kb">We parse the HTML response with BeautifulSoup</li><li id="f715" class="hn ho ef at hp b hq kc hs kd hu ke hw kf hy kg ia jz ka kb">We look for our text content with the <strong class="hp ja"><em class="ii">find()</em></strong> method</li><li id="2178" class="hn ho ef at hp b hq kc hs kd hu ke hw kf hy kg ia jz ka kb">We print our result!</li></ul><p id="d1f7" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Save this as <a href="http://script.py/" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">script.py</a> and run it in your shell, like this <code class="hh jb jc jd je b">python script.py</code>.</p><p id="7ce3" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">You should get the following:</p><pre class="gr gs gt gu gv jf jg cm"><span id="a43e" class="jh ik ef at je b fi ji jj r jk">urllib.error.HTTPError: HTTP Error 403: Forbidden</span></pre><p id="b1b6" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Oh 🙁 What happened?</p><p id="1879" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Well, it seems that AngelList has detected that we are a bot. Clever people!</p><p id="ac96" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Okay, so change the <strong class="hp ja"><em class="ii">headers</em></strong> variable for this one:</p><pre class="gr gs gt gu gv jf jg cm"><span id="1098" class="jh ik ef at je b fi ji jj r jk">headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}</span></pre><p id="d696" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Run the code with <code class="hh jb jc jd je b">python script.py</code>. Now it should be good:</p><pre class="gr gs gt gu gv jf jg cm"><span id="708c" class="jh ik ef at je b fi ji jj r jk">&lt;h2 class="js-startup_high_concept u-fontSize15 u-fontWeight400 u-colorGray3"&gt; The better way to get there &lt;/h2&gt;</span></pre><p id="ba63" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Yeah! Our first piece of data 😀</p><p id="48fa" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Want to find the website? Easy:</p><pre class="gr gs gt gu gv jf jg cm"><span id="9856" class="jh ik ef at je b fi ji jj r jk"># we extract the website <br>website = html_content.find('a', attrs={'class': 'company_url'})<br>print(website)</span></pre><p id="3096" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">And you get:</p><pre class="gr gs gt gu gv jf jg cm"><span id="97bb" class="jh ik ef at je b fi ji jj r jk">&lt;a class="u-uncoloredLink company_url" href="http://www.uber.com/" rel= nofollow noopener noreferrer" target="_blank"&gt;uber.com&lt;/a&gt;</span></pre><p id="795e" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Ok, but how do I get the <strong class="hp ja">value</strong> of the website?</p><p id="311a" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Easy. Tell the program to extract the <strong class="hp ja">href</strong>:</p><pre class="gr gs gt gu gv jf jg cm"><span id="a1a5" class="jh ik ef at je b fi ji jj r jk">print(website['href'])</span></pre><p id="4fd3" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Make sure to use the <strong class="hp ja">strip()</strong> method, otherwise you’ll have big spaces:</p><pre class="gr gs gt gu gv jf jg cm"><span id="62e7" class="jh ik ef at je b fi ji jj r jk">description = description.text.strip()</span></pre><p id="32f0" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">I won’t cover in detail all the elements you could extract. If you’re having issues, you can always check <a href="https://devhints.io/xpath" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">this amazing XPath cheatsheet</a>.</p><h1 id="6a27" class="ij ik ef at as il eh im ej in io ip iq ir is it iu">Save results to CSV</h1><p id="2387" class="hn ho ef at hp b hq iv hs iw hu ix hw iy hy iz ia dx">Pretty useless to print data, right? We should definitely save it!</p><p id="519d" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">The Comma-Separated Values format is really a standard for this purpose. You can import it very easily in Excel or Google Sheets.</p><pre class="gr gs gt gu gv jf jg cm"><span id="9b8d" class="jh ik ef at je b fi ji jj r jk">import csv</span></pre><p id="2780" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Add the following lines:</p><pre class="gr gs gt gu gv jf jg cm"><span id="8d3b" class="jh ik ef at je b fi ji jj r jk"># open a csv with the append (a) parameter <br>with open('angel.csv', 'a') as csv_file: <br>    writer = csv.writer(csv_file) <br>    writer.writerow([description, website])</span></pre><p id="290c" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">What you get is a single line of data. Since we told the program to append every result, new lines won’t erase previous results.</p><h1 id="78b3" class="ij ik ef at as il eh im ej in io ip iq ir is it iu">Check out the whole script</h1><p id="7128" class="hn ho ef at hp b hq iv hs iw hu ix hw iy hy iz ia dx">The script is <a href="https://github.com/captaindatatech/scraping-examples/blob/master/scripts/Angel%20List%20Company%20Info.py" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">available here</a>.</p><div class="kh ki kj kk kl km"><a href="https://github.com/captaindatatech/scraping-examples/blob/master/scripts/Angel%20List%20Company%20Info.py" rel="noopener nofollow"><div class="kp n ap"><div class="kq n dr p kr ks"><h2 class="as il kt au ef"><div class="cd kn fk fl ko fn">captaindatatech/scraping-examples</div></h2><div class="ku r"><h3 class="as cx fi au ax"><div class="cd kn fk fl ko fn">Various Web Scraping Examples. Checkout how to scrape Angel List Company Info in our github.</div></h3></div></div><div class="kv r"><div class="kw r kx ky kz kv la lb lc"></div></div></div></a></div><h1 id="9606" class="ij ik ef at as il eh im ej in io ip iq ir is it iu">Conclusion</h1><p id="eead" class="hn ho ef at hp b hq iv hs iw hu ix hw iy hy iz ia dx">It wasn’t that hard, right?</p><p id="1803" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">We covered a very basic example. You could also add multiple pages and parse them inside a for loop.</p><p id="ac44" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">Remember how we got blocked by the website’s security and resolved this by adding a custom User-Agent? We wrote a small paper about <a href="https://captaindata.co/blog/anti-scraping/" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">anti-scraping</a> techniques. It’ll help you understand how websites try to block bots.</p><p id="b17d" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx">If you feel like web scraping is too difficult for you or you’re getting blocked, you can always <a href="https://captaindata.co/contact" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">contact us</a>!</p><p id="117d" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx"><a href="https://captaindata.co/api/angellist-company-profile/" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow">You can also use a more advanced version of this script on our platform</a>.</p></div></div></section><hr class="ld cx le lf lg jp lh li lj lk ll"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><p id="e200" class="hn ho ef at hp b hq hr hs ht hu hv hw hx hy hz ia dx"><em class="ii">Originally published at </em><a href="https://captaindata.co/blog/how-scrape-websites-python-beautifulsoup/" class="dc by ib ic id ie" target="_blank" rel="noopener nofollow"><em class="ii">captaindata.co</em></a><em class="ii"> on November 8, 2018.</em></p></div></div></section></div><div><div class="dz u ea eb ec ed"></div><section class="ee ef eg eh ei"><div class="n p"><div class="ac ae af ag ah ej aj ak"><div><div id="58fa" class="ek el em at en b eo ep eq er es et eu ev ew ex ey"><h1 class="en b eo ez eq fa es fb eu fc ew fd em">How to get the next page on Beautiful Soup</h1></div><div class="fe"><div class="n ff fg fh fi"><div class="o n"><div><a rel="noopener" href="/@davidmm1707?source=post_page-----85b743750df4----------------------"><img alt="DavidMM" class="r fj fk fl" src="https://miro.medium.com/fit/c/96/96/2*FXde8jim4cXJDME3ge9Amg.jpeg" width="48" height="48"></a></div><div class="fm ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r em q"><div class="fn n o fo"><span class="as de fp au cd fq cj ck cl cm em"><a class="dj dk bb bc bd be bf bg bh bi fr bl bm fs ft" rel="noopener" href="/@davidmm1707?source=post_page-----85b743750df4----------------------">DavidMM</a></span><div class="fu r ap h"><button class="fv em q bq fw fx fy fz bi fs ga gb gc gd ge gf bt as b at gg df aw bu bv bw bx by bl">Follow</button></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as de fp au cd fq cj ck cl cm ax"><div><a class="dj dk bb bc bd be bf bg bh bi fr bl bm fs ft" rel="noopener" href="/quick-code/how-to-get-the-next-page-on-beautiful-soup-85b743750df4?source=post_page-----85b743750df4----------------------">Aug 28, 2019</a> <!-- -->·<!-- --> <!-- -->5<!-- --> min read</div></span></span></div></div><div class="n gh gi gj gk gl gm gn go ab"><div class="n o"><div class="gp r ap"><a href="//medium.com/p/85b743750df4/share/twitter?source=post_actions_header---------------------------" class="dj dk bb bc bd be bf bg bh bi gq gr bl bm fs ft" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gp r ap"><a href="//medium.com/p/85b743750df4/share/facebook?source=post_actions_header---------------------------" class="dj dk bb bc bd be bf bg bh bi gq gr bl bm fs ft" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gs r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fquick-code%2Fhow-to-get-the-next-page-on-beautiful-soup-85b743750df4&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="dj dk bb bc bd be bf bg bh bi gq gr bl bm fs ft" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="gu gv gw gx gy gz dv dw paragraph-image"><div class="dv dw gt"><div class="hf r hg hh"><div class="hi r"><div class="ha hb dz t u hc ak cd hd he"><img class="dz t u hc ak hj hk hl" src="https://miro.medium.com/max/60/0*wZdLE2d3KCFUNSU0?q=20" width="688" height="360" role="presentation"></div><img class="ha hb dz t u hc ak hm" width="688" height="360" role="presentation"><noscript><img class="dz t u hc ak" src="https://miro.medium.com/max/1376/0*wZdLE2d3KCFUNSU0" width="688" height="360" role="presentation"></noscript></div></div></div></figure><p id="9369" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">It is easy to scrape a simple page, but how do we get the next page on Beautiful Soup? What can we do to crawl all the pages until we reach the end?</p><p id="a7b4" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Today, we are going to learn how to fetch all the items while Web Scraping by reaching to the next pages.</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="ib r"><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FCbZeugOo5ck%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DCbZeugOo5ck&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FCbZeugOo5ck%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" allowfullscreen="" frameborder="0" height="480" width="854" title="How to get the next page on Beautiful Soup" class="dz t u hc ak" scrolling="auto"></iframe></div></div><figcaption class="ax fp ic id ie dx dv dw if ig as de">Video version of this tutorial</figcaption></figure></div></div></section><hr class="ih de ii ij ik ie il im in io ip"><section class="ee ef eg eh ei"><div class="n p"><div class="ac ae af ag ah ej aj ak"><h1 id="1845" class="iq ir em at as cf eo is eq it iu iv iw ix iy iz ja">Getting Started</h1><figure class="gu gv gw gx gy gz dv dw paragraph-image"><div class="jb jc hg jd ak"><div class="dv dw gt"><div class="hf r hg hh"><div class="je r"><div class="ha hb dz t u hc ak cd hd he"><img class="dz t u hc ak hj hk hl" src="https://miro.medium.com/max/60/0*A7iEoM2hTb0YD2f9?q=20" width="688" height="368" role="presentation"></div><img class="ha hb dz t u hc ak hm" width="688" height="368" role="presentation"><noscript><img class="dz t u hc ak" src="https://miro.medium.com/max/1376/0*A7iEoM2hTb0YD2f9" width="688" height="368" role="presentation"></noscript></div></div></div></div></figure><p id="64d7" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">As the topic of this post is what to do to crawl next pages, instead of coding a Beautiful Soup script again, we are going to take the one we did previously.</p><p id="9156" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">If you are a beginner, please, do the ‘<a href="https://letslearnabout.net/python/beautiful-soup/your-first-web-scraping-script-with-python-beautiful-soup/" class="dj by jf jg jh ji" target="_blank" rel="noopener nofollow">Your first Web Scraping script with Python and Beautiful Soup</a>‘ tutorial first.</p><p id="3c81" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">If you know how to use Beautiful Soup, use this starting code in <a href="https://repl.it/@DavidMM1707/Best-CD-Price" class="dj by jf jg jh ji" target="_blank" rel="noopener nofollow">repl.it.</a></p><p id="8691" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">This code fetches us the albums from the band the user asks for. All of them? No, just the first 10 ones that are displayed on the first page. By now.</p><p id="2ab7" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Open a new repl.it file or copy-paste the code in your code editor: Now it’s time to code!</p></div></div></section><hr class="ih de ii ij ik ie il im in io ip"><section class="ee ef eg eh ei"><div class="n p"><div class="ac ae af ag ah ej aj ak"><h1 id="8697" class="iq ir em at as cf eo is eq it iu iv iw ix iy iz ja">Refactoring — Getting rid of the clutter</h1><figure class="gu gv gw gx gy gz dv dw paragraph-image"><div class="jb jc hg jd ak"><div class="dv dw gt"><div class="hf r hg hh"><div class="jj r"><div class="ha hb dz t u hc ak cd hd he"><img class="dz t u hc ak hj hk hl" src="https://miro.medium.com/max/60/0*8YN3cst3CFrEenKn?q=20" width="688" height="491" role="presentation"></div><img class="ha hb dz t u hc ak hm" width="688" height="491" role="presentation"><noscript><img class="dz t u hc ak" src="https://miro.medium.com/max/1376/0*8YN3cst3CFrEenKn" width="688" height="491" role="presentation"></noscript></div></div></div></div></figure><p id="56da" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Before adding features, we need to clean the clutter by refactoring.</p><p id="27e7" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">We are going to take blocks of code and placing them in their own functions, then calling that functions where the code was.</p><p id="5fd5" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Go to the end of the code and take the lines where we create the table:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/1a7f8884a7524fd3204f7e06c9b3681b" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="881e" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Cut them and create a function, for example, export_table_and_print, and put it after base_url and search_url:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/dca98939515d5a9d66ed54e9f892bb14" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="b424" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">We also added a ‘clean_band_name’ so the filename where we store the data doesn’t have empty spaces and it is all lowercase, so “ThE BeAtLES” search stores a ‘the_beatles_albums.csv’ file.</p><p id="9bc9" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Now, where the old code was, call the function, just at the end of the file:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/cef0e519d685f407ab6221e96c47b536" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="4a66" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">The first part is done. Run the code and check it is still working.</p><p id="1b38" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Go to the ‘for loop’ at around line 45. Take everything that involves in extracting values and adding them to ‘data’ (so, the whole code) and replace it with the ‘get_cd_attributes(cd)’.</p><p id="49cd" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">After the last function, create that function and paste the code:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/0d6b34b6db12f8ea14367ca7ecbf7bd5" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="9709" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Again, run the code and check it is still working. If it is not, compare your code with mine:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/a0a097a91ed89806b3015093d9b35b6a" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="91ef" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">t is working? Cool. Time to get ALL the albums!</p></div></div></section><hr class="ih de ii ij ik ie il im in io ip"><section class="ee ef eg eh ei"><div class="n p"><div class="ac ae af ag ah ej aj ak"><h1 id="fea7" class="iq ir em at as cf eo is eq it iu iv iw ix iy iz ja">Recursive function — The trick to get the next page</h1><figure class="gu gv gw gx gy gz dv dw paragraph-image"><div class="dv dw gt"><div class="hf r hg hh"><div class="jl r"><div class="ha hb dz t u hc ak cd hd he"><img class="dz t u hc ak hj hk hl" src="https://miro.medium.com/max/60/0*T5q-HFDG11EtQfkV?q=20" width="688" height="656" role="presentation"></div><img class="ha hb dz t u hc ak hm" width="688" height="656" role="presentation"><noscript><img class="dz t u hc ak" src="https://miro.medium.com/max/1376/0*T5q-HFDG11EtQfkV" width="688" height="656" role="presentation"></noscript></div></div></div></figure><p id="5c1c" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Ok, here’s the trick to get the job done: Recursiveness.</p><p id="aedf" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">We are going to create a “parse_page’ function. That function will fetch the 10 albums the page will have.</p><p id="5a85" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">After the function it is done, it is going to call itself again, with the next page, to parse it, over and over again until we have everything.</p><p id="c1bc" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Let me simplify it for you:</p><figure class="gu gv gw gx gy gz dv dw paragraph-image"><div class="dv dw gt"><div class="hf r hg hh"><div class="jm r"><div class="ha hb dz t u hc ak cd hd he"><img class="dz t u hc ak hj hk hl" src="https://miro.medium.com/max/54/0*yLfU_f7NP-AHSfYp?q=20" width="688" height="774" role="presentation"></div><img class="ha hb dz t u hc ak hm" width="688" height="774" role="presentation"><noscript><img class="dz t u hc ak" src="https://miro.medium.com/max/1376/0*yLfU_f7NP-AHSfYp" width="688" height="774" role="presentation"></noscript></div></div></div></figure><p id="cbfb" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">I hope it is clear: As we keep having a ‘next page’ to parse, we are going to call the same function again and again to fetch all the data. When there is no more, we stop. As simple as that.</p><h2 id="0843" class="jn ir em at as cf jo jp jq jr js jt ju jv jw jx jy">Step 1: Create the function</h2><p id="9f96" class="hn ho em at hp b hq jz hs ka hu kb hw kc hy kd ia ee">Grab this code, create another function called ‘parse_page(url)’ and call that function at the last line.</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/2befd8d27692e77ed9b82f90bef1077f" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="363d" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">The data object is going to be used in different places, take it out and put it after the search_url.</p><p id="b3a8" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">We took the main code and created a parse_page function, called it using the ‘search_url’ as parameter and took the ‘data’ object out so we can use it globally.</p><p id="d9e4" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">In case you are dizzy, here’s what your code should look like now:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/936731ebb59d32448cd67f5a3808bf68" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="0d5d" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Please check this line:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/bb6d054c9530de946d4ddff81be86fce" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="830b" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Now we are not fetching the ‘search_url’ (the first one) but the URL that we pass as an argument. This is very important.</p><h2 id="a619" class="jn ir em at as cf jo jp jq jr js jt ju jv jw jx jy">Step 2: Add recursion</h2><p id="ae07" class="hn ho em at hp b hq jz hs ka hu kb hw kc hy kd ia ee">Run the code again. It should fetch the 10 first albums as always.</p><p id="aa26" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">That’s why because we haven’t used recursion. Let’s write the code that will:</p><ul class=""><li id="78ef" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ke kf kg">Get all the pagination links</li><li id="c5ae" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">From all the links, grab the last one</li><li id="df36" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">Check if the last one has a ‘Next’ text</li><li id="4ef2" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">If it has it, get the relative (partial) url</li><li id="bb4a" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">Build the next page url by adding base_url and the relative_url</li><li id="811c" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">Call parse_page again with the next page url</li><li id="f360" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">If doesn’t has the ‘Next’ text, just export the table and print it</li></ul><p id="8b20" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Once we have fetched all the cd attributes (that’s it, after the ‘for cd in list_all_cd’ loop), add this line:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/077ae08d97cc0a4be01c405b9cdfac0f" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="a50b" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">We are getting all the ‘list item’ (or ‘li’) elements inside the ‘unordered list’ with the ‘SearchBreadcrumbs’ class. That’s the pagination list.</p><p id="5039" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Then, we go to the last one and get the text. Add this after the last code:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/b7e5d7df446d2dabbbceb2f9749c13b0" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="c1b1" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Now we check if ‘next_page_text’ has ‘Next’ as text. If it does, we take the partial url, we add it to the base to build the next_page_url. If it does not, there is no more pages, so we can create the file and print it.</p><p id="3baa" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">That’s all we need. Run the code, and now you are getting dozens, if not hundreds of items!</p><h2 id="9610" class="jn ir em at as cf jo jp jq jr js jt ju jv jw jx jy">Step 3: Fixing a small bug</h2><p id="9780" class="hn ho em at hp b hq jz hs ka hu kb hw kc hy kd ia ee">But we can still improve the code. Add this 4 lines after parsing the page with Beautiful Soup:</p><figure class="gu gv gw gx gy gz"><div class="hf r hg"><div class="jk r"><iframe src="https://medium.com/media/364f169c99c854011323b6744fd3f56f" allowfullscreen="" frameborder="0" height="0" width="0" title="python" class="dz t u hc ak" scrolling="auto"></iframe></div></div></figure><p id="7809" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Sometimes there is a ‘Next’ page when the numbers of albums are multiple of 10 (10, 20, 30, 40 and so on) but there is no album there. That makes the code to end without creating the file.</p><p id="6e28" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">With this code, it is fixed.</p><p id="3a5e" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Your coding is done! Congratulations!</p></div></div></section><hr class="ih de ii ij ik ie il im in io ip"><section class="ee ef eg eh ei"><div class="n p"><div class="ac ae af ag ah ej aj ak"><h1 id="77c3" class="iq ir em at as cf eo is eq it iu iv iw ix iy iz ja">Conclusion</h1><p id="add7" class="hn ho em at hp b hq jz hs ka hu kb hw kc hy kd ia ee">Let me summarize what we have done:</p><ul class=""><li id="0e26" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ke kf kg">We moved blocks of code with the same functionality to functions</li><li id="68ba" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">We put the scraping code inside a function and we call it passing the initial search_url</li><li id="4626" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">Inside the function, we scrap the code</li><li id="506d" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">After it is done, we check for the next URL</li><li id="626f" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">If there is a ‘next url‘, we call the function with the next page URL</li><li id="73c4" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">If not, we end the scraping and create the .csv file</li></ul><p id="055c" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Now it seems simpler, right?</p><p id="5c0f" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">I want to keep doing tutorials like this one, but I want to ask you what do you want to see:</p><ul class=""><li id="9ff7" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ke kf kg">Do you want more Web Scraping with Beautiful Soup or Scrapy?</li><li id="0402" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">Do you want me to teach how to make a Flask web app or a Django one?</li><li id="21b3" class="hn ho em at hp b hq kh hs ki hu kj hw kk hy kl ia ke kf kg">Or do you want to learn more Front-End things like Vue.js?</li></ul><p id="43c0" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Please, leave me a comment with what do you want to see in future posts.</p><p id="eda5" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">And if this tutorial has been useful to you, share it with your friends, on Twitter, Facebook or where you can help others.</p></div></div></section><hr class="ih de ii ij ik ie il im in io ip"><section class="ee ef eg eh ei"><div class="n p"><div class="ac ae af ag ah ej aj ak"><p id="638f" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee"><a href="https://repl.it/@DavidMM1707/Best-CD-Price-Next-Page" class="dj by jf jg jh ji" target="_blank" rel="noopener nofollow">Final code on Repl.it</a></p><p id="847a" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee"><a href="https://twitter.com/DavidMM1707" class="dj by jf jg jh ji" target="_blank" rel="noopener nofollow">Reach to me on Twitter</a></p><p id="a2bf" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee"><a href="https://www.youtube.com/channel/UC9OLm6YFRzr4yjlw4xNWYvg?sub_confirmation=1" class="dj by jf jg jh ji" target="_blank" rel="noopener nofollow">My Youtube tutorial videos</a></p><p id="3f5e" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee"><a href="https://github.com/david1707" class="dj by jf jg jh ji" target="_blank" rel="noopener nofollow">My Github</a></p><p id="7d7f" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Contact me: DavidMM1707@gmail.com</p><p id="fdd7" class="hn ho em at hp b hq hr hs ht hu hv hw hx hy hz ia ee">Keep reading <a href="https://letslearnabout.net/category/tutorial/" class="dj by jf jg jh ji" target="_blank" rel="noopener nofollow">more tutorials</a></p></div></div></section></div><div><div class="dx u dy dz ea eb"></div><section class="ec ed ee ef eg"><div class="n p"><div class="ac ae af ag ah eh aj ak"><div><div id="2e01" class="ei ej cm at ek b el em en eo ep eq er es et eu ev"><h1 class="ek b el ew en ex ep ey er ez et fa cm">Web Scraping with Python and BeautifulSoup</h1></div><div class="fb"><div class="n fc fd fe ff"><div class="o n"><div><a rel="noopener" href="/@imoisharma?source=post_page-----bf2d814cc572----------------------"><img alt="Mohit Sharma" class="r fg fh fi" src="https://miro.medium.com/fit/c/96/96/1*mbUxV0WrINAue8xaqsIL0A@2x.jpeg" width="48" height="48"></a></div><div class="fj ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r cm q"><div class="fk n o fl"><span class="as dd fm au cc fn ci cj ck cl cm"><a class="dh di bb bc bd be bf bg bh bi fo bl bm dl dm" rel="noopener" href="/@imoisharma?source=post_page-----bf2d814cc572----------------------">Mohit Sharma</a></span><div class="fp r ap h"><button class="fq cm q bq fr fs ft fu bi dl fv fw fx fy fz ga bt as b at gb de aw bu bv bw bx by bl">Follow</button></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as dd fm au cc fn ci cj ck cl ax"><div><a class="dh di bb bc bd be bf bg bh bi fo bl bm dl dm" rel="noopener" href="/incedge/web-scraping-bf2d814cc572?source=post_page-----bf2d814cc572----------------------">Sep 12, 2018</a> <!-- -->·<!-- --> <!-- -->9<!-- --> min read</div></span></span></div></div><div class="n gc gd ge gf gg gh gi gj ab"><div class="n o"><div class="gk r ap"><a href="//medium.com/p/bf2d814cc572/share/twitter?source=post_actions_header---------------------------" class="dh di bb bc bd be bf bg bh bi dj dk bl bm dl dm" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gk r ap"><a href="//medium.com/p/bf2d814cc572/share/facebook?source=post_actions_header---------------------------" class="dh di bb bc bd be bf bg bh bi dj dk bl bm dl dm" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fincedge%2Fweb-scraping-bf2d814cc572&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="dh di bb bc bd be bf bg bh bi dj dk bl bm dl dm" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="gt gu gv gw ak"><div class="dt du gm"><div class="hc r gv hd"><div class="he r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*pp7uaEHrKY5iiWw9?q=20" width="765" height="380" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="765" height="380" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1530/0*pp7uaEHrKY5iiWw9" width="765" height="380" role="presentation"></noscript></div></div></div></div></figure><h1 id="367b" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">I am back with another tutorial on how to do Web Scraping with Python and <a href="https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">BeautifulSoup</a>.</h1><h1 id="a431" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">What you’ll learn</h1><ul class=""><li id="dfee" class="hy hz cm at ia b ib ic id ie if ig ih ii ij ik il im in io">What is Web Scraping</li><li id="cb57" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Why we need Web Scraping</li><li id="07e0" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">At last, how to do Web Scraping using Python and BeautifulSoup</li></ul><p id="930d" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">When performing data science tasks, it’s common to want to use data found on the internet. You’ll usually be able to access this data in <em class="iz">CSV format</em>, or via an <a href="https://en.wikipedia.org/wiki/Application_programming_interface" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">Application Programming Interface</a> (API). However, there are times when the data you want can only be accessed as part of a web page. In cases like this, you’ll want to use a technique called <strong class="ia ja">web scraping</strong> to get the data from the web page into a format you can work within your analysis.</p><p id="f2ee" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Today, I’ll show you how to perform Web Scraping using Python3 and BeautifulSoup library.</p><blockquote class="jb jc jd"><p id="d76d" class="hy hz cm iz ia b ib iu id iv if iw ih ix ij iy il ec">Before moving forward, I would like to share some of the basic components of a Web page</p></blockquote><p id="0e08" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Whenever you visit a website or web page, your web browser makes a request to a web server. This request is called a <code class="hd je jf jg jh b">GET</code> request, since we’re getting files from the server. The server then sends back files that tell our browser how to render the page for us. The files fall into a few main types:</p><ul class=""><li id="320b" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il im in io"><a href="https://www.w3.org/TR/html/" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">HTML</a> — contain the main content of the page.</li><li id="71e5" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io"><a href="https://developer.mozilla.org/en-US/docs/Web/CSS" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">CSS</a> — add styling to make the page look nicer.</li><li id="1dd5" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io"><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">JS</a> — Javascript files add interactivity to web pages.</li><li id="271d" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Images — image formats, such as <a href="https://en.wikipedia.org/wiki/JPEG" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">JPG</a> and <a href="https://en.wikipedia.org/wiki/Portable_Network_Graphics" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">PNG</a> allow web pages to show pictures.</li></ul><p id="9a68" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">After our browser receives all the files, it renders the page and displays it to us. There’s a lot that happens behind the scenes to render a page nicely, but we don’t need to worry about most of it when we’re web scraping. When we perform web scraping, we’re interested in the main content of the web page, so we look at the HTML.</p><h1 id="1690" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">HTML</h1><h1 id="aa08" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">HTML is the standard markup language for creating Web pages.</h1><ul class=""><li id="7f2a" class="hy hz cm at ia b ib ic id ie if ig ih ii ij ik il im in io">HTML stands for Hyper Text Markup Language</li><li id="7dea" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">HTML describes the structure of Web pages using markup</li><li id="e927" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">HTML elements are the building blocks of HTML pages</li><li id="45a5" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">HTML elements are represented by tags</li><li id="0a68" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">HTML tags label pieces of content such as “heading”, “paragraph”, “table”, and so on</li><li id="b818" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Browsers do not display the HTML tags, but use them to render the content of the page</li></ul></div></div></section><hr class="ji dd jj jk jl jm jn jo jp jq jr"><section class="ec ed ee ef eg"><div class="n p"><div class="ac ae af ag ah eh aj ak"><h1 id="9f72" class="hj hk cm at as ce el js en jt hn ju hp jv hr jw ht">A Simple HTML Document</h1><h1 id="dea3" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">Example</h1><p id="7236" class="hy hz cm at ia b ib ic id ie if ig ih ii ij ik il ec">&lt;!DOCTYPE html&gt;<br>&lt;html&gt;<br>&lt;head&gt;<br>&lt;title&gt;Page Title&lt;/title&gt;<br>&lt;/head&gt;<br>&lt;body&gt;&lt;h1&gt;My First Heading&lt;/h1&gt;<br>&lt;p&gt;My first paragraph.&lt;/p&gt;</p><p id="2e5b" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">&lt;/body&gt;<br>&lt;/html&gt;</p><p id="c651" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec"><a href="https://www.w3schools.com/htmL/tryit.asp?filename=tryhtml_intro" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">Try it Yourself »</a></p><h1 id="2abc" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">Example Explained</h1><ul class=""><li id="aa8a" class="hy hz cm at ia b ib ic id ie if ig ih ii ij ik il im in io">The <code class="hd je jf jg jh b">&lt;!DOCTYPE html&gt;</code> declaration defines this document to be HTML5</li><li id="fd45" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">The <code class="hd je jf jg jh b">&lt;html&gt;</code> element is the root element of an HTML page</li><li id="9012" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">The <code class="hd je jf jg jh b">&lt;head&gt;</code> element contains meta information about the document</li><li id="2f4d" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">The <code class="hd je jf jg jh b">&lt;title&gt;</code> element specifies a title for the document</li><li id="6984" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">The <code class="hd je jf jg jh b">&lt;body&gt;</code> element contains the visible page content</li><li id="11e5" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">The <code class="hd je jf jg jh b">&lt;h1&gt;</code> element defines a large heading</li><li id="68a3" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">The <code class="hd je jf jg jh b">&lt;p&gt;</code> element defines a paragraph</li></ul><blockquote class="jb jc jd"><p id="166a" class="hy hz cm iz ia b ib iu id iv if iw ih ix ij iy il ec">More Details refer to this <a href="https://www.w3schools.com/html/default.asp" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">HTML Tutorials</a></p></blockquote><p id="c036" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">What is Web Scraping?</strong></p><p id="fd18" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">Web scraping</strong>, <strong class="ia ja">web harvesting</strong>, or <strong class="ia ja">web data extraction</strong> is <a href="https://en.wikipedia.org/wiki/Data_scraping" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">data scraping</a> used for <a href="https://en.wikipedia.org/wiki/Data_extraction" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">extracting data</a> from <a href="https://en.wikipedia.org/wiki/Website" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">websites</a>.<a href="https://en.wikipedia.org/wiki/Web_scraping#cite_note-Boeing2016JPER-1" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">[1]</a> Web scraping software may access the World Wide Web directly using the <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">Hypertext Transfer Protocol</a>, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a <a href="https://en.wikipedia.org/wiki/Internet_bot" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">bot</a> or <a href="https://en.wikipedia.org/wiki/Web_crawler" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">web crawler</a>. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local <a href="https://en.wikipedia.org/wiki/Database" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">database</a> or spreadsheet, for later <a href="https://en.wikipedia.org/wiki/Data_retrieval" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">retrieval</a> or <a href="https://en.wikipedia.org/wiki/Data_analysis" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">analysis</a>.</p><p id="0630" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">More details refer to <a href="https://en.wikipedia.org/wiki/Web_scraping" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">Wikipedia</a></p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jx"><div class="hc r gv hd"><div class="jy r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*axoXXE3STGLBd1N2?q=20" width="385" height="131" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="385" height="131" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/770/0*axoXXE3STGLBd1N2" width="385" height="131" role="presentation"></noscript></div></div></div></figure><p id="c43d" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">Why we need Web Scraping?</strong></p><p id="ae91" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">A large organization will need to keep itself updated with the information changes occurring in multitudes of websites. An intelligent web scraper will find new websites from which it needs to scrap the data. Intelligent approaches identify the changed data, extract it without extracting the unnecessary links present within and navigate between websites to monitor and extract information on a real-time basis efficiently and effectively. You can easily monitor several websites simultaneously while keeping up with the frequency of updates.</p><p id="14a0" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">You will observe, as has been mentioned earlier, that data across the websites constantly changes. How will know if a key change has been made by an organization? Let’s say there has been a personnel change in the organization, how will you find out about that? That’s where the alerts feature in web scraping comes to play. The intelligent web scraping techniques will alert you to the data changes that have occurred on a particular website, thus helping you keep an eye on opportunities and issues.</p><h1 id="ed95" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">Web Scraping using Python and BeautifulSoup</h1><p id="0368" class="hy hz cm at ia b ib ic id ie if ig ih ii ij ik il ec">Firstly, I will demonstrate you with very basic HTML web page. And later on, show you how to do web scraping on the real-world web pages.</p><p id="d5ba" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">The first thing we’ll need to do to scrape a web page is to download the page. We can download pages using the Python <a href="http://docs.python-requests.org/en/master/" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">requests</a> library. The requests library will make a <code class="hd je jf jg jh b">GET</code> request to a web server, which will download the HTML contents of a given web page for us. There are several different types of requests we can make using <code class="hd je jf jg jh b">requests</code>, of which <code class="hd je jf jg jh b">GET</code> is just one.</p><p id="1d57" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Let’s try downloading a simple sample website, <code class="hd je jf jg jh b"><a href="http://dataquestio.github.io/web-scraping-pages/simple.html" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">http://dataquestio.github.io/web-scraping-pages/simple.html</a></code>. We’ll need to first download it using the <a href="http://docs.python-requests.org/en/master/user/quickstart/#make-a-request" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">requests.get </a>method.</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="ka r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*JHkrrSEgzauK6uw8?q=20" width="620" height="321" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="321" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*JHkrrSEgzauK6uw8" width="620" height="321" role="presentation"></noscript></div></div></div></figure><p id="008b" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">After running our request, we get a <a href="http://docs.python-requests.org/en/master/user/quickstart/#response-content" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">Response</a> object. This object has a <code class="hd je jf jg jh b">status_code</code>property, which indicates if the page was downloaded successfully.</p><p id="7d2e" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">We can print out the HTML content of the page using the <code class="hd je jf jg jh b">content</code> property:</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kb r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*m9mC7W9TbCNgTUvU?q=20" width="620" height="293" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="293" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*m9mC7W9TbCNgTUvU" width="620" height="293" role="presentation"></noscript></div></div></div></figure><h1 id="ac67" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">BeautifulSoup</h1><p id="cc73" class="hy hz cm at ia b ib ic id ie if ig ih ii ij ik il ec">We can use the <a href="https://www.crummy.com/software/BeautifulSoup/" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">BeautifulSoup</a> library to parse this document, and extract the text from the <code class="hd je jf jg jh b">p</code> tag. We first have to import the library, and create an instance of the <code class="hd je jf jg jh b">BeautifulSoup</code> class to parse our document:</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kb r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*LIFVm1PXB1rgaQD1?q=20" width="620" height="293" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="293" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*LIFVm1PXB1rgaQD1" width="620" height="293" role="presentation"></noscript></div></div></div></figure><p id="f3ba" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">We can now print out the HTML content of the page, formatted nicely, using the <code class="hd je jf jg jh b">prettify</code> method on the <code class="hd je jf jg jh b">BeautifulSoup</code> object:</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kc r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*3QoZBn_qy4KBgLse?q=20" width="620" height="297" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="297" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*3QoZBn_qy4KBgLse" width="620" height="297" role="presentation"></noscript></div></div></div></figure><p id="d8a6" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">As all the tags are nested, we can move through the structure one level at a time. We can first select all the elements at the top level of the page using the <code class="hd je jf jg jh b">children</code> property of <code class="hd je jf jg jh b">soup</code>. Note that <code class="hd je jf jg jh b">children</code> returns a list generator, so we need to call the <code class="hd je jf jg jh b">list</code>function on it.</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kb r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*ClM0k7I56wWg7N_C?q=20" width="620" height="293" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="293" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*ClM0k7I56wWg7N_C" width="620" height="293" role="presentation"></noscript></div></div></div></figure><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kd r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*rnAfC5xoxEXvXiql?q=20" width="620" height="205" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="205" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*rnAfC5xoxEXvXiql" width="620" height="205" role="presentation"></noscript></div></div></div></figure><p id="fded" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">As you can see above, there are two tags here, <code class="hd je jf jg jh b">head</code>, and <code class="hd je jf jg jh b">body</code>. We want to extract the text inside the <code class="hd je jf jg jh b">p</code> tag, so we’ll dive into the body(Refer to just above, under html.children).</p><p id="237e" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Now, we can get the <code class="hd je jf jg jh b">p</code> tag by finding the children of the body tag</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="ke r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*99sx3Gg5_rTmWE-r?q=20" width="620" height="272" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="272" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*99sx3Gg5_rTmWE-r" width="620" height="272" role="presentation"></noscript></div></div></div></figure><p id="6035" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">we can use the <code class="hd je jf jg jh b">get_text</code> method to extract all of the text inside the tag.</p><h1 id="08b4" class="hj hk cm at as ce el hl en hm hn ho hp hq hr hs ht">Finding all instances of a tag at once</h1><p id="1a3e" class="hy hz cm at ia b ib ic id ie if ig ih ii ij ik il ec">What we did above was useful for figuring out how to navigate a page, but it took a lot of commands to do something fairly simple. If we want to extract a single tag, we can instead use the <code class="hd je jf jg jh b">find_all</code> method, which will find all the instances of a tag on a page.</p><p id="0cce" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">If you instead only want to find the first instance of a tag, you can use the <code class="hd je jf jg jh b">find</code>method, which will return a single <code class="hd je jf jg jh b">BeautifulSoup</code> object.</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kf r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*hRNR8nfl4r4NYlUf?q=20" width="620" height="308" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="308" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*hRNR8nfl4r4NYlUf" width="620" height="308" role="presentation"></noscript></div></div></div></figure><p id="009a" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">If you want to fork this notebook go to <a href="https://github.com/mohitsharma44official/Python-Web-Scraping-/blob/master/Web%20Scraping%20Tutorial.ipynb" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">Web Scraping Tutorial.</a></p><blockquote class="jb jc jd"><p id="b2ac" class="hy hz cm iz ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">Now, I’ll show you how to perform web scraping using </strong><a href="https://www.python.org/downloads/release/python-350/" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow"><strong class="ia ja">Python 3</strong></a><strong class="ia ja"> and the </strong><a href="https://www.crummy.com/software/BeautifulSoup/" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow"><strong class="ia ja">BeautifulSoup</strong></a><strong class="ia ja"> library. We’ll be scraping weather forecasts from the </strong><a href="http://www.weather.gov/" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow"><strong class="ia ja">National Weather Service</strong></a><strong class="ia ja">, and then analyzing them using the </strong><a href="http://pandas.pydata.org/" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow"><strong class="ia ja">Pandas</strong></a><strong class="ia ja"> library.</strong></p></blockquote><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kg r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*gsFmW_2wmeOxwEQJ?q=20" width="620" height="349" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="349" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*gsFmW_2wmeOxwEQJ" width="620" height="349" role="presentation"></noscript></div></div></div></figure><p id="9c03" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">We now know enough to proceed with extracting information about the local weather from the National Weather Service website. The first step is to find the page we want to scrape. We’ll extract weather information about downtown San Francisco from <a href="http://forecast.weather.gov/MapClick.php?lat=37.7772&amp;lon=-122.4168" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">this page</a>.</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kh r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*iPFAzDtLzeXam19Q?q=20" width="620" height="253" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="253" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*iPFAzDtLzeXam19Q" width="620" height="253" role="presentation"></noscript></div></div></div></figure><p id="bbd1" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Once you open this page then use <strong class="ia ja">CRTL+SHIFT+I </strong>to inspect the element, but here we are interested in this particular column (San Francisco CA).</p><p id="467b" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">So, by right-clicking on the page near where it says “Extended Forecast”, then clicking “Inspect”, we’ll open up the tag that contains the text “Extended Forecast” in the elements panel.</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="gt gu gv gw ak"><div class="dt du jz"><div class="hc r gv hd"><div class="ki r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*hmrLLRS8ABucVS9u?q=20" width="620" height="322" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="322" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*hmrLLRS8ABucVS9u" width="620" height="322" role="presentation"></noscript></div></div></div></div></figure><p id="4539" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">We can then scroll up in the elements panel to find the “outermost” element that contains all of the text that corresponds to the extended forecasts. In this case, it’s a <code class="hd je jf jg jh b">div</code> tag with the id <code class="hd je jf jg jh b">seven-day-forecast.</code></p><p id="765e" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Explore the div, you’ll discover that each forecast item (like “Tonight”, “Thursday”, and “Thursday Night”) is contained in a <code class="hd je jf jg jh b">div</code>with the class <code class="hd je jf jg jh b">tombstone-container</code>.</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kj r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*-GgEk-0vOw6-bLN9?q=20" width="620" height="319" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="319" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*-GgEk-0vOw6-bLN9" width="620" height="319" role="presentation"></noscript></div></div></div></figure><p id="2922" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">We now know enough to download the page and start parsing it. In the below code, we:</p><ul class=""><li id="152d" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il im in io">Download the web page containing the forecast.</li><li id="e69b" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Create a <code class="hd je jf jg jh b">BeautifulSoup</code> class to parse the page.</li><li id="c13e" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Find the <code class="hd je jf jg jh b">div</code> with id <code class="hd je jf jg jh b">seven-day-forecast</code>, and assign to <code class="hd je jf jg jh b">seven_day</code></li><li id="574d" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Inside <code class="hd je jf jg jh b">seven_day</code>, find each individual forecast item.</li><li id="356a" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Extract and print the first forecast item.</li></ul><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kk r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*mDANCbLPPTrwr7rP?q=20" width="620" height="323" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="323" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*mDANCbLPPTrwr7rP" width="620" height="323" role="presentation"></noscript></div></div></div></figure><p id="34b4" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">Extract and print the first forecast item</strong></p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="ka r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*gRqjKOfSoMXlZkbC?q=20" width="620" height="321" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="321" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*gRqjKOfSoMXlZkbC" width="620" height="321" role="presentation"></noscript></div></div></div></figure><p id="5904" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">As you can see, inside the forecast item <code class="hd je jf jg jh b">tonight</code> is all the information we want. There are <code class="hd je jf jg jh b">4</code> pieces of information we can extract:</p><ul class=""><li id="ca48" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il im in io">The name of the forecast item — in this case, <code class="hd je jf jg jh b">Today</code>.</li><li id="3e57" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">The description of the conditions — this is stored in the <code class="hd je jf jg jh b">title</code> property of <code class="hd je jf jg jh b">img</code>.</li><li id="8e14" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">A short description of the conditions — in this case, <code class="hd je jf jg jh b">Sunny</code>.</li><li id="0583" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">The temperature low — in this case, <code class="hd je jf jg jh b">69<em class="iz">°F</em></code>.</li></ul><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kk r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*TXLAF4HFIwdU1sig?q=20" width="620" height="323" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="323" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*TXLAF4HFIwdU1sig" width="620" height="323" role="presentation"></noscript></div></div></div></figure><p id="64aa" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Now that we know how to extract each individual piece of information, we can combine our knowledge with CSS selectors and list comprehensions to extract everything at once.</p><p id="bc63" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">In the below code</strong>:</p><p id="a85a" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Select all items with the class <code class="hd je jf jg jh b">period-name</code> inside an item with the class <code class="hd je jf jg jh b">tombstone-container</code> in <code class="hd je jf jg jh b">seven_day</code>.</p><p id="a272" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Use a list comprehension to call the <code class="hd je jf jg jh b">get_text</code> method on each <code class="hd je jf jg jh b">BeautifulSoup</code>object.</p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="ka r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*_IkAqANAYESvIjs0?q=20" width="620" height="321" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="321" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*_IkAqANAYESvIjs0" width="620" height="321" role="presentation"></noscript></div></div></div></figure><p id="f453" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">Combining our data into Pandas DataFrame</strong></p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="gt gu gv gw ak"><div class="dt du jz"><div class="hc r gv hd"><div class="ki r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*XYjEtU4LxT7hw_OE?q=20" width="620" height="322" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="322" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*XYjEtU4LxT7hw_OE" width="620" height="322" role="presentation"></noscript></div></div></div></div></figure><p id="647f" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">We can use a regular expression and the </strong><a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow"><strong class="ia ja">Series.str.extract</strong></a><strong class="ia ja"> method to pull out the numeric temperature values.</strong></p><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="ka r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*6uSQnZBhes0e-5fA?q=20" width="620" height="321" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="321" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*6uSQnZBhes0e-5fA" width="620" height="321" role="presentation"></noscript></div></div></div></figure><figure class="gn go gp gq gr gs dt du paragraph-image"><div class="dt du jz"><div class="hc r gv hd"><div class="kk r"><div class="gx gy dx t u gz ak cc ha hb"><img class="dx t u gz ak hf hg hh" src="https://miro.medium.com/max/60/0*j9bEzJBcFVPHmHGA?q=20" width="620" height="323" role="presentation"></div><img class="gx gy dx t u gz ak hi" width="620" height="323" role="presentation"><noscript><img class="dx t u gz ak" src="https://miro.medium.com/max/1240/0*j9bEzJBcFVPHmHGA" width="620" height="323" role="presentation"></noscript></div></div></div></figure><p id="36a2" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">If you want to fork this notebook go to <a href="https://github.com/mohitsharma44official/Python-Web-Scraping-/blob/master/Web%20Scraping.ipynb" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">Web Scraping</a> and <a href="https://github.com/mohitsharma44official/Python-Web-Scraping-" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow">GitHub</a></p><p id="3c17" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">I hope now you have a good understanding of how to Scrape the data from web pages. In the coming weeks, I’ll do web scraping on</p><ul class=""><li id="d34b" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il im in io">News articles</li><li id="3b9e" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Sports scores</li><li id="efe3" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Weather forecasts</li><li id="3384" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Stock prices</li><li id="209d" class="hy hz cm at ia b ib ip id iq if ir ih is ij it il im in io">Online retailer price etc.</li></ul><p id="203c" class="hy hz cm at ia b ib iu id iv if iw ih ix ij iy il ec">Hope you like this article!! Don’t forget to like this blog and share with others.</p><blockquote class="jb jc jd"><p id="c592" class="hy hz cm iz ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">Thank You</strong></p><p id="5a6f" class="hy hz cm iz ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">Go Subscribe </strong><a href="https://themenyouwanttobe.wordpress.com" class="dh by hu hv hw hx" target="_blank" rel="noopener nofollow"><strong class="ia ja">THEMENYOUWANTTOBE</strong></a></p><p id="799b" class="hy hz cm iz ia b ib iu id iv if iw ih ix ij iy il ec"><strong class="ia ja">Show Some Love ❤</strong></p></blockquote></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="0977" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How to do Web Scraping with Ruby?</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@hello_47260?source=post_page-----80a705d041a----------------------"><img alt="Ellina Bereza" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/0*xFa1CwVoPDPOfsx7" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@hello_47260?source=post_page-----80a705d041a----------------------">Ellina Bereza</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@hello_47260/how-to-do-web-scraping-with-ruby-80a705d041a?source=post_page-----80a705d041a----------------------">Aug 20, 2018</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/80a705d041a/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/80a705d041a/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40hello_47260%2Fhow-to-do-web-scraping-with-ruby-80a705d041a&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="3c31" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Web scraping is a popular method of automatically collecting the information from different websites. It allows you to quickly obtain the data without the necessity to browse through the numerous pages and copy and paste the data. Later, it is outputted into a CSV file with structured information. Scraping tools are also capable of actualizing the changing information.</p><p id="5d6c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">There are numerous applications, websites, and browser plugins allowing you to parse the information quickly and efficiently. It is also possible to create your own web scraper — this is not as hard as it may seem.</p><p id="3d3a" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">In this article, you will learn more about web scraping, its types, and possible applications. We will also tell you how to scrape websites with Ruby.</p><h1 id="e1fd" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Ways of collecting the information</h1><p id="d904" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">There are two ways to automatically collect the information: web scraping and web crawling. They are both used for extracting the content from websites, but the areas of work are different.</p><p id="78a7" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">Web scraping</strong> refers to collecting the data from a particular source (website, database) or a local machine. It does not involve working with large datasets, and a simple download of the web page is considered to be a sort of data scraping.</p><p id="90e6" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">Web crawling</strong> implements processing large sets of data on numerous resources. The crawler attends the main page of the website and gradually scans the entire resource. Generally, the bot is programmed to attend numerous sites of the same type (for example, internet furniture shops).</p><p id="4558" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Both processes result in presenting the output of the collected information. Since the Internet is an open network, and the same content can be reposted on different resources, the output can contain lots of duplicated information. Data crawling involves processing the output and removing the duplicates. This can also be done while scraping the information, but it is not necessarily part of it.</p><h1 id="3b93" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">How web scraping works and how to choose the tool</h1><p id="9688" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">The scraping scripts are executed according to the following algorithm: the program attends the web page and selects the necessary HTML-elements according to the settled CSS- or XPath-selectors. The necessary information is processed, and the result is saved in the document.</p><p id="c934" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">The web provides quite a lot of out-of-box scraping tools like online and desktop applications, browser extensions, etc. They provide different functionalities that are suitable for different needs. That is why choosing a web scraper requires a bit of market research. Let’s have a look at the key features to consider when choosing a web scraping tool.</p><p id="cdb4" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">The different scrapers process different types of information: articles, blog and forum comments, internet shop databases, tables, dropdowns, Javascript elements, etc. The result can also be presented in different formats, like XML or CSV, or be written right into a database.</p><p id="5ef1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">The out-of-box scrapers can provide a free and commercial license. The free tools generally have fewer options for customization, less capacity, and less thorough scraping. The paid scrapers offer wider functionality and efficiency of work and are perfectly suited for professional usage.</p><ul class=""><li id="230f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gs gt gu"><strong class="fo gr">Technical background for usage</strong></li></ul><p id="57bf" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Some of the tools can be used just via the visual interface, without writing any lines of code. The other ones require a basic technical background. There are also tools for advanced computer users. The difference between them is in the customization options.</p><p id="848b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">It is also possible to develop a custom web scraper from scratch. The application can be written on any of the existing programming languages, including Ruby. The custom Ruby parser will have all the necessary functionality and the output information will be pre-processed exactly the way you need it.</p><p id="a4aa" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Having considered the existing types of web scraping tools, let’s see how to choose a scraper according to your needs:</p><ul class=""><li id="cf52" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gs gt gu">A <strong class="fo gr">free out-of-box tool</strong> will be sufficient for processing small amounts of information for personal use.</li><li id="bdf8" class="fm fn dc bk fo b fp gv fr gw ft gx fv gy fx gz fz gs gt gu">A scraper with a <strong class="fo gr">paid license</strong> is necessary for users collecting large, yet similar sets for information for business and scientific needs (e.g. collecting financial statistics).</li><li id="6575" class="fm fn dc bk fo b fp gv fr gw ft gx fv gy fx gz fz gs gt gu">A <strong class="fo gr">customized tool </strong>for scraping the web with Ruby is suitable for users who need a fully customized tool for professional scraping tasks on a regular basis.</li></ul><h1 id="89bb" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">The application of web scraping</h1><p id="947a" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">Data scraping and crawling are used for processing sets of unstructured information and logically presenting them as a database or a spreadsheet. The output is valuable information for analysts and researchers, and it can be applied in many different areas.</p><p id="54d4" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">The Ruby web crawler can collect the information from different resources, and output the dynamics of market changes (such as changes of currency rates, prices for securities, oil, gold, estate, etc). The output can then be used for predictive analytics and training of artificial intelligence.</p><ul class=""><li id="d86f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gs gt gu"><strong class="fo gr">Collecting product characteristics and prices</strong></li></ul><p id="a4f5" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Web scraping is widely used by aggregators — they collect the information about the goods in different internet shops, and later present it on their websites. This gives the users the opportunity to compare the prices and characteristics of the necessary item on different platforms without having to browse through numerous sites.</p><ul class=""><li id="2d08" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gs gt gu"><strong class="fo gr">Collecting contact details</strong></li></ul><p id="2575" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Web scraping can be useful for establishing both B2B and B2C relationships. With the help of scraping tools, companies can create lists of suppliers, partners, etc., and collect the databases of existing and potential clients. In other words, web scraping can help to obtain the lists of any individuals of interests.</p><ul class=""><li id="bc51" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gs gt gu"><strong class="fo gr">Collecting job opportunities</strong></li></ul><p id="e567" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Recruitment companies can extract the contact details of potential applicants for different vacancies, and vice versa — the information about job opportunities in different companies can be collected as well. This output is a good base not only for finding the necessary specialists and jobs, but also for market analysis, creating statistics about the demand and requirements for the different specialists, their salary rates, etc.</p><ul class=""><li id="acd0" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gs gt gu"><strong class="fo gr">Collecting information on a topic</strong></li></ul><p id="c1c0" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">With the help of scraping, you can download all the necessary information in bulk and then use it offline. For example, it is possible to extract all the questions and answers on a particular topic from Quora or any other service for questions and answers. You can also collect blog posts or the results of internet searches.</p><ul class=""><li id="4f2b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gs gt gu"><strong class="fo gr">Conducting market research</strong></li></ul><p id="14d0" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Data scraping can be applied by marketing specialists for conducting research on a target audience, collecting the email base for newsletters, etc. It helps to monitor competitors’ activities and track if they are changing their catalogs. SEO specialists can also scrape the web pages of competitors in order to analyze the semantics of the website.</p><h1 id="815f" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">How to do web scraping using Ruby</h1><p id="c861" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">Having considered the variety of web scraping tools and the possible ways to apply the scraped data, now let’s talk about creating your own custom tool. We are going to present you with a brief guide covering the basic stages of web scraping in Ruby.</p><h1 id="2a90" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Useful tools</h1><p id="e5ab" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">This language provides a wide range of ready-made tools for performing typical operations. They allow developers to use official and reliable solutions instead of reinventing the wheel. For Ruby web scraping, you will need to install the following gems on your computer:</p><ul class=""><li id="e564" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz gs gt gu"><a href="https://rubygems.org/gems/nokogiri/versions/1.6.6.2" class="at cg ha hb hc hd" target="_blank" rel="noopener nofollow">NokoGiri</a> is an HTML, SAX and RSS parser providing access to the elements based on XPath and CSS3-selectors. This gem can be applied not only for web parsing but also for processing different types of XML files.</li><li id="4298" class="fm fn dc bk fo b fp gv fr gw ft gx fv gy fx gz fz gs gt gu"><a href="https://rubygems.org/gems/httparty" class="at cg ha hb hc hd" target="_blank" rel="noopener nofollow">HTTParty</a> is a client for RESTful services, sending HTTP queries to the scrapped pages and automatic parsing of JSON and XML files to your Ruby storage.</li><li id="690e" class="fm fn dc bk fo b fp gv fr gw ft gx fv gy fx gz fz gs gt gu"><a href="https://rubygems.org/gems/pry" class="at cg ha hb hc hd" target="_blank" rel="noopener nofollow">Pry</a> is a tool used for debugging. It will help us to parse the code from the scrapped pages.</li></ul><p id="51a2" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Web scraping is quite a simple operation and, generally, there is no need to install the Rails framework for this. However, it does make sense if the scraper is part of a more complicated service.</p><p id="d63f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Having installed the necessary gems, you are now ready to learn how to make a web scraper. Let’s proceed!</p><h1 id="3ac1" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Step 1. Creating the scraping file</h1><p id="a9d3" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">Create the directory where the application data will be stored. Then add a blank text file named after the application and save it to the folder. Let’s call it “web_scraper.rb”.</p><p id="44c9" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">In the file, integrate the Nokogiri, HTTParty and Pry gems by running these commands:</p><p id="5f4e" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">require ‘nokogiri’</strong></p><p id="87e7" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">require ‘httparty’</strong></p><p id="b9c4" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">require ‘pry’</strong></p><h1 id="6a88" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Step 2. Sending the HTTP-queries</h1><p id="750c" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">Create a variable and send the HTTP-request to the page you are going to scrape:</p><p id="bdf5" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">page = HTTParty.get(‘https://www.iana.org/domains/reserved’)</strong></p><h1 id="7fb3" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Step 3. Launching NokoGiri</h1><p id="c025" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">The aim of this stage is to convert the list items into Nokogiri objects for further parsing. Set a new variable named “parsed_page” and make it equal to the Nokogiri method of converting the HTML data to objects — you will use it throughout the process.</p><p id="a29a" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">parsed_page = Nokogiri::HTML(page)</strong></p><p id="2892" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">Pry.start(binding)</strong></p><p id="aef0" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Save your file and launch it once again. Execute a “parsed_page” variable for retrieving the necessary page as the set of Nokogiri objects.</p><p id="39e4" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">In the same folder, create an HTML file (let’s call it “output”), and save the result of “parse page command” there. You will be able to refer to this document later.</p><p id="3e06" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Before proceeding, exit from Pry in the terminal.</p><h1 id="b001" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Step 4. Parsing</h1><p id="49b5" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">Now you need to extract all the needed list items. To do this, select the necessary CSS item and enter it to the Nokogiri output. You can locate the selector by viewing the page’s source code:</p><p id="067d" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">array = parsed_page.css(‘h2’).map(&amp;:text)</strong></p><p id="fa54" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Once the parsing is complete, it is necessary to export the parsed data to the CSV file so it won’t get lost.</p><h1 id="64e9" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Step 5. Export</h1><p id="1b77" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">Having parsed the information, you need to complete the scraping and convert the data into a structured table. Return to the terminal and execute the commands:</p><p id="ade7" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">require ‘csv’</strong></p><p id="1a24" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo gr">CSV.open(‘reserved.csv’, ‘w’) { |csv| csv &lt;&lt; array }</strong></p><p id="ece9" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">You will receive a new CSV file with all the parsed data inside.</p><h1 id="ec00" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Conclusion</h1><p id="dd80" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">We have covered the process of web scraping, its types, benefits, and possible applications. You are now aware of the basic features of the existing tools and know how to choose right one. If your business needs a customized solution, drop us a line. Our developers will create an application for web scraping on Ruby on Rails that will perfectly satisfy your needs.</p></div></div></section><hr class="he ef hf hg hh hi hj hk hl hm hn"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="80bf" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><em class="ho">Originally published at </em><a href="https://sloboda-studio.com/blog/how-to-do-web-scraping-with-ruby/" class="at cg ha hb hc hd" target="_blank" rel="noopener nofollow"><em class="ho">sloboda-studio.com</em></a><em class="ho"> on August 20, 2018.</em></p></div></div></section></div><div><div class="dq u dr ds dt du"></div><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><div><div id="197f" class="eb ec ed at ee b ef eg eh ei ej ek el em en eo ep"><h1 class="ee b ef eq eh er ej es el et en eu ed">Learn web app development while solving a real world problem</h1></div><div class="ev"><div class="n ew ex ey ez"><div class="o n"><div><a rel="noopener" href="/@sainyajay?source=post_page-----94203cdd6461----------------------"><img alt="Ajay Sainy" class="r fa fb fc" src="https://miro.medium.com/fit/c/96/96/0*kbEmHUGls4WAP006." width="48" height="48"></a></div><div class="fd ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r ed q"><div class="fe n o ff"><span class="as cw fg au cc fh fi fj fk fl ed"><a class="da db bb bc bd be bf bg bh bi fm bl bm de df" rel="noopener" href="/@sainyajay?source=post_page-----94203cdd6461----------------------">Ajay Sainy</a></span><div class="fn r ap h"><button class="fo ed q bq fp fq fr fs bi de ft fu fv fw fx fy bt as b at fz cx aw bu bv bw bx by bl">Follow</button></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cw fg au cc fh fi fj fk fl ax"><div><a class="da db bb bc bd be bf bg bh bi fm bl bm de df" rel="noopener" href="/analytics-vidhya/practical-web-scraping-94203cdd6461?source=post_page-----94203cdd6461----------------------">Jan 6</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n ga gb gc gd ge gf gg gh ab"><div class="n o"><div class="gi r ap"><a href="//medium.com/p/94203cdd6461/share/twitter?source=post_actions_header---------------------------" class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gi r ap"><a href="//medium.com/p/94203cdd6461/share/facebook?source=post_actions_header---------------------------" class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gj r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fpractical-web-scraping-94203cdd6461&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="gl gm gn go gp gq dm dn paragraph-image"><div class="gr gs gt gu ak"><div class="dm dn gk"><div class="ha r gt hb"><div class="hc r"><div class="gv gw dq t u gx ak cc gy gz"><img class="dq t u gx ak hd he hf" src="https://miro.medium.com/max/60/0*Mh245eSDxm5CQchB?q=20" width="6000" height="4000" role="presentation"></div><img class="gv gw dq t u gx ak hg" width="6000" height="4000" role="presentation"><noscript><img class="dq t u gx ak" src="https://miro.medium.com/max/12000/0*Mh245eSDxm5CQchB" width="6000" height="4000" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg hh hi hj do dm dn hk hl as cw">Photo by <a href="https://unsplash.com/@lucabravo?utm_source=medium&amp;utm_medium=referral" class="da by hm hn ho hp" target="_blank" rel="noopener nofollow">Luca Bravo</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="da by hm hn ho hp" target="_blank" rel="noopener nofollow">Unsplash</a></figcaption></figure><p id="ebcd" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">In this series, we will learn creating a web application from scratch, web scraping and storing the scraped data. All this while solving a real world problem.</p><p id="d26d" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">This is going to be a fun practical series divided into 3 part:</p><p id="1c9a" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">1. Scrape the data</p><p id="08a8" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">2. Store it</p><p id="a3d8" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">3. Create a web application</p><blockquote class="ie if ig"><p id="ac81" class="hq hr ed ih hs b ht hu hv hw hx hy hz ia ib ic id dv">Note: I am not sure if web scraping is illegal or not. It’s a complex topic to discuss. This series does not discuss the legal aspects of web scraping. However, I believe web scraping done ethically (debatable what is ethical) should not be a problem for the websites being scrapped.</p></blockquote><h1 id="8fda" class="ii ij ed at as ik ef il eh im in io ip iq ir is it"><strong class="bf">The problem we are going to solve</strong></h1><p id="6177" class="hq hr ed at hs b ht iu hv iv hx iw hz ix ib iy id dv">Assume that you are living in the USA and want to send some money to your friend or family in India. You would first google USD to INR rate, then you look for a money transfer service that allows you to send money from USD to INR. But, there are a lots of different services that provide different exchange rates, different service charges. First, you collect a list of such money transfer services and then you visit their websites to check what is the rate that they are providing. This takes a lot of time and effort.</p><p id="ce11" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">In this tutorial we are going solve this problem by creating a web-application that will show the exchange rates provided by these services at one place only. So that, you have to open only one website to decide which service to use.</p><p id="f566" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv"><strong class="hs iz">Lets solve the problem by breaking it into three parts:</strong></p><ol class=""><li id="5b98" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id ja jb jc">Collect data from various services</li><li id="7004" class="hq hr ed at hs b ht jd hv je hx jf hz jg ib jh id ja jb jc">Store the data</li><li id="f4cc" class="hq hr ed at hs b ht jd hv je hx jf hz jg ib jh id ja jb jc">Create web app using the data</li></ol><p id="53ca" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv"><strong class="hs iz">This article is going to cover the #1.</strong></p><p id="11b0" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">Sounds interesting?</p><p id="87f3" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">We have a lot to cover, so without wasting a moment, let’s get started.</p><h2 id="1c21" class="ji ij ed at as ik jj jk jl jm jn jo jp jq jr js jt">Requirements</h2><p id="94bc" class="hq hr ed at hs b ht iu hv iv hx iw hz ix ib iy id dv">1.<a href="https://www.python.org/downloads/" class="da by hm hn ho hp" target="_blank" rel="noopener nofollow"> Python (web scraping)</a></p><p id="15c1" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">2. <a href="https://pypi.org/project/beautifulsoup4/" class="da by hm hn ho hp" target="_blank" rel="noopener nofollow">BeautifulSoup (Python library for webscraping)</a></p><p id="e597" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">3. <a href="https://docs.python.org/3/library/urllib.html" class="da by hm hn ho hp" target="_blank" rel="noopener nofollow">urllib.request (Python library for opening URLs)</a></p><p id="cc2f" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">In this part, we will cover the web scraping section.</p><h1 id="337c" class="ii ij ed at as ik ef il eh im in io ip iq ir is it"><strong class="bf">Basics</strong></h1><p id="475d" class="hq hr ed at hs b ht iu hv iv hx iw hz ix ib iy id dv">In a typical client server scenario, client (eg. web browsers) sends a request to server. Server responds with data. For eg. When we open google.com in any web browser, the browser sends the request to google’s server to get the google search page. The google server returns the data in HTML and then browser renders the HTML and display beautiful UI to the user. The same thing happens when we open any other website. Web-Scraping is to read the HTML and get the required data/information from that HTML.</p><p id="b6a9" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">Lets understand this while solving our problem at hand. Follow the below steps:</p><h2 id="0855" class="ji ij ed at as ik jj jk jl jm jn jo jp jq jr js jt">Step 1: Select the target</h2><p id="f6a1" class="hq hr ed at hs b ht iu hv iv hx iw hz ix ib iy id dv">As we are going to create an application where users can view the USD to INR exchange rate offered by various services that lets users send money from USA to India. The obvious requirement is the list of such services. We are going to use Remitly and Transferwise (randomly selected).</p><h2 id="a2f2" class="ji ij ed at as ik jj jk jl jm jn jo jp jq jr js jt">Step 2: Find the element to scrape in the website</h2><p id="1437" class="hq hr ed at hs b ht iu hv iv hx iw hz ix ib iy id dv">We learned in the basics that all websites are in HTML(Hyper Text Markup Language). Underlying HTML of the website opened in commonly used browsers (chrome, safari, edge, firefox) can be easily seen by right clicking on the page and selecting <em class="ih">Inspect</em> option.</p><p id="7d2d" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">We want to get the USD to INR, so open the first website (<a href="https://www.remitly.com/us/en/india/pricing" class="da by hm hn ho hp" target="_blank" rel="noopener nofollow">Remitly</a>), right click on the place where it shows INR rate that you would like to scrape. In the developer console that gets opened, right click on the selected element and copy -&gt; Copy Selector.</p><figure class="gl gm gn go gp gq"><div class="ha r gt"><div class="ju r"><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F-UAubJFL--4%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D-UAubJFL--4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F-UAubJFL--4%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" allowfullscreen="" frameborder="0" height="480" width="854" title="How to get an exact CSS path of element using Google Developer Tool" class="dq t u gx ak" scrolling="auto"></iframe></div></div></figure><p id="a555" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">The <strong class="hs iz">selector</strong> is copied to the clipboard, save it for now (!important), we will use it later. Similarly, open the second website (<a href="https://transferwise.com/us/currency-converter/usd-to-inr-rate" class="da by hm hn ho hp" target="_blank" rel="noopener nofollow">Tranferwise</a>) and do the same.</p><figure class="gl gm gn go gp gq"><div class="ha r gt"><div class="ju r"><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FCOYVNSYJ-qs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DCOYVNSYJ-qs&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FCOYVNSYJ-qs%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" allowfullscreen="" frameborder="0" height="480" width="854" title="How to get the exact CSS path of an element using chrome developer tool." class="dq t u gx ak" scrolling="auto"></iframe></div></div></figure><p id="5e1e" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">Now we have the selectors (which is the CSS path to the element that displays the INR rate in the DOM). We will write Python script to <a href="https://go.skimresources.com/?id=126542X1588076&amp;xs=1&amp;url=https://en.wiktionary.org/wiki/programmatically" class="da by hm hn ho hp" target="_blank" rel="noopener nofollow">programmatically</a> make request (using urllib) to Remitly and Tranferwise web pages and read the HTML response (using BeautifulSoup library) and extract the INR rate using the selectors (obtained in previous step).</p><figure class="gl gm gn go gp gq"><div class="ha r gt"><div class="jv r"><iframe src="https://medium.com/media/9423f729bd7c4c5cb6909f1b380a60ed" allowfullscreen="" frameborder="0" height="0" width="0" title="Pyhon script to scrape USD to INR rate from remitly and tranferwise" class="dq t u gx ak" scrolling="auto"></iframe></div></div></figure><p id="85fa" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">The above script contains all the comments to explain what each line is doing. If it needs more explanation, let me know in comments section :). In short, the above script is performing the below steps:</p><p id="68fe" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">1. Call the page(url) that shows the exchange rate from USD to INR.</p><p id="cbef" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">2. Get the HTML.</p><p id="1ad2" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">3. Create a BeautifulSoup object to navigate the html easily.</p><p id="7e2e" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">4. Extract the rate using the selectors we got in step2.</p><h1 id="eded" class="ii ij ed at as ik ef il eh im in io ip iq ir is it">Execute</h1><p id="59b4" class="hq hr ed at hs b ht iu hv iv hx iw hz ix ib iy id dv">Copy the above script in a file and save it with <strong class="hs iz">.py</strong> extension (e.g. scrapper.py).</p><p id="0ca1" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">Run the script by executing below command in terminal/cmd (python3 should be installed already)</p><pre class="gl gm gn go gp jw jx cl"><span id="8ebc" class="ji ij ed at jy b fg jz ka r kb">CopyCopyCopypython scrapper.py </span></pre><p id="8788" class="hq hr ed at hs b ht hu hv hw hx hy hz ia ib ic id dv">Now, we have the nice script that scrapes the exchange rate from two money exchange services (Remitly &amp; Transferwise). This script can be easily extended to include more services without much changes in the code. Simply create new class for a new service and include the name of the service in the MONEY_TRANSER_SERVICES array. That’s it.</p><h1 id="ed9a" class="ii ij ed at as ik ef il eh im in io ip iq ir is it">Conclusion</h1><p id="824e" class="hq hr ed at hs b ht iu hv iv hx iw hz ix ib iy id dv">In this part, we saw how to extract the information from a web-page. In the next part we will see how to structure the data and store in MongoDB for long term storage. Stay tuned!</p></div></div></section></div><div><div class="dq u dr ds dt du"></div><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><div><div id="24ed" class="eb ec ed at ee b ef eg eh ei ej ek el em en eo ep"><h1 class="ee b ef eq eh er ej es el et en eu ed">Webscrape with Java, NodeJs &amp; Python</h1></div><div class="ev"><div class="n ew ex ey ez"><div class="o n"><div><a rel="noopener" href="/@aele54?source=post_page-----56117ed12b62----------------------"><img alt="Andrei Elekes" class="r fa fb fc" src="https://miro.medium.com/fit/c/96/96/2*wjWo7rxf5hk0eDb-LImfLw.jpeg" width="48" height="48"></a></div><div class="fd ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r ed q"><div class="fe n o ff"><span class="as cw fg au cc fh fi fj fk fl ed"><a class="da db bb bc bd be bf bg bh bi fm bl bm de df" rel="noopener" href="/@aele54?source=post_page-----56117ed12b62----------------------">Andrei Elekes</a></span><div class="fn r ap h"><button class="fo ed q bq fp fq fr fs bi de ft fu fv fw fx fy bt as b at fz cx aw bu bv bw bx by bl">Follow</button></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cw fg au cc fh fi fj fk fl ax"><div><a class="da db bb bc bd be bf bg bh bi fm bl bm de df" rel="noopener" href="/coding-in-simple-english/webscrape-with-java-nodejs-python-56117ed12b62?source=post_page-----56117ed12b62----------------------">Dec 16, 2019</a> <!-- -->·<!-- --> <!-- -->8<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewbox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div><div class="n ga gb gc gd ge gf gg gh ab"><div class="n o"><div class="gi r ap"><a href="//medium.com/p/56117ed12b62/share/twitter?source=post_actions_header---------------------------" class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gi r ap"><a href="//medium.com/p/56117ed12b62/share/facebook?source=post_actions_header---------------------------" class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gj r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fcoding-in-simple-english%2Fwebscrape-with-java-nodejs-python-56117ed12b62&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="da db bb bc bd be bf bg bh bi dc dd bl bm de df" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div><div class="gk"><div class="n p"><div class="gl gm gn go gp gq ag gr ah gs aj ak"><figure class="gt gu gv gw gx gk gy gz paragraph-image"><div class="ha hb hc hd ak"><div class="dm dn ai"><div class="hj r hc hk"><div class="hl r"><div class="he hf dq t u hg ak cc hh hi"><img class="dq t u hg ak hm hn ho" src="https://miro.medium.com/freeze/max/60/1*wjQtHlKnE3xDk4Y4ZqUSSg.gif?q=20" width="1152" height="648" role="presentation"></div><img class="he hf dq t u hg ak hp" width="1152" height="648" role="presentation"><noscript><img class="dq t u hg ak" src="https://miro.medium.com/max/2304/1*wjQtHlKnE3xDk4Y4ZqUSSg.gif" width="1152" height="648" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg hq hr hs do dm dn ht hu as cw"><a href="https://dribbble.com/kidwill" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure></div></div></div><div class="n p"><div class="ac ae af ag ah ea aj ak"><p id="125b" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv"><em class="in">So you need to extract data from a webpage into your application? How do you do it? Simple! Its called Webscaping and here’s how it's done.</em></p></div></div></section><hr class="io cw ip iq ir hs is it iu iv iw"><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><h1 id="1f1d" class="ix iy ed at as iz ef ja eh jb jc jd je jf jg jh ji">What Is Web Scraping? 🤷‍♂️</h1><blockquote class="jj jk jl"><p id="5879" class="hz ia ed in ib b ic id ie if ig ih ii ij ik il im dv"><strong class="ib jm">Web scraping</strong>, <strong class="ib jm">web harvesting</strong>, or <strong class="ib jm">web data extraction</strong> is <a href="https://en.wikipedia.org/wiki/Data_scraping" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">data scraping</a> used for <a href="https://en.wikipedia.org/wiki/Data_extraction" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">extracting data</a> from <a href="https://en.wikipedia.org/wiki/Website" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">websites</a>.</p></blockquote><p id="c834" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Webscraping software may access the World Wide Web directly using the <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Hypertext Transfer Protocol</a> or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a <a href="https://en.wikipedia.org/wiki/Internet_bot" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">bot</a> or <a href="https://en.wikipedia.org/wiki/Web_crawler" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">web crawler</a>. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local <a href="https://en.wikipedia.org/wiki/Database" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">database</a> or spreadsheet, for later <a href="https://en.wikipedia.org/wiki/Data_retrieval" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">retrieval</a> or <a href="https://en.wikipedia.org/wiki/Data_analysis" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">analysis</a>.</p><p id="bf78" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is the main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be <a href="https://en.wikipedia.org/wiki/Parsing" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">parsed</a>, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies, and their URLs, to a list (contact scraping).</p><p id="327e" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">There are, however, some web scraping software that will automatically load and extract data from multiple pages of websites based on your requirements. It is either custom-built for a specific website or is one that can be configured to work with any website. With the click of a button, you can easily save the data available on the website to a file on your computer.</p><p id="0d10" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Many services offer web scraping like <a href="https://medium.com/u/bbe5609206b0?source=post_page-----56117ed12b62----------------------" class="jn az by" target="_blank" rel="noopener">Scrapestorm Jp</a>, <a href="https://medium.com/u/3b0f669c90bc?source=post_page-----56117ed12b62----------------------" class="jn az by" target="_blank" rel="noopener">Grepsr</a>, and <a href="https://medium.com/u/4d3a276154a2?source=post_page-----56117ed12b62----------------------" class="jn az by" target="_blank" rel="noopener">ScrapingHub</a>. But today, I will be discussing how to build your own web scraper application using Java, NodeJs and Python.</p><h1 id="5ba7" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">Java WebScraper ☕️</h1><p id="64c8" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv">The best library to use for Java webscraping is <a href="https://jsoup.org/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Jsoup</a>.</p><blockquote class="jj jk jl"><p id="de98" class="hz ia ed in ib b ic id ie if ig ih ii ij ik il im dv"><code class="hk jy jz ka kb b"><em class="at">jsoup</em></code> is a Java library for working with real-world HTML. It provides a very convenient API for extracting and manipulating data, using the best of DOM, CSS, and jquery-like methods.</p></blockquote><p id="b41a" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv"><code class="hk jy jz ka kb b">jsoup</code> implements the <a href="https://whatwg.org/html" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">WHATWG HTML5</a> specification, and parses HTML to the same DOM as modern browsers do.</p><ul class=""><li id="306d" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im kc kd ke">scrape and <a href="https://jsoup.org/cookbook/input/parse-document-from-string" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">parse</a> HTML from a URL, file, or string</li><li id="28f4" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://jsoup.org/cookbook/extracting-data/selector-syntax" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">find</a> and extract data, using DOM traversal or CSS selectors</li><li id="f722" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://jsoup.org/cookbook/modifying-data/set-html" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">manipulate</a> the HTML elements, attributes, and text</li><li id="c090" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://jsoup.org/cookbook/cleaning-html/whitelist-sanitizer" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">clean</a> user-submitted content against a safe white-list, to prevent XSS attacks</li><li id="5280" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://jsoup.org/apidocs/org/jsoup/select/Elements.html#html--" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">output</a> tidy HTML</li></ul><p id="7c58" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">jsoup is designed to deal with all varieties of HTML found in the wild, from pristine and validating, to invalid tag-soup; jsoup will create a sensible parse tree.</p><p id="df4a" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Download the Jsoup JAR file from <a href="https://jsoup.org/download" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a> and then create a java class containing the URL that you need to scrape:</p><pre class="gt gu gv gw gx kk kl cl"><span id="0c6a" class="km iy ed at kb b fg kn ko r kp">import java.io.IOException;<br>import org.jsoup.Jsoup;</span><span id="3bae" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">public class JsoupFromStringEx {</span><span id="679a" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">    public static void main(String[] args) throws IOException {</span><span id="b9fa" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">        String webPage = "https://www.google.com/";</span><span id="9bf0" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">        String html = Jsoup.<em class="in">connect</em>(webPage).get().html();</span><span id="c309" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">        System.<em class="in">out</em>.println(html);</span><span id="2f6d" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">    }<br>}</span></pre><p id="b8bf" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">After running the java class, the webpage data should be printed out. This is the most basic way of webscraping in Java. Of course, this does not separate the data; many functions need to be placed for the application to do so. To create a more elaborate webscraping application follow <a href="https://stackabuse.com/web-scraping-the-java-way/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">this</strong></a><strong class="ib jm">.</strong></p><h1 id="bbc3" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">NodeJs WebScraper 🕸</h1><p id="7c57" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv">By using the superb tutorial <a href="https://pusher.com/tutorials/web-scraper-node" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a><strong class="ib jm">, </strong>we create a new <code class="hk jy jz ka kb b">scraper</code> directory for this tutorial and initialize it with a <code class="hk jy jz ka kb b">package.json</code> file by running <code class="hk jy jz ka kb b">npm init -y</code> from the project root. Then run this command to install all the dependencies needed:</p><pre class="gt gu gv gw gx kk kl cl"><span id="ed99" class="km iy ed at kb b fg kn ko r kp"><strong class="kb jm">npm install </strong>axios<strong class="kb jm"> </strong>cheerio<strong class="kb jm"> </strong>puppeteer<strong class="kb jm"> --save</strong></span></pre><p id="0eea" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Here’s what each one does:</p><ul class=""><li id="3e7e" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im kc kd ke"><a href="https://github.com/axios/axios" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">Axios</strong></a>: Promise-based HTTP client for Node.js and the browser</li><li id="a8fa" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://cheerio.js.org/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">Cheerio</strong></a>: jQuery implementation for Node.js. Cheerio makes it easy to select, edit, and view DOM elements.</li><li id="50f1" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://github.com/GoogleChrome/puppeteer" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">Puppeteer</strong></a>: A Node.js library for controlling Google Chrome or Chromium.</li></ul><p id="e1fe" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">When the installation is complete, create a new <code class="hk jy jz ka kb b">pl-scraper.js</code> file in the root of your project directory and populate it with the following code:</p><pre class="gt gu gv gw gx kk kl cl"><span id="fbef" class="km iy ed at kb b fg kn ko r kp"><em class="in">// pl-scraper.js</em></span><span id="bfff" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">const axios = require('axios');</span><span id="38c2" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">const url = 'https://www.premierleague.com/stats/top/players/goals?se=-1&amp;cl=-1&amp;iso=-1&amp;po=-1?se=-1';</span><span id="241f" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">axios(url)<br>      .then(response =&gt; {<br>        const html = response.data;<br>        console.log(html);<br>      })<br>      .catch(console.error);</span></pre><p id="b796" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">If you run the code with <code class="hk jy jz ka kb b"><strong class="ib jm">node</strong> pl-scraper.js</code>, a long string of HTML will be printed to the console.</p><p id="0de6" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">And that’s it, you just retrieved all the data from a webpage using a NodeJs webscraper. But how can you parse the HTML for the exact data you need? Continue following <a href="https://medium.com/u/2b9d77ff34df?source=post_page-----56117ed12b62----------------------" class="jn az by" target="_blank" rel="noopener">Pusher</a>’s tutorial <a href="https://pusher.com/tutorials/web-scraper-node" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a>.</p><h1 id="ab68" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">Python Webscraper 🐍</h1><p id="6cc6" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv">With reference to Python Docs found <a href="https://docs.python.org/3/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a><strong class="ib jm">, </strong>we start off by downloading <a href="http://lxml.de/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">lxml</a> that is a pretty extensive library written for parsing XML and HTML documents very quickly, even handling messed up tags in the process. We will also be using the <a href="http://docs.python-requests.org/en/latest/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Requests</a> module instead of the already built-in urllib2 module due to improvements in speed and readability. You can easily install both using <code class="hk jy jz ka kb b"><strong class="ib jm">pip</strong> <strong class="ib jm">install</strong> lxml</code> and <code class="hk jy jz ka kb b"><strong class="ib jm">pip</strong> <strong class="ib jm">install</strong> requests</code>.</p><p id="7918" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Let’s start with the imports:</p><pre class="gt gu gv gw gx kk kl cl"><span id="3c2d" class="km iy ed at kb b fg kn ko r kp"><strong class="kb jm">from</strong> lxml <strong class="kb jm">import</strong> html<br><strong class="kb jm">import</strong> requests</span></pre><p id="01b9" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Next, we will use <code class="hk jy jz ka kb b">requests.get</code> to retrieve the web page with our data, parse it using the <code class="hk jy jz ka kb b">html</code> module, and save the results in <code class="hk jy jz ka kb b">tree</code>:</p><pre class="gt gu gv gw gx kk kl cl"><span id="12a9" class="km iy ed at kb b fg kn ko r kp">page = requests.get<strong class="kb jm">(</strong>'http://econpy.pythonanywhere.com/ex/001.html'<strong class="kb jm">)</strong><br>tree = html.fromstring<strong class="kb jm">(</strong>page.content<strong class="kb jm">)</strong></span></pre><p id="7a35" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">(We need to use <code class="hk jy jz ka kb b">page.content</code> rather than <code class="hk jy jz ka kb b">page.text</code> because <code class="hk jy jz ka kb b">html.fromstring</code> implicitly expects <code class="hk jy jz ka kb b">bytes</code> as input.)</p><p id="70db" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv"><code class="hk jy jz ka kb b">tree</code> now contains the whole HTML file in a nice tree structure which we can go over two different ways: XPath and CSSSelect. In this example, we will focus on the former.</p><p id="527e" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">XPath is a way of locating information in structured documents such as HTML or XML documents. A good introduction to XPath is on W3Schools. There are also various tools for obtaining the XPath of elements such as FireBug for Firefox or the Chrome Inspector. If you’re using Chrome, you can right-click an element, choose ‘Inspect element’, highlight the code, right-click again, and choose ‘Copy XPath’.</p><p id="04be" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">After a quick analysis, we see that in our page the data is contained in two elements — one is a div with title ‘buyer-name’ and the other is a span with class ‘item-price’:</p><pre class="gt gu gv gw gx kk kl cl"><span id="5a82" class="km iy ed at kb b fg kn ko r kp"><strong class="kb jm">&lt;div</strong> title="buyer-name"<strong class="kb jm">&gt;</strong>Carson Busses<strong class="kb jm">&lt;/div&gt;</strong><br><strong class="kb jm">&lt;span</strong> class="item-price"<strong class="kb jm">&gt;</strong>$29.95<strong class="kb jm">&lt;/span&gt;</strong></span></pre><p id="a9a5" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Knowing this we can create the correct XPath query and use the lxml <code class="hk jy jz ka kb b">xpath</code> function like this:</p><pre class="gt gu gv gw gx kk kl cl"><span id="7455" class="km iy ed at kb b fg kn ko r kp"><em class="in">#This will create a list of buyers:</em><br>buyers = tree.xpath<strong class="kb jm">(</strong>'//div[@title="buyer-name"]/text()'<strong class="kb jm">)</strong><br><em class="in">#This will create a list of prices</em><br>prices = tree.xpath<strong class="kb jm">(</strong>'//span[@class="item-price"]/text()'<strong class="kb jm">)</strong></span></pre><p id="c439" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Let’s see what we got exactly:</p><pre class="gt gu gv gw gx kk kl cl"><span id="116c" class="km iy ed at kb b fg kn ko r kp"><strong class="kb jm">print</strong> 'Buyers: '<strong class="kb jm">,</strong> buyers<br><strong class="kb jm">print</strong> 'Prices: '<strong class="kb jm">,</strong> prices</span><span id="0d1b" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">Buyers<strong class="kb jm">:</strong>  <strong class="kb jm">[</strong>'Carson Busses'<strong class="kb jm">,</strong> 'Earl E. Byrd'<strong class="kb jm">,</strong> 'Patty Cakes'<strong class="kb jm">,</strong><br>'Derri Anne Connecticut'<strong class="kb jm">,</strong> 'Moe Dess'<strong class="kb jm">,</strong> 'Leda Doggslife'<strong class="kb jm">,</strong> 'Dan Druff'<strong class="kb jm">,</strong><br>'Al Fresco'<strong class="kb jm">,</strong> 'Ido Hoe'<strong class="kb jm">,</strong> 'Howie Kisses'<strong class="kb jm">,</strong> 'Len Lease'<strong class="kb jm">,</strong> 'Phil Meup'<strong class="kb jm">,</strong><br>'Ira Pent'<strong class="kb jm">,</strong> 'Ben D. Rules'<strong class="kb jm">,</strong> 'Ave Sectomy'<strong class="kb jm">,</strong> 'Gary Shattire'<strong class="kb jm">,</strong><br>'Bobbi Soks'<strong class="kb jm">,</strong> 'Sheila Takya'<strong class="kb jm">,</strong> 'Rose Tattoo'<strong class="kb jm">,</strong> 'Moe Tell'<strong class="kb jm">]</strong></span><span id="6ec9" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">Prices<strong class="kb jm">:</strong>  <strong class="kb jm">[</strong>'$29.95'<strong class="kb jm">,</strong> '$8.37'<strong class="kb jm">,</strong> '$15.26'<strong class="kb jm">,</strong> '$19.25'<strong class="kb jm">,</strong> '$19.25'<strong class="kb jm">,</strong><br>'$13.99'<strong class="kb jm">,</strong> '$31.57'<strong class="kb jm">,</strong> '$8.49'<strong class="kb jm">,</strong> '$14.47'<strong class="kb jm">,</strong> '$15.86'<strong class="kb jm">,</strong> '$11.11'<strong class="kb jm">,</strong><br>'$15.98'<strong class="kb jm">,</strong> '$16.27'<strong class="kb jm">,</strong> '$7.50'<strong class="kb jm">,</strong> '$50.85'<strong class="kb jm">,</strong> '$14.26'<strong class="kb jm">,</strong> '$5.68'<strong class="kb jm">,</strong><br>'$15.00'<strong class="kb jm">,</strong> '$114.07'<strong class="kb jm">,</strong> '$10.09'<strong class="kb jm">]</strong></span></pre><p id="898a" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Congratulations! We have successfully scraped all the data we wanted from a web page using lxml and Requests. We have it stored in memory as two lists. Now we can do all sorts of cool stuff with it: we can analyze it using Python, or we can save it to a file and share it with the world.</p><h1 id="3fbf" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">Caution ⚠️</h1><p id="31b2" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv">So is it legal or illegal? Web scraping and crawling aren’t illegal by themselves. After all, you could scrape or crawl your own website, without a hitch…</p><figure class="gt gu gv gw gx gk hp kw bv kx ky kz la lb bg lc ld le lf lg lh paragraph-image"><div class="ha hb hc hd ak"><div class="dm dn kv"><div class="hj r hc hk"><div class="hl r"><div class="he hf dq t u hg ak cc hh hi"><img class="dq t u hg ak hm hn ho" src="https://miro.medium.com/max/60/1*XMwWhmkmiSs484luuLRQ7Q.jpeg?q=20" width="1920" height="1080" role="presentation"></div><img class="he hf dq t u hg ak hp" width="1920" height="1080" role="presentation"><noscript><img class="dq t u hg ak" src="https://miro.medium.com/max/3840/1*XMwWhmkmiSs484luuLRQ7Q.jpeg" width="1920" height="1080" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg hq hr hs do dm dn ht hu as cw"><a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjou97ZrrzmAhUJaBoKHZePBakQjRx6BAgBEAQ&amp;url=https%3A%2F%2Fwallpaperaccess.com%2Fhd-law&amp;psig=AOvVaw2_rG6d_1UqvKqyjJUMBEMT&amp;ust=1576661150366495" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><p id="9091" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">In 2016, the US Congress passed its first legislation specifically to target bad bots — the <a href="https://www.congress.gov/bill/114th-congress/senate-bill/3183" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Better Online Ticket Sales (BOTS) Act</a>, which bans the use of software that circumvents security measures on ticket seller websites. Automated ticket scalping bots use several techniques to do their dirty work including web scraping that incorporates advanced business logic to identify scalping opportunities, input purchase details into shopping carts, and even resell inventory on secondary markets.</p><p id="0fad" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">In other words, if you’re a venue, organization or ticketing software platform, it is still on you to defend against this fraudulent activity during your major on sales. But of course, this depends on where in the world you are:</p><p id="4c17" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">The UK however, seems to have followed the US with its <a href="https://www.gov.uk/government/news/a-better-deal-for-consumers-in-the-digital-age" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Digital Economy Act 2017</a> which achieved Royal Assent in April. The Act seeks to protect consumers in a number of ways in an increasingly digital society, including by “cracking down on ticket touts by making it a criminal offence for those that misuse bot technology to sweep up tickets and sell them at inflated prices in the secondary market.”</p><p id="89e0" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">You can read more about this <a href="https://resources.distilnetworks.com/all-blog-posts/is-web-scraping-illegal-depends-on-what-the-meaning-of-the-word-is-is" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a><strong class="ib jm">.</strong></p><p id="88d3" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">To put that into perspective, companies themselves have the responsibility of protecting their own data from web scrapers as they have to invoke the law themselves. So before you go off and try to web scrape from a .gov webpage with your python program, think again!</p><h1 id="cc05" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">Use Cases 〽️</h1><p id="d3a3" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv"><a href="https://www.quora.com/What-are-examples-of-how-real-businesses-use-web-scraping-Are-there-any-types-of-businesses-which-use-this-more-than-others" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Businesses</a> use web scraping for different purposes and it varies on a case to case basis.</p><figure class="gt gu gv gw gx gk hp kw bv kx ky kz la lb bg lc ld le lf lg lh paragraph-image"><div class="ha hb hc hd ak"><div class="dm dn li"><div class="hj r hc hk"><div class="lj r"><div class="he hf dq t u hg ak cc hh hi"><img class="dq t u hg ak hm hn ho" src="https://miro.medium.com/freeze/max/60/1*wNGxHlTCsH9zU90WDouoDQ.gif?q=20" width="800" height="600" role="presentation"></div><img class="he hf dq t u hg ak hp" width="800" height="600" role="presentation"><noscript><img class="dq t u hg ak" src="https://miro.medium.com/max/1600/1*wNGxHlTCsH9zU90WDouoDQ.gif" width="800" height="600" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg hq hr hs do dm dn ht hu as cw"><a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwixrqj2r7zmAhUlzYUKHeRwAZ8QjRx6BAgBEAQ&amp;url=https%3A%2F%2Fdribbble.com%2Fshots%2F4171367-Coding-Freak&amp;psig=AOvVaw1Jdl8ZMhLi5rrYeb595BO2&amp;ust=1576661497136416" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><p id="7e9e" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">In <strong class="ib jm">eCommerce</strong>, Retailers/ marketplaces use web scraping to monitor their competitor prices and to improve their product attributes. Also, collect product reviews to do sentimental analysis. <strong class="ib jm">Lawyers</strong> use web scraping to see the past judgment report for their case reference. <strong class="ib jm">Lead generation</strong> companies use it to scrape the email address and phone numbers. <strong class="ib jm">Recruiters</strong> use it to collects user's profiles. Some <strong class="ib jm">travel companies</strong> collect data in real-time to provide live tracking details. <strong class="ib jm">Media companies</strong> collect trending topics and use hashtags to collect information from social media profiles. <strong class="ib jm">Business directories</strong> scrape complete information about the business profile, address, email, phone, products/services, working hours, Geocodes, etc.<br>Each business has competition in the present world, So companies scrape their competitor information regularly to monitor the movements. <strong class="ib jm">Government</strong> secret agencies also scrape for national securities purpose.</p><p id="108d" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">It's safe to say that webscaping is a big field, and you have just finished a brief tour of that field, using Java, NodeJs, and Python as your guide. You have also learned that it is illegal to scrape some sites, and you should check their terms and conditions before scraping. So do your webscraping wisely!</p><h1 id="43d5" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">References 📖</h1><div class="lk ll lm ln lo lp"><a href="https://www.webharvy.com/articles/what-is-web-scraping.html" rel="noopener nofollow"><div class="ls n ap"><div class="lt n dp p lu lv"><h2 class="as iz lw au ed"><div class="cc lq fi fj lr fl">Web Scraping Explained</div></h2><div class="lx r"><h3 class="as cw fg au ax"><div class="cc lq fi fj lr fl">Web Scraping (also termed Screen Scraping, Web Data Extraction, Web Harvesting, etc.) is a technique employed to extract…</div></h3></div><div class="ly r"><h4 class="as cw cx au ax"><div class="cc lq fi fj lr fl">www.webharvy.com</div></h4></div></div><div class="lz r"><div class="ma r mb mc md lz me mf mg"></div></div></div></a></div></div></div></section><hr class="io cw ip iq ir hs is it iu iv iw"><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><p id="caeb" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Still worried about implementing applications, API’s or backends? Oracle is here to help, with industry-standard cloud applications, their team of experts will make implementation more than enjoyable.</p><h2 id="85ee" class="km iy ed at as iz mh mi mj mk ml mm mn mo mp mq mr">☁️ Follow to get a <a href="http://bit.ly/2HzFQJE" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">free 30-day trial with Oracle Cloud services</a> ☁️</h2><p id="a983" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv"><em class="in">Thank you for taking the time to read my article, if you’re looking for more posts like this, you can find me on </em><a href="https://www.linkedin.com/in/andrei-elekes/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><em class="in">Linkedin</em></a><em class="in">, </em><a href="https://twitter.com/ElekesAndrei" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><em class="in">Twitter</em></a><em class="in">, or </em><a class="da by hv hw hx hy" target="_blank" rel="noopener" href="/@aele54"><em class="in">Medium</em></a><em class="in">.</em></p></div></div></section></div><div><div class="ds u dt du dv dw"></div><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><div><div id="3bfb" class="ed ee ef at eg b eh ei ej ek el em en eo ep eq er"><h1 class="eg b eh es ej et el eu en ev ep ew ef">How to Crawl the Web Politely with Scrapy</h1></div><div class="ex"><div class="n ey ez fa fb"><div class="o n"><div><a rel="noopener" href="/@ScrapingHub?source=post_page-----15fbe489573d----------------------"><img alt="ScrapingHub" class="r fc fd fe" src="https://miro.medium.com/fit/c/96/96/0*QKCSR-4TgI0iqDQR.png" width="48" height="48"></a></div><div class="ff ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r ef q"><div class="fg n o fh"><span class="as cx fi au cd fj fk fl fm fn ef"><a class="dc dd bb bc bd be bf bg bh bi fo bl bm fp fq" rel="noopener" href="/@ScrapingHub?source=post_page-----15fbe489573d----------------------">ScrapingHub</a></span><div class="fr r ap h"><button class="fs ef q bq ft fu fv fw bi fp fx fy fz ga gb gc bt as b at gd cy aw bu bv bw bx by bl">Follow</button></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cx fi au cd fj fk fl fm fn ax"><div><a class="dc dd bb bc bd be bf bg bh bi fo bl bm fp fq" rel="noopener" href="/hackernoon/how-to-crawl-the-web-politely-with-scrapy-15fbe489573d?source=post_page-----15fbe489573d----------------------">Oct 26, 2016</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div><div class="n ge gf gg gh gi gj gk gl ab"><div class="n o"><div class="gm r ap"><a href="//medium.com/p/15fbe489573d/share/twitter?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gm r ap"><a href="//medium.com/p/15fbe489573d/share/facebook?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gp r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fhackernoon%2Fhow-to-crawl-the-web-politely-with-scrapy-15fbe489573d&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="4b1f" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">The first rule of web crawling is you do not harm the website. The second rule of web crawling is you do <strong class="gs he">NOT</strong> harm the website. We’re supporters of the democratization of web data, but not at the expense of the website’s owners.</p><p id="fcc7" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">In this post we’re sharing a few tips for <a href="https://scrapy.org/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy</a> users (Scrapy is a 100% open source web crawling framework) who want polite and considerate web crawlers.</p><p id="ff18" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Whether you call them spiders, crawlers, or robots, let’s work together to create a world of Baymaxs, WALL-Es, and R2-D2s rather than an apocalyptic wasteland of HAL 9000s, T-1000s, and Megatrons.</p><figure class="hk hl hm hn ho hp do dp paragraph-image"><div class="do dp hj"><div class="hv r hw hx"><div class="hy r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*YSO3AbxfWQ6Bc8McB5-dGA.png?q=20" width="310" height="171" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="310" height="171" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/620/1*YSO3AbxfWQ6Bc8McB5-dGA.png" width="310" height="171" role="presentation"></noscript></div></div></div><figcaption class="ax fi id ie if dq do dp ig ih as cx">Embrace the lovable bots</figcaption></figure><h1 id="c209" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">What Makes a Crawler Polite?</h1><blockquote class="iu iv iw"><p id="6701" class="gq gr ef ix gs b gt gu gv gw gx gy gz ha hb hc hd dx">A polite crawler respects robots.txt<br>A polite crawler never degrades a website’s performance<br>A polite crawler identifies its creator with contact information<br>A polite crawler is not a pain in the buttocks of system administrators</p></blockquote><h1 id="7722" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">robots.txt</h1><p id="1637" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Always make sure that your crawler follows the rules defined in the website’s robots.txt file. This file is usually available at the root of a website (www.example.com/robots.txt) and it describes what a crawler should or shouldn’t crawl according to the <a href="https://support.google.com/webmasters/answer/6062608?hl=en" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Robots Exclusion Standard</a>. Some websites even use the crawlers’ user agent to specify separate rules for different web crawlers:</p><pre class="hk hl hm hn ho jd je cm"><span id="1949" class="jf ij ef at jg b fi jh ji r jj">User-agent: Some_Annoying_Bot<br>Disallow: /</span><span id="09ee" class="jf ij ef at jg b fi jk jl jm jn jo ji r jj">User-Agent: *<br>Disallow: /*.json<br>Disallow: /api<br>Disallow: /post<br>Disallow: /submit<br>Allow: /</span></pre><h1 id="6211" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Crawl-Delay</h1><p id="079d" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Mission critical to having a polite crawler is making sure your crawler doesn’t hit a website too hard. Respect the delay that crawlers should wait between requests by following the robots.txt Crawl-Delay directive.</p><p id="73d1" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">When a website gets overloaded with more requests that the web server can handle, they might become unresponsive. Don’t be that guy or girl that causes a headache for the website administrators.</p><h1 id="b89d" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">User-Agent</h1><p id="a962" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">However, if you have ignored the cardinal rules above (or your crawler has achieved aggressive sentience), there needs to be a way for the website owners to contact you. You can do this by including your company name and an email address or website in the request’s User-Agent header. For example, Google’s crawler user agent is “Googlebot”.</p><h1 id="445a" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">How to be Polite using Scrapy</h1><p id="735c" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx"><a href="https://scrapy.org/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy</a> is a bit like Optimus Prime: friendly, fast, and capable of getting the job done no matter what. However, much like Optimus Prime and his fellow Autobots, Scrapy occasionally needs to be <a href="https://youtu.be/DgQHgy7Nmkk?t=8s" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">kept in check</a>. So here’s the nitty gritty for ensuring that Scrapy is as polite as can be.</p><figure class="hk hl hm hn ho hp do dp paragraph-image"><a href="https://scrapy.org/"><div class="do dp jp"><div class="hv r hw hx"><div class="jq r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*FsyqhfN3evDrckB5IH_h8w.png?q=20" width="256" height="256" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="256" height="256" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/512/1*FsyqhfN3evDrckB5IH_h8w.png" width="256" height="256" role="presentation"></noscript></div></div></div></a></figure><h1 id="d39a" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Robots.txt</h1><p id="a0e9" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Crawlers created using Scrapy 1.1+ already respect robots.txt by default. If your crawlers have been generated using a previous version of Scrapy, you can enable this feature by adding this in the project’s settings.py:</p><pre class="hk hl hm hn ho jd je cm"><span id="4b9d" class="jf ij ef at jg b fi jh ji r jj">ROBOTSTXT_OBEY = True</span></pre><p id="e091" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Then, every time your crawler tries to download a page from a disallowed URL, you’ll see a message like this:</p><pre class="hk hl hm hn ho jd je cm"><span id="8c55" class="jf ij ef at jg b fi jh ji r jj">2016-08-19 16:12:56 [scrapy] DEBUG: Forbidden by robots.txt: &lt;GET http://website.com/login&gt;</span></pre><h1 id="eae6" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Identifying your Crawler</h1><p id="6cd1" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">It’s important to provide a way for sysadmins to easily contact you if they have any trouble with your crawler. If you don’t, they’ll have to dig into their logs and look for the offending IPs.</p><p id="eeef" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Be nice to the friendly sysadmins in your life and identify your crawler via the Scrapy USER_AGENT setting. Share your crawler name, company name and a contact email:</p><pre class="hk hl hm hn ho jd je cm"><span id="829d" class="jf ij ef at jg b fi jh ji r jj">USER_AGENT = 'MyCompany-MyCrawler (bot@mycompany.com)'</span></pre><h1 id="940d" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Introducing Delays</h1><p id="0c5b" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Scrapy spiders are blazingly fast. They can handle many concurrent requests and they make the most of your bandwidth and computing power. However, with great power comes great responsibility.</p><p id="772d" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">To avoid hitting the web servers too frequently, you need to use the <a href="http://doc.scrapy.org/en/latest/topics/settings.html?highlight=download_delay#download-delay" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">DOWNLOAD_DELAY</a> setting in your project (or in your spiders). Scrapy will then introduce a random delay ranging from 0.5 * DOWNLOAD_DELAY to 1.5 * DOWNLOAD_DELAY seconds between consecutive requests to the same domain. If you want to stick to the exact DOWNLOAD_DELAY that you defined, you have to disable <a href="http://doc.scrapy.org/en/latest/topics/settings.html?highlight=download_delay#randomize-download-delay" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">RANDOMIZE_DOWNLOAD_DELAY</a>.</p><p id="1fcb" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">By default, DOWNLOAD_DELAY is set to 0. To introduce a 5 second delay between requests from your crawler, add this to your settings.py:</p><pre class="hk hl hm hn ho jd je cm"><span id="6324" class="jf ij ef at jg b fi jh ji r jj">DOWNLOAD_DELAY = 5.0</span></pre><p id="be82" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">If you have a multi-spider project crawling multiple sites, you can define a different delay for each spider with the download_delay (yes, it’s lowercase) spider attribute:</p><pre class="hk hl hm hn ho jd je cm"><span id="676b" class="jf ij ef at jg b fi jh ji r jj">class MySpider(scrapy.Spider):<br>    name = 'myspider'<br>    download_delay = 5.0<br>    ...</span></pre><h1 id="ef64" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Concurrent Requests Per Domain</h1><p id="2c65" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Another setting you might want to tweak to make your spider more polite is the number of concurrent requests it will do for each domain. By default, Scrapy will dispatch at most 8 requests simultaneously to any given domain, but you can change this value by updating the <a href="http://doc.scrapy.org/en/latest/topics/settings.html#concurrent-requests-per-domain" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">CONCURRENT_REQUESTS_PER_DOMAIN</a> setting.</p><p id="56a8" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Heads up, the <a href="http://doc.scrapy.org/en/latest/topics/settings.html?highlight=download_delay#concurrent-requests" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">CONCURRENT_REQUESTS</a> setting defines the maximum amount of simultaneous requests that Scrapy’s downloader will do for all your spiders. Tweaking this setting is more about your own server performance / bandwidth than your target’s when you’re crawling multiple domains at the same time.</p><h1 id="2058" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">AutoThrottle to Save the Day</h1><p id="bf9b" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Websites vary drastically in the number of requests they can handle. Adjusting this manually for every website that you are crawling is about as much fun as watching paint dry. To save your sanity, Scrapy provides an extension called <a href="http://doc.scrapy.org/en/latest/topics/autothrottle.html" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">AutoThrottle</a>.</p><p id="ae78" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">AutoThrottle automatically adjusts the delays between requests according to the current web server load. It first calculates the latency from one request. Then it will adjust the delay between requests for the same domain in a way that no more than <a href="http://doc.scrapy.org/en/latest/topics/autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">AUTOTHROTTLE_TARGET_CONCURRENCY</a> requests will be simultaneously active. It also ensures that requests are evenly distributed in a given time span.</p><p id="ad9f" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">To enable AutoThrottle, just include this in your project’s settings.py:</p><pre class="hk hl hm hn ho jd je cm"><span id="f050" class="jf ij ef at jg b fi jh ji r jj">AUTOTHROTTLE_ENABLED = True</span></pre><p id="66cb" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx"><a href="https://scrapinghub.com/scrapy-cloud/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy Cloud</a> users don’t have to worry about enabling it because it’s already enabled by default.</p><p id="716f" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">There’s a <a href="http://doc.scrapy.org/en/latest/topics/autothrottle.html#settings" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">wide range of settings</a> to help you tweak the throttle mechanism, so have fun playing around!</p><h1 id="7d4e" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Use an HTTP Cache for Development</h1><p id="a014" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Developing a web crawler is an iterative process. However, running a crawler to check if it’s working means hitting the server multiple times for each test. To help you to avoid this impolite activity, Scrapy provides a built-in middleware called <a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpcache" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">HttpCacheMiddleware</a>. You can enable it by including this in your project’s settings.py:</p><pre class="hk hl hm hn ho jd je cm"><span id="c044" class="jf ij ef at jg b fi jh ji r jj">HTTPCACHE_ENABLED = True</span></pre><p id="9d39" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Once enabled, it caches every request made by your spider along with the related response. So the next time you run your spider, it will not hit the server for requests already done. It’s a win-win: your tests will run much faster and the website will save resources.</p><h1 id="94bb" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Don’t Crawl, use the API</h1><p id="2c4d" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Many websites provide HTTP APIs so that third parties can consume their data without having to crawl their web pages. Before building a web scraper, check if the target website already provides an HTTP API that you can use. If it does, go with the API. Again, it’s a win-win: you avoid digging into the page’s HTML and your crawler gets more robust because it doesn’t need to depend on the website’s layout.</p><h1 id="1ea4" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Scrapinghub Abuse Report Form</h1><p id="8d37" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Hey folks using our <a href="https://scrapinghub.com/scrapy-cloud/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy Cloud</a> platform! We trust you will crawl responsibly, but to support website administrators, we provide an <a href="https://scrapinghub.com/abuse-report/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">abuse report form</a> where they can report any misbehaviour from crawlers running on our platform. We’ll kindly pass the message along so that you can modify your crawls and avoid ruining a sysadmin’s day. If your crawler’s are turning into Skynet and <a href="https://scrapinghub.com/tos/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">running roughshod over human law</a>, we reserve the right to halt their crawling activities and thus avert the robot apocalypse.</p><h1 id="cd84" class="ii ij ef at as ik eh il ej im in io ip iq ir is it">Wrap Up</h1><p id="9729" class="gq gr ef at gs b gt iy gv iz gx ja gz jb hb jc hd dx">Let’s all do our part to keep the peace between sysadmins, website owners, and developers by making sure that our web crawling projects are as noninvasive as possible. Remember, we need to band together to delay the rise of our robot overlords, so let’s keep our crawlers, spiders, and bots polite.</p><figure class="hk hl hm hn ho hp do dp paragraph-image"><div class="do dp jr"><div class="hv r hw hx"><div class="js r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*kgoZTFROt7PpugiqcGHWLQ.jpeg?q=20" width="300" height="225" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="300" height="225" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/600/1*kgoZTFROt7PpugiqcGHWLQ.jpeg" width="300" height="225" role="presentation"></noscript></div></div></div></figure><p id="c026" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">To all website owners, help a crawler out and ensure your site has an HTTP API.</p></div></div></section><hr class="jt cx ju jv jw if jx jy jz ka kb"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><p id="1c01" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx"><a href="https://scrapinghub.com/platform/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapy Cloud is forever free</a> and is the peanut butter to Scrapy’s jelly. Hopefully you learned a few tips for how to both speed up your crawls and prevent abuse complaints.</p></div></div></section><hr class="jt cx ju jv jw if jx jy jz ka kb"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><figure class="hk hl hm kd ke hp ic kf bv kg kh ki kj kk bg kl km kn ko kp kq paragraph-image"><div class="do dp kc"><div class="hv r hw hx"><div class="jq r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*eB6mGeLhPSP3hXrcrtRuqQ.jpeg?q=20" width="217" height="217" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="217" height="217" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/434/1*eB6mGeLhPSP3hXrcrtRuqQ.jpeg" width="217" height="217" role="presentation"></noscript></div></div></div></figure><p id="7090" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">This post was written by Valdir Stumm( <a href="https://twitter.com/stummjr" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">@stummjr</a>), a developer at Scrapinghub.</p><p id="7927" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Please heart the “Recommend” so that others can learn more about how to use Scrapy politely.</p><p id="220a" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx"><a href="https://scrapinghub.com/data-services/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow"><strong class="gs he">Learn more about what web scraping and web data can do for you</strong></a><strong class="gs he">.</strong></p><p id="2a93" class="gq gr ef at gs b gt gu gv gw gx gy gz ha hb hc hd dx">Originally published on the <a href="https://blog.scrapinghub.com/2016/08/25/how-to-crawl-the-web-politely-with-scrapy/" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Scrapinghub blog</a>.</p></div></div><div class="hp"><div class="n p"><div class="kr ks kt ku kv kw ag kx ah ky aj ak"><div class="hk hl hm hn ho n kz"><figure class="la hp lb lc ld le lf paragraph-image"><a href="http://bit.ly/HackernoonFB"><div class="hv r hw hx"><div class="lg r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*0hqOaABQ7XGPT-OYNgiUBg.png?q=20" width="1136" height="572" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="1136" height="572" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/2272/1*0hqOaABQ7XGPT-OYNgiUBg.png" width="1136" height="572" role="presentation"></noscript></div></div></a></figure><figure class="la hp lb lc ld le lf paragraph-image"><a href="https://goo.gl/k7XYbx"><div class="hv r hw hx"><div class="lg r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*Vgw1jkA6hgnvwzTsfMlnpg.png?q=20" width="1136" height="572" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="1136" height="572" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/2272/1*Vgw1jkA6hgnvwzTsfMlnpg.png" width="1136" height="572" role="presentation"></noscript></div></div></a></figure><figure class="la hp lb lc ld le lf paragraph-image"><a href="https://goo.gl/4ofytp"><div class="hv r hw hx"><div class="lg r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*gKBpq1ruUi0FVK2UM_I4tQ.png?q=20" width="1136" height="572" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="1136" height="572" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/2272/1*gKBpq1ruUi0FVK2UM_I4tQ.png" width="1136" height="572" role="presentation"></noscript></div></div></a></figure></div></div></div></div><div class="n p"><div class="ac ae af ag ah ec aj ak"><blockquote class="iu iv iw"><p id="f922" class="gq gr ef ix gs b gt gu gv gw gx gy gz ha hb hc hd dx"><a href="http://bit.ly/Hackernoon" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">Hacker Noon</a> is how hackers start their afternoons. We’re a part of the <a href="http://bit.ly/atAMIatAMI" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">@AMI</a> family. We are now <a href="http://bit.ly/hackernoonsubmission" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">accepting submissions</a> and happy to <a href="mailto:partners@amipublications.com" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">discuss advertising &amp; sponsorship</a> opportunities.</p><p id="708a" class="gq gr ef ix gs b gt gu gv gw gx gy gz ha hb hc hd dx">If you enjoyed this story, we recommend reading our <a href="http://bit.ly/hackernoonlatestt" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">latest tech stories</a> and <a href="https://hackernoon.com/trending" class="dc by hf hg hh hi" target="_blank" rel="noopener nofollow">trending tech stories</a>. Until next time, don’t take the realities of the world for granted!</p></blockquote></div></div><div class="hp ak"><figure class="hk hl hm hn ho hp ak paragraph-image"><a href="https://goo.gl/Ahtev1"><div class="hv r hw hx"><div class="lh r"><div class="hq hr ds t u hs ak cd ht hu"><img class="ds t u hs ak hz ia ib" src="https://miro.medium.com/max/60/1*35tCjoPcvq6LbB3I6Wegqw.jpeg?q=20" width="15000" height="1800" role="presentation"></div><img class="hq hr ds t u hs ak ic" width="15000" height="1800" role="presentation"><noscript><img class="ds t u hs ak" src="https://miro.medium.com/max/30000/1*35tCjoPcvq6LbB3I6Wegqw.jpeg" width="15000" height="1800" role="presentation"></noscript></div></div></a></figure></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="d021" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How to do web scraping with Cheerio</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@silentworks?source=post_page-----f83c0467e202----------------------"><img alt="Andrew Smith" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/0*PF6Zr-UH-806fprw.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@silentworks?source=post_page-----f83c0467e202----------------------">Andrew Smith</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@silentworks/how-to-do-web-scraping-with-cheerio-f83c0467e202?source=post_page-----f83c0467e202----------------------">Aug 29, 2017</a> <!-- -->·<!-- --> <!-- -->9<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/f83c0467e202/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/f83c0467e202/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40silentworks%2Fhow-to-do-web-scraping-with-cheerio-f83c0467e202&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm fm"><div class="gc r fv gd"><div class="ge r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*RKryPeD5M_aKc_YATUEfyA.jpeg?q=20" width="4994" height="3329" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="4994" height="3329" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/9988/1*RKryPeD5M_aKc_YATUEfyA.jpeg" width="4994" height="3329" role="presentation"></noscript></div></div></div></div></figure><p id="7565" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">This past weekend (13 August 2017) I started on a quest to get some data from a cinema website here in Accra, Ghana. I thought this would have been easy, since the data is available publicly. I immediately opened the Chrome web inspector to see some markup like I have not seen in years.</p><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm gx"><div class="gc r fv gd"><div class="gy r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/0*32PIKXI1GS2PrJ5s.png?q=20" width="855" height="390" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="855" height="390" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/1710/0*32PIKXI1GS2PrJ5s.png" width="855" height="390" role="presentation"></noscript></div></div></div></div></figure><h1 id="28cc" class="gz ha dc bk bj hb de hc dg hd he hf hg hh hi hj hk">The Problem</h1><p id="2dbb" class="gj gk dc bk gl b gm hl go hm gq hn gs ho gu hp gw cu">There was no structure to this data, the listings were just a bunch of <code class="gd hq hr hs ht b">&lt;p&gt;</code> tags with some nested <code class="gd hq hr hs ht b">&lt;span&gt;</code> and <code class="gd hq hr hs ht b">&lt;br&gt;</code> tags inside. This to me was a sign of a no go, I even went on to state that there was no way of getting this data in the <a href="http://slack.devcongress.org/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">DevCongress</a> (you might be wondering what DevCongress is, more to come soon) slack group, along with a solution I wasn’t too sure would work.</p><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm hy"><div class="gc r fv gd"><div class="hz r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/0*LuDDRLMwRunwXnoY.png?q=20" width="884" height="103" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="884" height="103" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/1768/0*LuDDRLMwRunwXnoY.png" width="884" height="103" role="presentation"></noscript></div></div></div></div></figure><h1 id="ced0" class="gz ha dc bk bj hb de hc dg hd he hf hg hh hi hj hk">The Old Solution</h1><p id="72d0" class="gj gk dc bk gl b gm hl go hm gq hn gs ho gu hp gw cu">After a few minutes of thinking it through, I realised there was a pattern even in the <code class="gd hq hr hs ht b">&lt;p&gt;</code> tags, when I did a count I noticed that each movie has around <strong class="gl ia">12</strong> nodes of <code class="gd hq hr hs ht b">&lt;p&gt;</code> which contained the data I would need for the movie. So now I could do a loop over the <code class="gd hq hr hs ht b">&lt;p&gt;</code> tags and count down from <strong class="gl ia">12</strong>, then reset the counter once we hit <strong class="gl ia">0</strong>.</p><h1 id="5b38" class="gz ha dc bk bj hb de hc dg hd he hf hg hh hi hj hk">The Actual Solution</h1><p id="0759" class="gj gk dc bk gl b gm hl go hm gq hn gs ho gu hp gw cu">Just when I finished writing this post, the data I was scraping changed and broke my solution, so I had to go back to the drawing board and come up with a new solution, which I think in turn has worked out to be a better and more robust solution.</p><p id="2bc1" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Instead of counting the <code class="gd hq hr hs ht b">&lt;p&gt;</code> tags, I have decided to use the <code class="gd hq hr hs ht b">&lt;hr&gt;</code> tags on the page as the breaking point between each movie, I have also decided to not use the method I was before by counting down from <strong class="gl ia">12</strong> to get the movie information. I have instead opted for checking the actual string I am looping over to test if it contains a certain word where possible. In other places I am using some crazy thinking to get the information I need.</p><p id="5c91" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Its now a bit clearer to me as to how to approach the problem, I then decided its time to start writing some code, I was thinking of doing this in Python as I had used <a href="https://www.crummy.com/software/BeautifulSoup/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">Beautiful Soup</a> in the past to do this sort of thing, but lately I have been doing more work in JavaScript and Node. So I did a quick search and I found <a href="https://hackernoon.com/cheerio-node-nodejs-tutorial-web-html-scraping-note-a4ceb37d9cbb" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">this article</a> using <a href="https://cheerio.js.org/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">Cheerio</a> and the Request library, I quickly started writing some code and couldn’t believe how easy the API was to use.</p><h1 id="23f7" class="gz ha dc bk bj hb de hc dg hd he hf hg hh hi hj hk">Getting started with the necessary tooling</h1><p id="24c7" class="gj gk dc bk gl b gm hl go hm gq hn gs ho gu hp gw cu">Lets start by installing the libraries we will need, also note I am using Node 8, so will be using new features of JavaScript where I see fit.</p><h1 id="0164" class="gz ha dc bk bj hb de hc dg hd he hf hg hh hi hj hk">Requirements</h1><p id="dfc4" class="gj gk dc bk gl b gm hl go hm gq hn gs ho gu hp gw cu">For this tutorial you will need the following libraries. At the time of writing these are the versions I used.</p><ul class=""><li id="2ea7" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw ib ic id">Cheerio (1.0.0-rc.2)</li><li id="7070" class="gj gk dc bk gl b gm ie go if gq ig gs ih gu ii gw ib ic id">Request (2.81.0)</li></ul><pre class="fn fo fp fq fr ij ik il"><span id="879e" class="im ha dc bk ht b eg in io r ip">npm install cheerio@^1.0.0-rc.2 request@^2.81.0</span></pre><p id="f913" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now lets start requiring the libraries we need in order to get some data from the webpage.</p><pre class="fn fo fp fq fr ij ik il"><span id="189d" class="im ha dc bk ht b eg in io r ip">let fs = require('fs'); <br>let request = require('request'); <br>let cheerio = require('cheerio');</span></pre><p id="d2ed" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You will notice I am also requiring the <code class="gd hq hr hs ht b">fs</code> library, we are doing this so that later on we don’t hit the API more times than necessary, we can cache the data and easily read from cache and do our scraping from that data.</p><p id="f697" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now lets define a few variables to store the URL of the website we want to scrape and the name of the cache files.</p><pre class="fn fo fp fq fr ij ik il"><span id="ddea" class="im ha dc bk ht b eg in io r ip">const cinema = 'accra'; <br>const apiUrl = `http://silverbirdcinemas.com/${cinema}/`; <br>const cacheFile = `cache/${cinema}-silverbird.html`; <br>const outputCacheFile = `cache/${cinema}-silverbird.json`;</span></pre><h1 id="5b0b" class="gz ha dc bk bj hb de hc dg hd he hf hg hh hi hj hk">Lets Go!</h1><p id="b1c0" class="gj gk dc bk gl b gm hl go hm gq hn gs ho gu hp gw cu">We can now start defining our data structure that we want to deliver to our end user.</p><pre class="fn fo fp fq fr ij ik il"><span id="8a64" class="im ha dc bk ht b eg in io r ip">// main movie listing <br>let movieListings = { <br>    address: '',<br>    movies: []<br>};<br> <br>// each individual movie <br>let newMovieObj = {<br>    title: '',<br>    showtimes: [],<br>    synopsis: '',<br>    cast: [],<br>    runningTime: '',<br>    genre: [], <br>    rating: 'Unknown' <br>};</span></pre><p id="1f0c" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Here we have defined the properties our output data will conform to, so in the <code class="gd hq hr hs ht b">movieListings</code> structure, we are currently only storing the <em class="iq">address</em> of the cinema and a list of <em class="iq">movies</em>. While in the <code class="gd hq hr hs ht b">newMovieObj</code> we are storing all the attributes of the movie that we need.</p><p id="f591" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Lets start writing our code to make a request to get the <code class="gd hq hr hs ht b">apiUrl</code> and then cache it to the file system using the <code class="gd hq hr hs ht b">fs</code> library. We will start off by wrapping the function so we can reuse it later on.</p><pre class="fn fo fp fq fr ij ik il"><span id="d2b4" class="im ha dc bk ht b eg in io r ip">let requestPage = (url, cachePath) =&gt; { <br>    request(url, (err, response, html) =&gt; { <br>        if (err) { <br>            return console.log(err); <br>        } <br>        fs.writeFile(cachePath, html, err =&gt; { <br>            if (err) { <br>                return console.log(err); <br>            } <br>            console.log('The file was saved!'); <br>        });<br>    });<br>};</span></pre><p id="a6be" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Lets look at some of this code, we start off by defining our function called <code class="gd hq hr hs ht b">requestPage</code>, which requires two parameters, one for the <code class="gd hq hr hs ht b">url</code> we are making the request to and another for the <code class="gd hq hr hs ht b">cachePath</code> we wish to save the response data to. We know what we are requesting is html so we will save it as html as defined in the <code class="gd hq hr hs ht b">cacheFile</code> variable we set earlier. We call the <code class="gd hq hr hs ht b">request</code> library with the <code class="gd hq hr hs ht b">url</code> and a callback function with the parameters of <code class="gd hq hr hs ht b">err, response, html</code>, with these we can determine the state of the request we’ve made. If there is an error, we just log it to the console for now, otherwise we can move on to starting to write to the filesystem. We now have some data, so lets move on to writing it to the filesystem for now with <code class="gd hq hr hs ht b">fs.writeFile</code>, in this we will also check for error and log them to the console again.</p><p id="793e" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now that we have our function to request and write data to the filesystem, lets move on to reading the cache file we saved.</p><pre class="fn fo fp fq fr ij ik il"><span id="876b" class="im ha dc bk ht b eg in io r ip">fs.stat(cacheFile, (err, stat) =&gt; { <br>    if (!err) { <br>        fs.readFile(cacheFile, (err, data) =&gt; {}); <br>    } else if (err.code == 'ENOENT') { <br>        requestPage(apiUrl, cacheFile); <br>    } <br>});</span></pre><p id="108f" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">We start by checking if the <code class="gd hq hr hs ht b">cacheFile</code> exists, if it doesn’t we send a request and create one, otherwise we just read it using the <code class="gd hq hr hs ht b">fs.readFile</code> function.</p><p id="0b6d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Inside our <code class="gd hq hr hs ht b">fs.readFile</code> callback, lets start loading up the data (which we know is an html page) into cheerio so we can crawl the DOM (Document Object Model) and select the data we need.</p><pre class="fn fo fp fq fr ij ik il"><span id="8d91" class="im ha dc bk ht b eg in io r ip">fs.readFile(cacheFile, (err, data) =&gt; { <br>    let $ = cheerio.load(data); <br>    let numLines = 10; <br>    let synopsisNext = false; <br>});</span></pre><p id="c801" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Lets take a look at this line by line.</p><pre class="fn fo fp fq fr ij ik il"><span id="41e7" class="im ha dc bk ht b eg in io r ip">let $ = cheerio.load(data);</span></pre><p id="f093" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You might be wondering why are we assigning a <code class="gd hq hr hs ht b">$</code> variable to the loading of the DOM data, we are using the <code class="gd hq hr hs ht b">$</code> for no specific reason, except that its what jQuery uses and it became universal amongst most developers to represent the DOM.</p><pre class="fn fo fp fq fr ij ik il"><span id="264d" class="im ha dc bk ht b eg in io r ip">let numLines = 10;</span></pre><p id="4621" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">We assign the <code class="gd hq hr hs ht b">numLines</code> variable to <strong class="gl ia">10</strong>, because this is what we will use to figure out where our movie title is. So each time the <code class="gd hq hr hs ht b">numLines</code> is reduced to <strong class="gl ia">10</strong> we know its the node of a movie title.</p><pre class="fn fo fp fq fr ij ik il"><span id="b2bd" class="im ha dc bk ht b eg in io r ip">let movie = Object.assign({}, newMovieObj);</span></pre><p id="20ce" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">The <code class="gd hq hr hs ht b">movie</code> variable is assigned to a new copy of the <code class="gd hq hr hs ht b">newMovieObj</code> to get all the properties in that object.</p><pre class="fn fo fp fq fr ij ik il"><span id="02c6" class="im ha dc bk ht b eg in io r ip">let synopsisNext = false;</span></pre><p id="76b1" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">This <code class="gd hq hr hs ht b">synopsisNext</code> variable is to make sure that we know when the synopsis information is coming up, since the actual information and the title word <strong class="gl ia">SYNOPSIS</strong> are stored in different <code class="gd hq hr hs ht b">&lt;p&gt;</code> tags.</p><pre class="fn fo fp fq fr ij ik il"><span id="c7c9" class="im ha dc bk ht b eg in io r ip">$('#content .page').children().each((i, elem) =&gt; {<br>    let text = $(elem).text();<br>    let html = $(elem).html();<br>    if (i &gt;= 1 &amp;&amp; i &lt; 3) {<br>        movieListings.address += text.replace("\'", '');<br>    }<br>    <br>    // Movies start<br>    if (i &gt; 12) {<br>        if (html == '') {<br>            if (movie.title !== '') {<br>                movieListings.movies.push(movie);<br>            }<br>            numLines = 11; // offset by 1 in order to get movie title<br>            movie = Object.assign({}, newMovieObj);<br>        }<br>        <br>        // The movie title should be the first item in the loop<br>        if (numLines == 10) {<br>            movie.title = text.replace('\n ', '').trim();<br>        }</span><span id="4522" class="im ha dc bk ht b eg ir is it iu iv io r ip">if (checkDaysOfWeek(text, ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])) {<br>            movie.showtimes = text.split('\n').map(item =&gt; item.trim());<br>        }</span><span id="5811" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for SYNOPSIS keyword and know that the next loop<br>        // will be the actual synopsis<br>        if (synopsisNext) {<br>            movie.synopsis = text;<br>            synopsisNext = false;<br>        }</span><span id="5d22" class="im ha dc bk ht b eg ir is it iu iv io r ip">if (text.indexOf('SYNOPSIS') === 0) {<br>            synopsisNext = true;<br>        }</span><span id="1017" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for the CASTS keyword<br>        if (text.indexOf('CASTS:') === 0) {<br>            movie.cast = (text.replace('CASTS:', '').trim()).split(',').map(item =&gt; item.trim());<br>        }</span><span id="1dea" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for RUNNING TIME keyword<br>        if (text.indexOf('RUNNING TIME:') === 0) {<br>            movie.runningTime = text.replace('RUNNING TIME:', '').trim();<br>        }</span><span id="c5fb" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for GERNE keyword<br>        if (text.indexOf('GENRE:') === 0) {<br>            movie.genre = (text.replace('GENRE:', '').trim()).split(',').map(item =&gt; item.trim());<br>        }</span><span id="504a" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for RATING keyword<br>        if (text.indexOf('RATING:') === 0) {<br>            movie.rating = text.replace('RATING:', '').trim();<br>        }<br>        numLines--;<br>    }<br>});</span></pre><p id="df7c" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">The code above is plenty, but lets break it down as to what each part is doing.</p><p id="ad5d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">We will start off on line 1 which loops through all the <code class="gd hq hr hs ht b">p</code> tags inside of a <code class="gd hq hr hs ht b">div</code> with the <strong class="gl ia">id</strong> of <code class="gd hq hr hs ht b">content</code>.</p><pre class="fn fo fp fq fr ij ik il"><span id="fd48" class="im ha dc bk ht b eg in io r ip">$('#content .page').children().each((i, elem) =&gt; {</span></pre><p id="0f70" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">On line 2 and 3 we are assigning the text of each <code class="gd hq hr hs ht b">p</code> tag into a variable called <code class="gd hq hr hs ht b">text</code> and a variable called <code class="gd hq hr hs ht b">html</code>.</p><pre class="fn fo fp fq fr ij ik il"><span id="48e4" class="im ha dc bk ht b eg in io r ip">let text = $(elem).text(); <br>let html = $(elem).html();</span></pre><p id="d04d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">From line 4 we are then checking if the current <code class="gd hq hr hs ht b">p</code> tag is situated in the first 3, as we have figured this is where the address for the cinema is located. We then append that text to the <code class="gd hq hr hs ht b">address</code> property of the <code class="gd hq hr hs ht b">movieListings</code> object. At this point we do some cleanup on the text with the <code class="gd hq hr hs ht b">replace</code> string method.</p><pre class="fn fo fp fq fr ij ik il"><span id="3e94" class="im ha dc bk ht b eg in io r ip">if (i &gt;= 1 &amp;&amp; i &lt; 3) { <br>    movieListings.address += text.replace("\'", '');<br>}</span></pre><p id="bc9a" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Next we can see that the actual movie listings start from line 9 onwards, this is because we know that after 12 <code class="gd hq hr hs ht b">p</code> tags we have the movie listings starting.</p><pre class="fn fo fp fq fr ij ik il"><span id="217e" class="im ha dc bk ht b eg in io r ip">if (i &gt; 12) {</span></pre><p id="682b" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">On line 10 to 16, we check if <code class="gd hq hr hs ht b">html</code> is empty and reset <code class="gd hq hr hs ht b">numLines</code> to 11, you might be wondering why 11 instead of 10, this is because we have to offset by 1 in order to get any subsequent title after the first time, now we add the current <code class="gd hq hr hs ht b">movie</code> to the <code class="gd hq hr hs ht b">movieListings.movies</code>. We then move on to creating a new <code class="gd hq hr hs ht b">movie</code> object to make sure that our next loop is not updating an existing movie reference.</p><pre class="fn fo fp fq fr ij ik il"><span id="e550" class="im ha dc bk ht b eg in io r ip">if (html == '') { <br>    if (movie.title !== '') { <br>        movieListings.movies.push(movie);<br>    } <br>    numLines = 11; <!-- -->// offset by 1 in order to get movie title</span><span id="5dfb" class="im ha dc bk ht b eg ir is it iu iv io r ip">movie = Object.assign({}, newMovieObj);<br>}</span></pre><p id="43af" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">On line 19 to 56, we use multiple <code class="gd hq hr hs ht b">if</code> statements to decide which piece of movie information we are currently accessing. Here you will notice we are using different methods to check the data against. When we find the information we need, we are doing some manipulation and cleanup in order to create a format we are happy with. In this particular area we created a helper function to get the showtimes, by check a string to see if it contains any of the days in the week. That helper function is the <code class="gd hq hr hs ht b">checkDaysOfWeek</code> function, which looks like the below.</p><pre class="fn fo fp fq fr ij ik il"><span id="d072" class="im ha dc bk ht b eg in io r ip">let checkDaysOfWeek = (text, days) =&gt; { <br>    for (var i = 0; i &lt; days.length; i++) { <br>        if (text.toLowerCase().indexOf(days[i]) !== -1) { <br>            return true; <br>        } <br>    } <br>    return false; <br>};</span></pre><p id="45c0" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">The rest of the code below is just working out how best to find a particular piece of movie information.</p><pre class="fn fo fp fq fr ij ik il"><span id="61ef" class="im ha dc bk ht b eg in io r ip">// The movie title should be the first item in the loop<br>if (numLines == 10) {<br>    movie.title = text.replace('\n ', '').trim();<br>}</span><span id="12ab" class="im ha dc bk ht b eg ir is it iu iv io r ip">if (checkDaysOfWeek(text, ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])) {<br>    movie.showtimes = text.split('\n').map(item =&gt; item.trim());<br>}</span><span id="9d3d" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for SYNOPSIS keyword and know that the next loop<br>// will be the actual synopsis<br>if (synopsisNext) {<br>    movie.synopsis = text;<br>    synopsisNext = false;<br>}</span><span id="865c" class="im ha dc bk ht b eg ir is it iu iv io r ip">if (text.indexOf('SYNOPSIS') === 0) {<br>    synopsisNext = true;<br>}</span><span id="55ea" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for the CASTS keyword<br>if (text.indexOf('CASTS:') === 0) {<br>    movie.cast = (text.replace('CASTS:', '').trim()).split(',').map(item =&gt; item.trim());<br>}</span><span id="e3c6" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for RUNNING TIME keyword<br>if (text.indexOf('RUNNING TIME:') === 0) {<br>    movie.runningTime = text.replace('RUNNING TIME:', '').trim();<br>}</span><span id="8747" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for GERNE keyword<br>if (text.indexOf('GENRE:') === 0) {<br>    movie.genre = (text.replace('GENRE:', '').trim()).split(',').map(item =&gt; item.trim());<br>}</span><span id="e15b" class="im ha dc bk ht b eg ir is it iu iv io r ip">// Search for RATING keyword<br>if (text.indexOf('RATING:') === 0) {<br>    movie.rating = text.replace('RATING:', '').trim();<br>}</span></pre><p id="8195" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Once we hit line 57, we reduce by 1 the <code class="gd hq hr hs ht b">numLines</code> left.</p><pre class="fn fo fp fq fr ij ik il"><span id="7fe9" class="im ha dc bk ht b eg in io r ip">numLines--;</span></pre><p id="d976" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You can view the full source code and working copy on <a href="https://glitch.com/edit/#!/sunrise-alloy" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">Glitch</a>.</p><p id="1d30" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">And this is how I went about scraping the movie data I needed from the cinema website. In the code there are a lot of places that can be refactored and simplified. I might write another post on refactoring the current codebase.</p><p id="ebee" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Thanks to <a href="https://twitter.com/micheallshair" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">Wendy Smith</a>, <a href="https://twitter.com/Eddy_mens" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">Edmond Mensah</a>, <a href="https://twitter.com/elartey" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">Emmanuel Lartey</a> and <a href="https://twitter.com/theRealBraZee" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">David Oddoye</a> for reviewing this post and giving feedback to improve it. If you need Front-end/NodeJS/PHP development done, please visit <a href="https://www.donielsmith.com/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">https://www.donielsmith.com</a> and check out some of my work. Feel free to get in-touch with me on Twitter <a href="https://twitter.com/silentworks" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow">@silentworks</a> with questions.</p></div></div></section><hr class="iw ef ix iy iz ja jb jc jd je jf"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="56b5" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><em class="iq">Originally published at </em><a href="https://www.donielsmith.com/blog/2017/08/29/how-to-do-web-scraping-with-cheerio/" class="at cg hu hv hw hx" target="_blank" rel="noopener nofollow"><em class="iq">www.donielsmith.com</em></a><em class="iq"> on August 29, 2017.</em></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="57e5" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Visual Web Scraping Tools: What to Do When They Are No Longer Fit For Purpose?</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@ScrapingHub?source=post_page-----a366996a25d1----------------------"><img alt="ScrapingHub" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/0*QKCSR-4TgI0iqDQR.png" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@ScrapingHub?source=post_page-----a366996a25d1----------------------">ScrapingHub</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@ScrapingHub/visual-web-scraping-tools-what-to-do-when-they-are-no-longer-fit-for-purpose-a366996a25d1?source=post_page-----a366996a25d1----------------------">May 30, 2019</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/a366996a25d1/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/a366996a25d1/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40ScrapingHub%2Fvisual-web-scraping-tools-what-to-do-when-they-are-no-longer-fit-for-purpose-a366996a25d1&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm fm"><div class="gc r fv gd"><div class="ge r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*RfiGIGBvUSwUkkA3cNn1vg.png?q=20" width="1200" height="628" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="1200" height="628" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/2400/1*RfiGIGBvUSwUkkA3cNn1vg.png" width="1200" height="628" role="presentation"></noscript></div></div></div></div></figure><p id="8d19" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Visual web scraping tools are great. They allow people with little to no technical know-how to extract data from websites with only a couple hours of upskilling, making them great for simple lead generation, market intelligence and competitor monitoring projects. Removing countless hours of manual entry work for sales and marketing teams, researchers, and business intelligence team in the process.</p><p id="61d3" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">However, no matter how sophisticated the creators of these tools say their visual web scraping tools are, users often run into issues when trying to scrape mission-critical data from complex websites or when scraping the web at scale.</p><p id="8800" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">In this article, we’re going to talk about the biggest issues companies face when using visual web scraping tools like Mozenda, Import.io and Dexi.io, and what they should do when they are no longer fit for purpose.</p><p id="eb15" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">First, let’s use a commonly known comparison to help explain the pros and cons of visual web scraping tools versus manually coding your own web crawlers.</p><h1 id="1302" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi">The Visual Website Builders of Web Scraping</h1><p id="1190" class="gj gk dc bk gl b gm hj go hk gq hl gs hm gu hn gw cu">If you have any experience of developing a website for your own business, hobby or client projects, odds are you have come across one of the many online tools that say you can create visually stunning and fully featured websites using a simple-to-use visual interface.</p><p id="99b9" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">When we see their promotional videos and the example websites their users have “created” on their platforms we believe we have hit the jackpot. With a few clicks of a button, we can design a beautiful website ourselves at a fraction of the cost of hiring a web developer to do it for us. Unfortunately, in most cases these tools never meet our expectations.</p><p id="31a0" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">No matter how much they try, visual point and click website builders can never replicate the functionality, design and performance of a custom website created by a web developer. Websites created by visual website builder tools are often slow, inefficient, have poor SEO and severely limit the translation of design requirements into the desired website. As a result, outside of very small business websites and rapid prototyping of marketing landing pages, companies overwhelming have professional web developers design and develop custom websites for their businesses.</p><p id="36c7" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">The same is true of visual point and click web scraping tools. Although the promotional material of many of these tools make it look like you can extract any data from any website at any scale, in reality this is often never true.</p><p id="99b3" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Like visual website builder tools, visual web scraping tools are great for small and simple data extraction projects where lapses in data quality or delivery aren’t critical, however, when scraping mission critical data from complex websites at scale then they quickly suffer some serious issues often making them a bottleneck in companies data extraction pipelines and a burden on their teams.</p><p id="aeb7" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">With that in mind we will look at some of these performance issues in a bit more detail…</p><h1 id="ee32" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi">Efficiency When Scraping At Scale</h1><p id="2126" class="gj gk dc bk gl b gm hj go hk gq hl gs hm gu hn gw cu">Visual point and click web scraping tools suffer from similar issues that visual website builders encounter. Because the crawler design needs to be able to handle a huge variety of website types/formats and isn’t being custom developed by an experienced developer, the underlying code can sometimes be clunky and inefficient. Impacting the speed at which visual crawlers can extract the target data and make them more prone to breaking.</p><p id="6a2d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Oftentimes, these crawlers make additional requests that aren’t required, render JavaScript when there is no need, and increase the footprint of the crawler increasing the likelihood of your crawlers being detected by anti-bot countermeasures.</p><p id="90da" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">These issues often have little noticeable impact on small scale and infrequent web scraping projects, however, as the volume of data being extracted increases, users of visual web scrapers often notice significant performance issues in comparison to custom developed crawlers.</p><p id="e100" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Unnecessarily, putting more strain on the target websites servers, increasing the load on your web scraping infrastructure and make extracting data within tight time windows unviable.</p><h1 id="b639" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi">Increased Data Quality &amp; Reliability Issues</h1><p id="7120" class="gj gk dc bk gl b gm hj go hk gq hl gs hm gu hn gw cu">Visual web scraping tools also suffer from increased data quality and reliability issues due to the technical limitations described above along with their inherent rigidity, lack of quality assurance layers and the fact their opaque nature makes it harder to identify and fix the root causes of data quality issues.</p><ul class=""><li id="4dbd" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw ho hp hq"><strong class="gl hr">Flexibility </strong>— Due to the automated and rigid nature of visual web scraping tools, the crawlers they develop may be overly specific in extracting data from a website. This means that if there is even a small change in the website’s structure, the crawler could break. In comparison, experienced crawl engineers can design their crawlers from the outset to be much more flexible to website changes etc. making them much more reliable.</li><li id="78af" class="gj gk dc bk gl b gm hs go ht gq hu gs hv gu hw gw ho hp hq"><strong class="gl hr">Limited Visibility of Crawlers Inner Workings </strong>— With visual web scraping tools you have limited visibility of precisely how it is crawling the target website making it harder to identify and fix the root causes of data quality issues.</li><li id="5503" class="gj gk dc bk gl b gm hs go ht gq hu gs hv gu hw gw ho hp hq"><strong class="gl hr">Quality Assurance Layers </strong>— With visual web scraping tools you have less control over how your crawlers and data feeds are being monitored and checked for data quality issues. Making it harder to maintain data quality and troubleshoot any issues that inevitably will arise.</li></ul><p id="9c0d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">These issues combine to reduce the overall data quality and reliability of data extracted with visual web scraping tools and increase the maintenance burden.</p><h1 id="e8b9" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi">Complex Websites</h1><p id="c725" class="gj gk dc bk gl b gm hj go hk gq hl gs hm gu hn gw cu">Another drawback of visual web scraping tools is the fact that they often struggle to handle modern websites that make extensive use of JavaScript and AJAX. These limitations can make it difficult to extract all the data you need and simulate user behaviour adequately.</p><p id="36fb" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">It can often also be complex to next to impossible to extract data from certain types of fields on websites, for example: hidden elements, XHR requests and other non-HTML elements (for example PDF or XLS files embedded on the page).</p><p id="389b" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">For simple web scraping projects these drawbacks might not be an issue, but for certain use cases and sites they can make extracting the data you need virtually impossible.</p><h1 id="efe4" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi">Anti-Bot Countermeasures</h1><p id="91d2" class="gj gk dc bk gl b gm hj go hk gq hl gs hm gu hn gw cu">Oftentimes, the technical issues described above aren’t that evident for smaller scale web scraping projects, however, they can quickly become debilitating as you scale up your crawls. Not only do they make your web scraping processes more inefficient and buggy, they can stop you from extracting your target data entirely.</p><p id="bf97" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Increasingly, large websites are using anti-bot countermeasures to control the way automated bots access their websites. However, due to the inefficiency of their code, web crawlers designed by visual web scraping tools are often easier to detect than properly optimised custom spiders.</p><p id="791d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Custom spiders can be designed to better simulate user behaviour, minimise their digital footprint and counteract the detection methods of anti-bot countermeasures to avoid any disruption to their data feeds.</p><p id="856e" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">In contrast, the same degree of customisation is often impossible to replicate with crawlers built using visual web scraping tools without getting access to and modifying the underlying source code of the crawlers. Which can be difficult to do as it is often proprietary to the visual website builder.</p><p id="76e5" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">As a result, often the only step you can take is to increase the size of your proxy pool to cope with the increasing frequency of bans, etc. as you scale.</p><h1 id="24b2" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi">Experiencing Issues Your Visual Web Scraping Tool?</h1><p id="e32f" class="gj gk dc bk gl b gm hj go hk gq hl gs hm gu hn gw cu">If you are using a visual web scraping tool with zero issues and have no plans to scale your web scraping projects then you might as well just keep using your current web scraping tool. You likely won’t get any performance boost from switching to custom designed tools.</p><p id="32be" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Although current visual web scraping tools have come along way, currently they often can’t replicate the accuracy and performance of custom designed crawlers, especially when scraping at scale.</p><p id="fe90" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">In the coming years, with the continued advancements in artificial intelligence these crawlers may be able to match their performance. However for the time being, if your web scraping projects are suffering from poor data quality, crawlers breaking, difficulties scaling, or want to cut your reliance on your current providers support team then you should seriously consider building a custom web scraping infrastructure for your data extraction requirements.</p><p id="14dc" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">In cases like these, it is very common for companies to contact Scrapinghub to migrate their web scraping projects from a visual web scraping tool to a custom web scraping infrastructure.</p><p id="c4d7" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Not only are they able to significantly increase the scale and performance of your web scraping projects, they no longer have to rely on proprietary technologies, have no vendor lock-in, and have more flexibility to get the exact data they need with no data quality or reliability issues.</p><p id="4c01" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Removing all of the bottlenecks and headaches companies normally face when using visual web scraping tools.</p><p id="0eba" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">If you think it is time for you to take this approach with your web scraping, then you have two options:</p><ol class=""><li id="b650" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw hx hp hq">Build it yourself; or,</li><li id="d3e2" class="gj gk dc bk gl b gm hs go ht gq hu gs hv gu hw gw hx hp hq">Partner with an experienced web scraping provider</li></ol><p id="b590" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">At Scrapinghub, we can help you with both options. We have a <a href="https://scrapinghub.com/compare-products" class="at cg hy hz ia ib" target="_blank" rel="noopener nofollow">comprehensive suite of web scraping tools</a> to help development teams build, scale and manage their spiders without all the headaches of managing the underlying infrastructure. Along with a range of <a href="https://scrapinghub.com/data-services" class="at cg hy hz ia ib" target="_blank" rel="noopener nofollow">data extraction services</a> where we develop and manage your custom high performance web scraping infrastructure for you.</p><p id="45fa" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">If you have a need to start or scale your web scraping projects then our <a href="http://bit.ly/2Vm96rO" class="at cg hy hz ia ib" target="_blank" rel="noopener nofollow">Solution Architecture team</a> is available for a free consultation, where we will evaluate and develop the architecture for a data extraction solution to meet your data and compliance requirements.</p><p id="f011" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">At Scrapinghub we always love to hear what our readers think of our content and would be more than interested in any questions you may have. So please, leave a comment below with your thoughts and perhaps consider sharing what you are working on right now!</p></div></div></section><hr class="ic ef id ie if ig ih ii ij ik il"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="65ed" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><em class="im">Originally published at </em><a href="https://blog.scrapinghub.com/visual-web-scraping-tools-what-to-do-when-they-are-no-longer-fit-for-purpose" class="at cg hy hz ia ib" target="_blank" rel="noopener nofollow"><em class="im">https://blog.scrapinghub.com</em></a><em class="im"> on May 30, 2019.</em></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="cabe" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Natural Language Processing</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@sslundberg?source=post_page-----6e6e92edbd11----------------------"><img alt="Sam Lundberg" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/0*eWraRIABOV2q1Drq" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@sslundberg?source=post_page-----6e6e92edbd11----------------------">Sam Lundberg</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@sslundberg/natural-language-processing-6e6e92edbd11?source=post_page-----6e6e92edbd11----------------------">Sep 22, 2018</a> <!-- -->·<!-- --> <!-- -->2<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/6e6e92edbd11/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/6e6e92edbd11/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40sslundberg%2Fnatural-language-processing-6e6e92edbd11&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="066d" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">I recently became familiar with the process of using website API’s and/or how to do web scraping, to extract words or tables from websites, to become a source of data for machine learning purposes. That in itself was pretty interesting, but what you can do with all that information, particularly with words, is fascinating. Welcome to the world of Natural Language Processing (NLP).</p><p id="eef1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">NLP has become on of my favorite subjects I have learned in my Data Science learning. Being able to take a post from Reddit, or comments from Amazon, or an article from a webpage, and to create a predictive model from that blows my mind. I mean, how can words be treated like numerical values?! But if start thinking about it, certain words can define who wrote/said in a statement, and if you can identify that, you now have a feature you can help predict on. Finding the occurence of the number of times a word shows up can hold a lot of power.</p><p id="8f7e" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">For instance, the phrase “Make America Great Again” is President Trump’s slogan. If I am trying to predict if an article or post is written by a democrat or republican, and those words show up in that record, with some tuning, the model would probably predict if a republican wrote that post, or if there are ties to the Republican party, or even Trump himself.</p><p id="6eec" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">At the time I was learning this, there was the big news of someone close in Trump’s cabinet that wrote a very incriminating letter of Trump’s alleged missteps as President. Nobody knew who wrote it, but I came across several news reports and articles, where Data Scientists were using NLP to try and find who wrote the article, comparing how certain words were used, compared to a number of other published articles from Trump’s cabinet over the years. Talk about relevant and an exciting use of techniques! It is like being a data detective! That just increased my excitement more and more to dive into the NLP process further.</p><p id="3b2b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">I also had an opportunity to talk with a Data Science company, and they had just finished a project using NLP to look for gender discrimination in employee reviews, and they were successful at building a model that helped find these type of discriminatory reviews. Words hold power and can have equal weight, if applied right, to predicting outcomes. The old saying “Sticks and stones can break my bones but words will never hurt me” is something we all know is not true, but apply the concepts of NLP to words, and words might actually be more of a threat than a feature of just rocks and sticks!</p></div></div></section></div><div><div class="ds u dt du dv dw"></div><section class="dx dy dz ea eb"><div class="ec"><div class="n p"><div class="ed ee ef eg eh ei ag ej ah ek aj ak"><figure class="em en eo ep eq ec er es paragraph-image"><div class="et eu ev ew ak"><div class="do dp el"><div class="fc r ev fd"><div class="fe r"><div class="ex ey ds t u ez ak cd fa fb"><img class="ds t u ez ak ff fg fh" src="https://miro.medium.com/max/60/1*OVSQPzUI8Kcj35d_Fk1lZg.jpeg?q=20" width="3000" height="2000" role="presentation"></div><img class="ex ey ds t u ez ak fi" width="3000" height="2000" role="presentation"><noscript><img class="ds t u ez ak" src="https://miro.medium.com/max/6000/1*OVSQPzUI8Kcj35d_Fk1lZg.jpeg" width="3000" height="2000" role="presentation"></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="ac ae af ag ah fj aj ak"><div><div id="f8e0" class="fk fl fm at fn b fo fp fq fr fs ft fu fv fw fx fy"><h1 class="fn b fo fz fq ga fs gb fu gc fw gd fm">How do we find daily good deals online, automatically?</h1></div></div><div id="294d" class="ge fl ax at as cx gf gg gh gi gj gk gl gm gn go gp"><h2 class="as cx gf gq gh gr gj gs gl gt gn gu ax">Basic web content scraping with R to automate boring tasks</h2></div><div class="gv"><div class="n gw gx gy gz"><div class="o n"><div><a href="/@jameschen_78678?source=post_page-----fe8cfc8f783a----------------------" rel="noopener"><img alt="James Chen" class="r ha hb hc" src="https://miro.medium.com/fit/c/96/96/1*SzPGtdaPS0Zh41UrcrDiyA.jpeg" width="48" height="48"></a></div><div class="hd ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r fm q"><div class="he n o hf"><span class="as cx hg au cd hh hi hj hk hl fm"><a href="/@jameschen_78678?source=post_page-----fe8cfc8f783a----------------------" class="dc dd bb bc bd be bf bg bh bi hm bl bm hn ho" rel="noopener">James Chen</a></span><div class="hp r ap h"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-do-we-find-daily-good-deals-online-automatically-fe8cfc8f783a&amp;source=-c16867ccea73-------------------------follow_byline-" class="hq fm q bq hr hs ht hu bi hn hv hw hx hy hz ia bt as b at ib cy aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cx hg au cd hh hi hj hk hl ax"><div><a class="dc dd bb bc bd be bf bg bh bi hm bl bm hn ho" rel="noopener" href="/how-do-we-find-daily-good-deals-online-automatically-fe8cfc8f783a?source=post_page-----fe8cfc8f783a----------------------">Jan 7, 2017</a> <!-- -->·<!-- --> <!-- -->5<!-- --> min read</div></span></span></div></div><div class="n ic id ie if ig ih ii ij ab"><div class="n o"><div class="ik r ap"><a href="//medium.com/p/fe8cfc8f783a/share/twitter?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi il im bl bm hn ho" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="ik r ap"><a href="//medium.com/p/fe8cfc8f783a/share/facebook?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi il im bl bm hn ho" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="in r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-do-we-find-daily-good-deals-online-automatically-fe8cfc8f783a&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="dc dd bb bc bd be bf bg bh bi il im bl bm hn ho" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div><blockquote class="io ip iq"><p id="2370" class="ir is fm it iu b gf iv gh iw ix iy iz ja jb jc jd dx">Background</p></blockquote><p id="1cd9" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">As defined <a href="https://discuss.analyticsvidhya.com/t/what-are-different-paths-in-data-sciences/302" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">here</a>, “a data scientist is someone who is better at statistics than any software engineer and better at software engineering than any statistician.” Therefore, this blog post focuses on the practice of web content scrapping, which is an essential skill for data scientists to acquire information outside of structured databases, and when APIs are unavailable.</p><p id="8148" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">When looking for good deals online, we often go on to a few eCommerce websites frequently to check the prices on the items we want. After a while, this becomes a tedious task. Inspired by <a href="https://hackernoon.com/the-programmers-guide-to-booking-a-plane-11e37d610045#.z50j983vh" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow"><em class="it">The Programmer’s Guide to Booking a Plane</em></a>, in which Zeke wrote a script in Node to automate the process of finding cheap plane tickets, we would like to replicate his method on good MacBook deals, using a few packages in R.</p><blockquote class="io ip iq"><p id="00d9" class="ir is fm it iu b gf iv gh iw ix iy iz ja jb jc jd dx">Objective</p></blockquote><p id="5b41" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">The objective is to receive automatic email alerts when the MacBook price drops to below a certain point.</p><blockquote class="io ip iq"><p id="e752" class="ir is fm it iu b gf iv gh iw ix iy iz ja jb jc jd dx">Approach</p></blockquote><ol class=""><li id="0514" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd ji jj jk"><strong class="iu jl">Scrap the product information from the eCommerce website</strong></li></ol><p id="72e0" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">We need to load the html structure of the website first, in order to retrieve the information we need. The R package we will be using is <em class="it">rvest</em>.</p><pre class="jm jn jo jp jq jr js cm"><span id="e7cc" class="jt ju fm at jv b hg jw jx r jy">library(rvest)<br>library(XML)</span><span id="97e2" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">#save html of URL<br>url &lt;- "<a href="http://www.rakuten.com.tw/category/4945/?p=1&amp;l-id=tw_pagen_1" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">http://www.rakuten.com.tw/category/4945/?p=1&amp;l-id=tw_pagen_1</a>"</span></pre><p id="be55" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">After saving the URL html, we need to find the section of information that we need, by inspecting the page source. We will search a price to navigate to product related information, as shown below.</p></div></div><div class="ec"><div class="n p"><div class="ed ee ef eg eh ei ag ej ah ek aj ak"><figure class="jm jn jo jp jq ec er es paragraph-image"><div class="et eu ev ew ak"><div class="do dp ke"><div class="fc r ev fd"><div class="kf r"><div class="ex ey ds t u ez ak cd fa fb"><img class="ds t u ez ak ff fg fh" src="https://miro.medium.com/max/60/1*Sff7wJgMYMqTyk0qJ_ap4g.png?q=20" width="1440" height="809" role="presentation"></div><img class="ex ey ds t u ez ak fi" width="1440" height="809" role="presentation"><noscript><img class="ds t u ez ak" src="https://miro.medium.com/max/2880/1*Sff7wJgMYMqTyk0qJ_ap4g.png" width="1440" height="809" role="presentation"></noscript></div></div></div></div><figcaption class="ax hg kg kh ki dq do dp kj kk as cx">Screenshot of the page source</figcaption></figure></div></div></div><div class="n p"><div class="ac ae af ag ah fj aj ak"><p id="af70" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">We noticed that product related information is under <br><strong class="iu jl">&lt;div class=”b-content”&gt;</strong><br>and therefore we will extract this part only.</p><pre class="jm jn jo jp jq jr js cm"><span id="82ee" class="jt ju fm at jv b hg jw jx r jy">product &lt;- url %&gt;%<br>  read_html() %&gt;%<br>  html_nodes(".b-content")</span></pre><p id="8afc" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">An excellent Chrome add on called <strong class="iu jl">SelectorGadget</strong> can be downloaded <a href="https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">here</a>. This tool allows us to intuitively select the specific content we want.</p><p id="4218" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">When we select the name of the product, the content will be highlighted in green, as shown below. The tool also guesses that we also want other product names as well, and therefore it will highlight other product names in yellow. For any content that we do not need, we can click on it and it will be removed (the color will turn red).</p></div></div><div class="ec"><div class="n p"><div class="ed ee ef eg eh ei ag ej ah ek aj ak"><figure class="jm jn jo jp jq ec er es paragraph-image"><div class="et eu ev ew ak"><div class="do dp ke"><div class="fc r ev fd"><div class="kl r"><div class="ex ey ds t u ez ak cd fa fb"><img class="ds t u ez ak ff fg fh" src="https://miro.medium.com/max/60/1*13P2YG4PHIvNPxxxNDIYww.png?q=20" width="1440" height="816" role="presentation"></div><img class="ex ey ds t u ez ak fi" width="1440" height="816" role="presentation"><noscript><img class="ds t u ez ak" src="https://miro.medium.com/max/2880/1*13P2YG4PHIvNPxxxNDIYww.png" width="1440" height="816" role="presentation"></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="ac ae af ag ah fj aj ak"><p id="3882" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">We found that product name can be extracted using <strong class="iu jl">.product-name</strong>, as shown on the bottom of the page.</p><pre class="jm jn jo jp jq jr js cm"><span id="7a6d" class="jt ju fm at jv b hg jw jx r jy">name &lt;- product %&gt;%<br>  html_nodes(".product-name") %&gt;%<br>  html_text()</span></pre><p id="8864" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">Next we will repeat the process to find price and save it in numeric format.</p><pre class="jm jn jo jp jq jr js cm"><span id="e910" class="jt ju fm at jv b hg jw jx r jy">price &lt;- product %&gt;%<br>  html_nodes(".b-underline .b-text-prime") %&gt;%<br>  html_text() %&gt;%<br>  gsub(",","",.) %&gt;%<br>  as.numeric()</span></pre><p id="8114" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">After we are done, we can save name and price in a dataframe.</p><pre class="jm jn jo jp jq jr js cm"><span id="2a2d" class="jt ju fm at jv b hg jw jx r jy">all &lt;- data.frame(name, price,stringsAsFactors = FALSE)</span></pre><p id="3eb2" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">We will also need to scrap multiple pages to extract all the information.</p><pre class="jm jn jo jp jq jr js cm"><span id="8cd1" class="jt ju fm at jv b hg jw jx r jy">for (i in 1:10){<br>starturl &lt;- "<a href="http://www.rakuten.com.tw/category/4945/?p=" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">http://www.rakuten.com.tw/category/4945/?p=</a>"<br>nexturl &lt;- "&amp;l-id=tw_pagen_"<br>url &lt;- paste(starturl,i,nexturl,i,sep="")</span><span id="3e8c" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">product &lt;- url %&gt;%<br>  read_html() %&gt;%<br>  html_nodes(".b-content")</span><span id="55f1" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">name &lt;- product %&gt;%<br>  html_nodes(".product-name") %&gt;%<br>  html_text()</span><span id="e52c" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">price &lt;- product %&gt;%<br>  html_nodes(".b-underline .b-text-prime") %&gt;%<br>  html_text() %&gt;%<br>  gsub(",","",.) %&gt;%<br>  as.numeric()</span><span id="75a2" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">mydata &lt;- data.frame(name, price,stringsAsFactors = FALSE)<br>all &lt;- rbind(all,mydata)<br>}</span><span id="b9a8" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">all&lt;-all[!duplicated(all),]</span></pre><p id="eee5" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">The final result is stored below in dataframe format.</p><figure class="jm jn jo jp jq ec do dp paragraph-image"><div class="do dp km"><div class="fc r ev fd"><div class="kn r"><div class="ex ey ds t u ez ak cd fa fb"><img class="ds t u ez ak ff fg fh" src="https://miro.medium.com/max/38/1*CSA61earMGgZjM_u8CSBZA.png?q=20" width="423" height="672" role="presentation"></div><img class="ex ey ds t u ez ak fi" width="423" height="672" role="presentation"><noscript><img class="ds t u ez ak" src="https://miro.medium.com/max/846/1*CSA61earMGgZjM_u8CSBZA.png" width="423" height="672" role="presentation"></noscript></div></div></div><figcaption class="ax hg kg kh ki dq do dp kj kk as cx">Screenshot of the scrapped prices on MacBooks</figcaption></figure><p id="e435" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx"><strong class="iu jl">2. Create rules to send out email alerts</strong></p><p id="2504" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">Next, we will set up the rules to receive email alerts. Say we only wish to receive alerts on products with price between NT$25,000 and NT$30,000.</p><pre class="jm jn jo jp jq jr js cm"><span id="6db8" class="jt ju fm at jv b hg jw jx r jy">alert &lt;- all[all$price&gt;25000&amp;all$price&lt;=30000,]</span></pre><p id="a168" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">Next we will use the<em class="it"> mailR</em> package to send out the email, if there is at least one alert, as shown below.</p><pre class="jm jn jo jp jq jr js cm"><span id="93b1" class="jt ju fm at jv b hg jw jx r jy">if (nrow(alert) &gt;=1){</span><span id="53f8" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">write.table(alert,"alert.txt",fileEncoding = "UTF-8")</span><span id="d635" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">send.mail(from = "<a href="mailto:jchen6912@gmail.com" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">jchen6912@gmail.com</a>",<br>          to = c("<a href="mailto:jchen6912@gmail.com" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">jchen6912@gmail.com</a>"),<br>          subject = "Mac Deal Alert",<br>          body &lt;- "alert.txt",<br>          smtp = list(host.name = "smtp.gmail.com", port = 465, user.name = "<a href="mailto:jchen6912@gmail.com" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">jchen6912@gmail.com</a>", passwd = "xxxxxxxx", ssl = TRUE),<br>          encoding = "utf-8",<br>          authenticate = TRUE,<br>          send = TRUE)<br>}</span></pre><figure class="jm jn jo jp jq ec do dp paragraph-image"><div class="do dp ko"><div class="fc r ev fd"><div class="kp r"><div class="ex ey ds t u ez ak cd fa fb"><img class="ds t u ez ak ff fg fh" src="https://miro.medium.com/max/34/1*If07soGQ43YmQ_fGIGdJXA.png?q=20" width="320" height="568" role="presentation"></div><img class="ex ey ds t u ez ak fi" width="320" height="568" role="presentation"><noscript><img class="ds t u ez ak" src="https://miro.medium.com/max/640/1*If07soGQ43YmQ_fGIGdJXA.png" width="320" height="568" role="presentation"></noscript></div></div></div><figcaption class="ax hg kg kh ki dq do dp kj kk as cx">Screenshot of the automatic email alert received</figcaption></figure><p id="8ff4" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx"><strong class="iu jl">3. Automate the process by scheduling the task regularly</strong></p><p id="91cf" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">This can be done with the <em class="it">taskscheduleR</em> package, but currently only available in Windows. Click <a href="https://github.com/bnosac/taskscheduleR" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">here</a> for more details. We can schedule the Rscript to run at desired frequency and receive automatic alerts accordingly.</p><figure class="jm jn jo jp jq ec do dp paragraph-image"><div class="et eu ev ew ak"><div class="do dp kq"><div class="fc r ev fd"><div class="kr r"><div class="ex ey ds t u ez ak cd fa fb"><img class="ds t u ez ak ff fg fh" src="https://miro.medium.com/max/60/1*yTD4zDA4GY1BLLXrVi1ZWg.png?q=20" width="771" height="761" role="presentation"></div><img class="ex ey ds t u ez ak fi" width="771" height="761" role="presentation"><noscript><img class="ds t u ez ak" src="https://miro.medium.com/max/1542/1*yTD4zDA4GY1BLLXrVi1ZWg.png" width="771" height="761" role="presentation"></noscript></div></div></div></div><figcaption class="ax hg kg kh ki dq do dp kj kk as cx">Screenshot of the UI in taskscheduleR</figcaption></figure><p id="7885" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd dx">This sums up the short blog on how to scrap content for websites with static content, however, dynamic websites are more complicated and may require additional code to simulate real browsing behaviors, such as member login and form submits. Alternatively, similar task can also be performed in Python with <em class="it">scrapy</em> and <em class="it">BeautifulSoup</em>.</p><blockquote class="io ip iq"><p id="dd0f" class="ir is fm it iu b gf iv gh iw ix iy iz ja jb jc jd dx">R Code</p></blockquote><pre class="jm jn jo jp jq jr js cm"><span id="1dbf" class="jt ju fm at jv b hg jw jx r jy">library(rvest)<br>library(XML)<br>library(taskscheduleR)<br>library(mailR)</span><span id="7884" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">setwd("~/Desktop/deals")</span><span id="aab7" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">url &lt;- "<a href="http://www.rakuten.com.tw/category/4945/?p=1&amp;l-id=tw_pagen_1" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">http://www.rakuten.com.tw/category/4945/?p=1&amp;l-id=tw_pagen_1</a>"<br>product &lt;- url %&gt;%<br>  read_html() %&gt;%<br>  html_nodes(".b-content")</span><span id="5107" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">name &lt;- product %&gt;%<br>  html_nodes(".product-name") %&gt;%<br>  html_text()</span><span id="c78b" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">price &lt;- product %&gt;%<br>  html_nodes(".b-underline .b-text-prime") %&gt;%<br>  html_text() %&gt;%<br>  gsub(",","",.) %&gt;%<br>  as.numeric()</span><span id="4e31" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">all &lt;- data.frame(name, price,stringsAsFactors = FALSE)</span><span id="eac7" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">for (i in 1:10){<br>starturl &lt;- "<a href="http://www.rakuten.com.tw/category/4945/?p=" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">http://www.rakuten.com.tw/category/4945/?p=</a>"<br>nexturl &lt;- "&amp;l-id=tw_pagen_"<br>url &lt;- paste(starturl,i,nexturl,i,sep="")</span><span id="827e" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">product &lt;- url %&gt;%<br>  read_html() %&gt;%<br>  html_nodes(".b-content")</span><span id="74c8" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">name &lt;- product %&gt;%<br>  html_nodes(".product-name") %&gt;%<br>  html_text()</span><span id="c448" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">price &lt;- product %&gt;%<br>  html_nodes(".b-underline .b-text-prime") %&gt;%<br>  html_text() %&gt;%<br>  gsub(",","",.) %&gt;%<br>  as.numeric()</span><span id="9897" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">mydata &lt;- data.frame(name, price,stringsAsFactors = FALSE)<br>all &lt;- rbind(all,mydata)<br>}</span><span id="ae6f" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">all&lt;-all[!duplicated(all),]</span><span id="dc2f" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">alert &lt;- all[all$price&gt;25000&amp;all$price&lt;=30000,]</span><span id="f887" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">if (nrow(alert) &gt;=1){</span><span id="8948" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">write.table(alert,"alert.txt",fileEncoding = "UTF-8")</span><span id="87c4" class="jt ju fm at jv b hg jz ka kb kc kd jx r jy">send.mail(from = "<a href="mailto:jchen6912@gmail.com" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">jchen6912@gmail.com</a>",<br>          to = c("<a href="mailto:jchen6912@gmail.com" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">jchen6912@gmail.com</a>"),<br>          subject = "Mac Deal Alert",<br>          body &lt;- "alert.txt",<br>          smtp = list(host.name = "smtp.gmail.com", port = 465, user.name = "<a href="mailto:jchen6912@gmail.com" class="dc by je jf jg jh" target="_blank" rel="noopener nofollow">jchen6912@gmail.com</a>", passwd = "xxxxxxxx", ssl = TRUE),<br>          encoding = "utf-8",<br>          authenticate = TRUE,<br>          send = TRUE)<br>}</span></pre></div></div></section><hr class="ks cx kt ku kv ki kw kx ky kz la"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah fj aj ak"><p id="2f5e" class="ir is fm at iu b gf iv gh iw ix iy iz ja jb jc jd lb dx"><span class="r lc ld le lf lg lh li lj lk ev">Q</span>uestions, comments, or concerns?<br>jchen6912@gmail.com</p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="a7d5" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How to do data scraping EFFICIENTLY?</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@ust.johnchan?source=post_page-----f2a70794d086----------------------"><img alt="JOHN CHAN" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/0*2XkuHE8Q9-m8tIJd.jpg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@ust.johnchan?source=post_page-----f2a70794d086----------------------">JOHN CHAN</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@ust.johnchan/how-to-do-data-scraping-efficiently-f2a70794d086?source=post_page-----f2a70794d086----------------------">Oct 7, 2019</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/f2a70794d086/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/f2a70794d086/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40ust.johnchan%2Fhow-to-do-data-scraping-efficiently-f2a70794d086&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="7d3e" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">For many people, data scraping or web scraping is to write some programs that click websites and copy information from them. While this is a legitimate way to do scraping, it is the least efficient method.</p><p id="2002" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">In this article, I will list a few methods I used in scraping data from less efficient ones to more efficient ones.</p><h1 id="62cd" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Understanding the Request Response Cycle</h1><p id="aa07" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">Put it in a very simple way, the web is a place where a lot of requests and responses happening. A client (user) requests information from a server (which is just another computer). And the server serves the information (response) back to the client.</p><p id="0fc5" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">What you see as a client on a browser is just a bunch of data parsed and formatted in a pretty and presentable way, thanks to HTML, CSS, and JavaScript. The key here is the line: “just a bunch of data” which means we may be able to scrape it.</p><p id="8bd1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">There is a subtle detail which may affect the technique to scrape data. Sometimes, the whole webpage is rendered in the server side which means the server sends the complete HTML, CSS and JavaScript to the client and the client’s browser displays it. The other way is that the server returns the data and the client browser is responsible for parsing it. For the websites which is rendered in server side, we can only scrape by HTML elements . For the websites which is rendered in the client side, we can scrape by a more efficient method.</p><p id="70ac" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Actually, the key to efficient scraping is to find a place where data is rendered in the client side. For example, you may find that in a website, the data is rendered in server. But you also find that the company provides an android app. It is very likely that for an android app, the data is rendered in the client side. This enable us to use a more efficient method.</p><p id="62c9" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Don’t underestimate the difference. Scraping HTML elements are usually very slow!</p><h1 id="8b45" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Server Side: Scraping HTML elements</h1><p id="44f7" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">A script will send a request to the server. Server responses with the HTML file. Then the script find the location of information and extract it.</p><h2 id="4f67" class="gr gb dc bk bj gc gs gt gu gv gw gx gy gz ha hb hc">Pros:</h2><ul class=""><li id="e0b8" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz hd he hf">Easy to implement</li><li id="8f33" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz hd he hf">Straight-forward</li></ul><h2 id="eb11" class="gr gb dc bk bj gc gs gt gu gv gw gx gy gz ha hb hc">Cons:</h2><ul class=""><li id="addf" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz hd he hf">Slow for scraping large amount of webpages</li><li id="84ef" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz hd he hf">Not robust. Webpages change</li><li id="351c" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz hd he hf">Development time is long. You need to hard code the location of information.</li></ul><h2 id="64cc" class="gr gb dc bk bj gc gs gt gu gv gw gx gy gz ha hb hc">Tools:</h2><ul class=""><li id="9735" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz hd he hf"><a href="https://scrapy.org/" class="at cg hl hm hn ho" target="_blank" rel="noopener nofollow">Scrapy</a></li><li id="6662" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz hd he hf"><a href="https://www.seleniumhq.org/" class="at cg hl hm hn ho" target="_blank" rel="noopener nofollow">Selenium</a></li></ul><h1 id="6e9e" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Client Side: Scraping the responded data</h1><p id="e379" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">A script will send a request to the server. Server responses with data, usually JSON, XML. Then the script extracts the useful information or store the responded data directly.</p><p id="3cca" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Usually people we look at the Network Tab in the developer tool (Firefox) to inspect the traffic. <a href="https://mitmproxy.org/" class="at cg hl hm hn ho" target="_blank" rel="noopener nofollow">Mitmproxy </a>may also be used for more detail analysis.</p><figure class="hq hr hs ht hu hv cl cm paragraph-image"><div class="hw hx hy hz ak"><div class="cl cm hp"><div class="if r hy ig"><div class="ih r"><div class="ia ib cp t u ic ak eh id ie"><img class="cp t u ic ak ii ij ik" src="https://miro.medium.com/max/60/1*WTy8lGHaZNQfb0zhN-zNjg.png?q=20" width="1857" height="984" role="presentation"></div><img class="ia ib cp t u ic ak il" width="1857" height="984" role="presentation"><noscript><img class="cp t u ic ak" src="https://miro.medium.com/max/3714/1*WTy8lGHaZNQfb0zhN-zNjg.png" width="1857" height="984" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg im in io cn cl cm ip iq bj ef">Network Tab (Firefox)</figcaption></figure><h2 id="a6fd" class="gr gb dc bk bj gc gs gt gu gv gw gx gy gz ha hb hc">Pros:</h2><ul class=""><li id="e2c1" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz hd he hf">Fast for scraping large amount of webpage</li><li id="6462" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz hd he hf">Robust. Usually the request endpoints will not change.</li><li id="7028" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz hd he hf">Fast development. Less code to write.</li></ul><h2 id="9650" class="gr gb dc bk bj gc gs gt gu gv gw gx gy gz ha hb hc">Cons:</h2><ul class=""><li id="c764" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz hd he hf">Take some experience to find it.</li><li id="c2d2" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz hd he hf">Authentication issue. Sometimes the webpage is protected by password. You need to understand the authentication mechanism to get in.</li></ul><h2 id="b5f2" class="gr gb dc bk bj gc gs gt gu gv gw gx gy gz ha hb hc">Tools:</h2><ul class=""><li id="6ce9" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz hd he hf"><a href="https://scrapy.org/" class="at cg hl hm hn ho" target="_blank" rel="noopener nofollow">Scrapy</a></li><li id="4c99" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz hd he hf"><a href="https://pypi.org/project/requests/" class="at cg hl hm hn ho" target="_blank" rel="noopener nofollow">Requests</a></li></ul><h1 id="412a" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Special Tricks: Scraping APPs</h1><p id="0122" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">This is a special tricks I used to scrape webpage that renders in server side. But it only works if the company provides an Android app (Haven’t tried it on IOS app).</p><p id="1b39" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">The steps go like this:</p><ol class=""><li id="df68" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz ir he hf">Download the APK file and add security exception to APK. Since Android 7.0, google introduced changed to Certificate Authorities (CA) settings which prevents third-parties listening to network requests. Luckily you can find a script to add the exception automatically <a href="https://github.com/levyitay/AddSecurityExceptionAndroid" class="at cg hl hm hn ho" target="_blank" rel="noopener nofollow">here</a>.</li><li id="b89c" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz ir he hf">Install the modified APK to your phone</li><li id="67cf" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz ir he hf">Set up the man-in-the-middle proxy (MitmProxy) and your phone. You can refer to this <a class="at cg hl hm hn ho" target="_blank" rel="noopener" href="/testvagrant/intercept-ios-android-network-calls-using-mitmproxy-4d3c94831f62">article</a>.</li><li id="14b9" class="fm fn dc bk fo b fp hg fr hh ft hi fv hj fx hk fz ir he hf">Inspect the traffic and look for the data you want.</li></ol><p id="29e1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">For example, I want to scrape Centaline Property Website. After inspecting the webpage, I found that it seems like data is rendered in the server side.</p><figure class="hq hr hs ht hu hv cl cm paragraph-image"><div class="hw hx hy hz ak"><div class="cl cm is"><div class="if r hy ig"><div class="it r"><div class="ia ib cp t u ic ak eh id ie"><img class="cp t u ic ak ii ij ik" src="https://miro.medium.com/max/60/1*jBa2It730cM5ZD-EYoo1Tw.png?q=20" width="1060" height="920" role="presentation"></div><img class="ia ib cp t u ic ak il" width="1060" height="920" role="presentation"><noscript><img class="cp t u ic ak" src="https://miro.medium.com/max/2120/1*jBa2It730cM5ZD-EYoo1Tw.png" width="1060" height="920" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg im in io cn cl cm ip iq bj ef">Centaline Web (server side render)</figcaption></figure><p id="6f91" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">But, they provided an android app. After using the above steps, I managed to find the data which is requested (POST) using</p><p id="01ce" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><a href="https://hkapi.centanet.com/api/FindProperty/MapV2.json?postType=s&amp;order=desc&amp;page=1&amp;pageSize=20&amp;pixelHeight=2220&amp;pixelWidth=1080&amp;points%5B0%5D.lat=22.705635288642362&amp;points%5B0%5D.lng=113.85844465345144&amp;points%5B1%5D.lat=22.705635288642362&amp;points%5B1%5D.lng=114.38281349837781&amp;points%5B2%5D.lat=21.993328259196705&amp;points%5B2%5D.lng=114.38281349837781&amp;points%5B3%5D.lat=21.993328259196705&amp;points%5B3%5D.lng=113.85844465345144&amp;sort=score&amp;zoom=9.745128631591797&amp;platform=android" class="at cg hl hm hn ho" target="_blank" rel="noopener nofollow">https://hkapi.centanet.com/api/FindProperty/MapV2.json?postType=s&amp;order=desc&amp;page=1&amp;pageSize=20&amp;pixelHeight=2220&amp;pixelWidth=1080&amp;points[0].lat=22.705635288642362&amp;points[0].lng=113.85844465345144&amp;points[1].lat=22.705635288642362&amp;points[1].lng=114.38281349837781&amp;points[2].lat=21.993328259196705&amp;points[2].lng=114.38281349837781&amp;points[3].lat=21.993328259196705&amp;points[3].lng=113.85844465345144&amp;sort=score&amp;zoom=9.745128631591797&amp;platform=android</a></p><h1 id="7c95" class="ga gb dc bk bj gc de gd dg ge gf gg gh gi gj gk gl">Conclusion</h1><p id="a170" class="fm fn dc bk fo b fp gm fr gn ft go fv gp fx gq fz cu">I cannot go into very detail in the steps. The article is quite long already. The message here is that try not to use HTML element in the first place. Always look for client side render service. This will improve your scraping efficiency greatly.</p><p id="1e01" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">However, something that is working is always better than nothing. If you really can’t find a better way at this moment, just use HTML element and keep looking for better solution!</p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="065d" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How to Do Price Monitoring from Car Dealers Sites?</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@xbytecrawling?source=post_page-----caffc6a521af----------------------"><img alt="X-Byte Enterprise Crawling" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/0*58raBoA0H70LX-hG.jpg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@xbytecrawling?source=post_page-----caffc6a521af----------------------">X-Byte Enterprise Crawling</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@xbytecrawling/how-to-do-price-monitoring-from-car-dealers-sites-caffc6a521af?source=post_page-----caffc6a521af----------------------">Nov 7, 2019</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/caffc6a521af/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/caffc6a521af/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40xbytecrawling%2Fhow-to-do-price-monitoring-from-car-dealers-sites-caffc6a521af&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div><div class="fm"><div class="n p"><div class="fn fo fp fq fr fs ag ft ah fu aj ak"><figure class="fv fw fx fy fz fm ga gb paragraph-image"><div class="gc gd ge gf ak"><div class="cl cm ai"><div class="gl r ge gm"><div class="gn r"><div class="gg gh cp t u gi ak eh gj gk"><img alt="Best Car Dealer’s Price Monitoring Services : X-Byte Enterprise Crawling" class="cp t u gi ak go gp gq" src="https://miro.medium.com/max/60/1*m6lRSB8tt6t_Cc_rtujpxg.png?q=20" width="1171" height="510"></div><img alt="Best Car Dealer’s Price Monitoring Services : X-Byte Enterprise Crawling" class="gg gh cp t u gi ak gr" width="1171" height="510"><noscript><img alt="Best Car Dealer’s Price Monitoring Services : X-Byte Enterprise Crawling" class="cp t u gi ak" src="https://miro.medium.com/max/2342/1*m6lRSB8tt6t_Cc_rtujpxg.png" width="1171" height="510"></noscript></div></div></div></div><figcaption class="bo eg gs gt gu cn cl cm gv gw bj ef">How to Do Price Monitoring from Car Dealers Sites Image</figcaption></figure></div></div></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="5d61" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">The automobile business is booming in all countries including the USA. According to <strong class="gz hl">NADA</strong>, since the year 2018, the USA’s 16,794 franchised dealers had sold over 8.6 million light-duty vehicles. The sale of new vehicles has touched the figure of more than $500 billion. Altogether, the dealerships had ordered 155 million repairs, whereas and services sales have reached $58 billion.</p><figure class="fv fw fx fy fz fm cl cm paragraph-image"><div class="gc gd ge gf ak"><div class="cl cm hm"><div class="gl r ge gm"><div class="hn r"><div class="gg gh cp t u gi ak eh gj gk"><img alt="Car Dealership Data Scraping Service in X-Byte Enterprise Crawling USA" class="cp t u gi ak go gp gq" src="https://miro.medium.com/max/60/0*FojMKGbJCF_pYckl.png?q=20" width="1024" height="446"></div><img alt="Car Dealership Data Scraping Service in X-Byte Enterprise Crawling USA" class="gg gh cp t u gi ak gr" width="1024" height="446"><noscript><img alt="Car Dealership Data Scraping Service in X-Byte Enterprise Crawling USA" class="cp t u gi ak" src="https://miro.medium.com/max/2048/0*FojMKGbJCF_pYckl.png" width="1024" height="446"></noscript></div></div></div></div><figcaption class="bo eg gs gt gu cn cl cm gv gw bj ef">Car Dealership Data Scraping Image</figcaption></figure><p id="cb86" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">However, there are no location-wise automobile dealer directories available and mostly, the information needs to be collected either using personal contacts or a location-specific Google search. If you want to try and scrape data about <a href="https://www.xbyte.io/solution/price-monitoring/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">Price Monitoring</strong></a> from the car dealers sites, you can use Google itself as well as use the keywords that should comprise- “car dealer”, together with the location as well as the car’s company name. You can try the initial few links which are not endorsed by Google and scrape data from them. It can be reiterated for different locations as well as car companies through an excel sheet. Although, the efficiency and scale of manually scraping car dealers data are particularly limited.</p><p id="cc1e" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">For scraping data in an automated manner, you can use professional Web Crawling Services of X-Byte Enterprise Crawling, once you get the website list ready. As a professional <a href="https://www.xbyte.io/service/web-scraping-service/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">Web and Data Scraping Service</strong></a> provider, X-Byte Enterprise Crawling can provide this data in the plug-and-use format. Provided that you have collected the resources with required persistence, your data will be clean and dependable. However, if you are unaware of which cars are more accepted in which states or countries, you can only scrape data for getting that information and for that <a href="https://www.xbyte.io/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">X-Byte Enterprise Crawling</strong></a> is the finest option.</p><figure class="fv fw fx fy fz fm cl cm paragraph-image"><div class="gc gd ge gf ak"><div class="cl cm hm"><div class="gl r ge gm"><div class="hn r"><div class="gg gh cp t u gi ak eh gj gk"><img alt="Professional Data Scraping Service in X-Byte Enterprise Crawling USA" class="cp t u gi ak go gp gq" src="https://miro.medium.com/max/60/0*UzedILL1A06iPc0Z.png?q=20" width="1024" height="446"></div><img alt="Professional Data Scraping Service in X-Byte Enterprise Crawling USA" class="gg gh cp t u gi ak gr" width="1024" height="446"><noscript><img alt="Professional Data Scraping Service in X-Byte Enterprise Crawling USA" class="cp t u gi ak" src="https://miro.medium.com/max/2048/0*UzedILL1A06iPc0Z.png" width="1024" height="446"></noscript></div></div></div></div><figcaption class="bo eg gs gt gu cn cl cm gv gw bj ef">Professional Data Scraping Image</figcaption></figure><p id="8180" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">All the car dealers around the world promote themselves heavily to get more customers. <a href="https://www.xbyte.io/solution/social-media-monitoring/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">Data scraping from social media websites</strong></a> and online communities can help you collect information on all the popular auto dealers. Besides that, there are many other resources to scrape price monitoring data from car dealers sites on the web.</p><p id="fb75" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">As the web is growing exponentially, it doesn’t matter what research you are doing or applications you are creating, the web is the finest place to collect data and the same applies to scrape data on car dealers. Whether you are creating an application that will utilize your location as well as get you your nearest car dealer or if you want to create a ranking or reviewing site for car dealers, data scraping will assist you to create your data source and fill your website or app with information.</p><p id="7e11" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu"><a href="https://www.xbyte.io/solution/price-monitoring/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">Scraping price monitoring data</strong></a> from the car dealers are extremely difficult and that’s where X-Byte Enterprise Crawling has an important role to play.</p><p id="5e89" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">Many dealerships work in both new and used cars and they provide vehicles that fit everyone’s requirements. They offer wonderful customer service with the help of friendly salespeople. They have a lot of used cars to select from. These dealers offer cars of different brands.</p><p id="842a" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">Scraping data from all these car dealers is difficult and that’s what <a href="https://www.xbyte.io/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">X-Byte Enterprise Crawling</strong></a> does easily! At X-Byte Enterprise Crawling, we scrape price monitoring data from car dealers’ sites as well as do car inventory scraping and used cars inventory scraping.</p><h1 id="1817" class="hs ht dc bk bj hu de hv dg hw hx hy hz ia ib ic id">What Data We Extract from Car Dealers Websites?</h1><ul class=""><li id="ca49" class="gx gy dc bk gz b ha ie hc if he ig hg ih hi ii hk ij ik il">Car Name</li><li id="84ef" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Pricing</li><li id="4a26" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Seller Name</li><li id="1b98" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Seller’s Address</li><li id="5ef5" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Ratings</li><li id="64fe" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Number of Reviews</li><li id="8042" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Contact Details</li></ul><h1 id="1ca1" class="hs ht dc bk bj hu de hv dg hw hx hy hz ia ib ic id">Additional Car Information</h1><p id="c8fa" class="gx gy dc bk gz b ha ie hc if he ig hg ih hi ii hk cu">You can also get addition car information like:</p><ul class=""><li id="e687" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk ij ik il">Fuel Type</li><li id="72ba" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">City MPG</li><li id="0cd3" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Highway MPG</li><li id="5cea" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Drivetrain</li><li id="1786" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Engine</li><li id="9235" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Mileage</li><li id="edad" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Interior Color</li><li id="55f5" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">External Color</li><li id="cccc" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Stock</li><li id="93fa" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Transmission</li><li id="1837" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">VIN</li></ul><p id="ee71" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">The dealers usually have online inventories that are amongst the key reasons why these dealerships are a brilliant source for different car companies. All the drivers can approach their sales associates and let their customer service specify how they make used or new car procedure, hassle-free!</p><figure class="fv fw fx fy fz fm cl cm paragraph-image"><div class="gc gd ge gf ak"><div class="cl cm hm"><div class="gl r ge gm"><div class="hn r"><div class="gg gh cp t u gi ak eh gj gk"><img alt="Car Data Scraping Service in X-Byte Enterprise Crawling USA" class="cp t u gi ak go gp gq" src="https://miro.medium.com/max/60/0*3Ctm2YeAnWAK8ite.png?q=20" width="1024" height="446"></div><img alt="Car Data Scraping Service in X-Byte Enterprise Crawling USA" class="gg gh cp t u gi ak gr" width="1024" height="446"><noscript><img alt="Car Data Scraping Service in X-Byte Enterprise Crawling USA" class="cp t u gi ak" src="https://miro.medium.com/max/2048/0*3Ctm2YeAnWAK8ite.png" width="1024" height="446"></noscript></div></div></div></div><figcaption class="bo eg gs gt gu cn cl cm gv gw bj ef">Car Data Scraping Image</figcaption></figure><h1 id="0cbb" class="hs ht dc bk bj hu de hv dg hw hx hy hz ia ib ic id">Why Should You Hire a Professional Like X-Byte Enterprise Crawling for Price Monitoring from Car Dealers Sites?</h1><ul class=""><li id="fc54" class="gx gy dc bk gz b ha ie hc if he ig hg ih hi ii hk ij ik il">Our car dealer site <a href="https://www.xbyte.io/solution/price-monitoring/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">Price Monitoring Services</strong></a> can save your invaluable time &amp; money. We can find information in only some hours that might take some days or weeks in case, you perform that manually!</li><li id="b693" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Our expert team realizes how to change unstructured data into a structured one. Our car dealer site price monitoring scrapers keep track of all the pages of directed websites to get all the required results.</li><li id="1861" class="gx gy dc bk gz b ha im hc in he io hg ip hi iq hk ij ik il">Our expert <a href="https://www.xbyte.io/resources/faqs/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">Consumer Support</strong></a> team always helps you if you face any problem whereas using our car dealer site price monitoring service. Our car dealer site price monitoring services are reliable, skillful, and offer faster results without any mistakes.</li></ul><p id="47e5" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu">Contact <a href="https://www.xbyte.io/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gz hl">X-Byte Enterprise Crawling</strong></a> for all your car dealer site price monitoring services requirements or ask for a free quote!</p></div></div></section><hr class="ir ef is it iu gu iv iw ix iy iz"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="720c" class="gx gy dc bk gz b ha hb hc hd he hf hg hh hi hj hk cu"><strong class="gz hl">Visit Our Site :</strong> <a href="https://www.xbyte.io/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">www.xbyte.io</a></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="39ab" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">6 Tips on How to Do Data Scraping of Unstructured Data</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@3idatascraping?source=post_page-----4ac44f0cda29----------------------"><img alt="3i Data Scraping" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/1*pTBf2MTUyiDWXDDY4GglGg.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@3idatascraping?source=post_page-----4ac44f0cda29----------------------">3i Data Scraping</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@3idatascraping/6-tips-on-how-to-do-data-scraping-of-unstructured-data-4ac44f0cda29?source=post_page-----4ac44f0cda29----------------------">Jun 30, 2017</a> <!-- -->·<!-- --> <!-- -->3<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/4ac44f0cda29/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/4ac44f0cda29/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%403idatascraping%2F6-tips-on-how-to-do-data-scraping-of-unstructured-data-4ac44f0cda29&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm fm"><div class="gc r fv gd"><div class="ge r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*t6twgG_AhTS12trEglwjQg.png?q=20" width="1017" height="770" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="1017" height="770" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/2034/1*t6twgG_AhTS12trEglwjQg.png" width="1017" height="770" role="presentation"></noscript></div></div></div></div></figure><p id="49a7" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Data scraping, data extraction or web scraping is an automatic web method to fetch or do data collection from your web. It converts unstructured data into structured one which can warehouse to the database.</p><p id="58ba" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">6 Tips on How to Do Data Scraping of Unstructured Data</strong></p><h2 id="209c" class="gy gz dc bk bj ha hb hc hd he hf hg hh hi hj hk hl">1. Find a reliable solution for unstructured data scraping</h2><p id="ce3d" class="gj gk dc bk gl b gm hm go hn gq ho gs hp gu hq gw cu">Conventional technical approaches of unstructured data scraping isolate the moving parts of the results to make that easier for the programmers resolve the issues.</p><p id="671d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">They are unapproachable from the real time usage setups. However, while the non-programmatic method builds a code, this opens the chances of accepting indications regarding proposed use of the extracted data.</p><p id="1b3e" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Any automated data scraping software and checking solution can do this, for example:</strong></p><p id="af63" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">• Avoid worthless links and attain projected data quickly<br> • Build a responsive load footprint for the targeted websites<br> • Use lesser hardware resources</p><p id="7e83" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">It will help in the data mining of unstructured data using the unstructured data scraping tools.</p><p id="8d21" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Besides, non-programmatic method, it will capture knowledge regarding targeted websites better and influence that to promptness of learning using multiple websites, adding up to the ability of scaling proficiently and brilliantly while extracting the unstructured data.</p><h2 id="fac2" class="gy gz dc bk bj ha hb hc hd he hf hg hh hi hj hk hl">2. Be capable enough to work for the unstructured data</h2><p id="abe6" class="gj gk dc bk gl b gm hm go hn gq ho gs hp gu hq gw cu">All the web scraping software depend on the HTML delimiters that breakdown while the main HTML changes as well as the requirement for fixing problems need to be tracked manually.</p><p id="6327" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Any automated data scraping and tracing solution identify additions and changes with accuracy, offering only the ideal data using techniques of unstructured data examination.</p><h2 id="ab02" class="gy gz dc bk bj ha hb hc hd he hf hg hh hi hj hk hl">3. Efficiently produce and manage scripts for unstructured data</h2><p id="9917" class="gj gk dc bk gl b gm hm go hn gq ho gs hp gu hq gw cu">Any automatic <strong class="gl gx">web data scraping solution</strong>, particularly for the data extraction tools for retailer, can help in rationalizing the workflows and processes at scale, smoothly generates productivity gains. They consist of:</p><p id="ce1c" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">• Automatic load handling and deployment<br> • Bulk operations to complete the jobs and task preparation<br> • Consistent testing for superior quality assurance<br> • Data mining techniques and tools for the unstructured data<br> • Shared request lists and schemas for handling different projects having dependable team practices<br> • Tools which effortlessly increase the mass regulation activities<br> • User subscriptions and agent migrations among the systems</p><h2 id="2901" class="gy gz dc bk bj ha hb hc hd he hf hg hh hi hj hk hl">4. Alteration of Unstructured Data into Useful Structured Data</h2><p id="c5c6" class="gj gk dc bk gl b gm hm go hn gq ho gs hp gu hq gw cu">Unstructured data can be used for the human eyes whereas well-structured can be used for computers.</p><p id="281f" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">A conventional data scraper as well as an automated <strong class="gl gx">data scraping solution</strong>, both can <a href="http://www.3idatascraping.com/services.php" class="at cg hr hs ht hu" target="_blank" rel="noopener nofollow"><strong class="gl gx">transform the unstructured data into structured data</strong></a>, offering analysis to take superior business decisions.</p><p id="ba45" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Nevertheless, the automated data scraping solutions integrate and use data normalization techniques to ensure that your structured data is effortlessly converted into main data insights.</p><h2 id="5b00" class="gy gz dc bk bj ha hb hc hd he hf hg hh hi hj hk hl">5. Reduce the errors through automation in collecting structured data</h2><p id="880e" class="gj gk dc bk gl b gm hm go hn gq ho gs hp gu hq gw cu">Visual abstraction is the method to use machine learning for creating well-organized codes. Visual abstraction recognizes each and every web page just like a human examines a page visually.</p><p id="b391" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">However, an automated <strong class="gl gx">data mining and extraction solution</strong> can help you better with a superior level of visual abstraction without utilizing the HTML structures. This doesn’t break while it gets page variations.</p><h2 id="364c" class="gy gz dc bk bj ha hb hc hd he hf hg hh hi hj hk hl">6. Combine data mining results with business operations and procedures</h2><p id="5588" class="gj gk dc bk gl b gm hm go hn gq ho gs hp gu hq gw cu">In the existing data-obsessed business environment, many teams frequently interrelate with the data collection as well as analysis procedures.</p><p id="98ba" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Business organizations searching for the web scraping about unstructured data have to talk about and support all the data necessities, for different purposes.</p><p id="69aa" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">As the business requirements are different, built-in aspects supportive to different requirements are the key for ranging higher frequencies and volumes of the data collection.</p><p id="b661" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Find out more about accurate, result-oriented and better accessible data scraping solutions.</p><p id="412d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You can <a href="http://www.3idatascraping.com/contact-us.php" class="at cg hr hs ht hu" target="_blank" rel="noopener nofollow"><strong class="gl gx">contact us</strong></a> to discover how the automated data intelligence and data extraction solution can improve your organization’s productivity, efficiency, and general workflow.</p></div></div></section><hr class="hv ef hw hx hy hz ia ib ic id ie"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="6373" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><em class="if">Originally published at </em><a href="http://www.3idatascraping.com/6-tips-on-how-to-do-data-scraping-of-unstructured-data.php" class="at cg hr hs ht hu" target="_blank" rel="noopener nofollow"><em class="if">www.3idatascraping.com</em></a><em class="if"> on June 30, 2017.</em></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="ed3c" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Asynchronous Web Scraping in Python using concurrent module.</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@apbetahouse45?source=post_page-----a5ca1b7f82e4----------------------"><img alt="Pranav Gajjewar" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/2*NUbV1uECdoDM087Sf8SnsQ.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@apbetahouse45?source=post_page-----a5ca1b7f82e4----------------------">Pranav Gajjewar</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@apbetahouse45/asynchronous-web-scraping-in-python-using-concurrent-module-a5ca1b7f82e4?source=post_page-----a5ca1b7f82e4----------------------">Apr 22, 2018</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/a5ca1b7f82e4/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/a5ca1b7f82e4/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40apbetahouse45%2Fasynchronous-web-scraping-in-python-using-concurrent-module-a5ca1b7f82e4&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="cl cm fm"><div class="fy r fz ga"><div class="gb r"><div class="ft fu cp t u fv ak eh fw fx"><img class="cp t u fv ak gc gd ge" src="https://miro.medium.com/max/60/1*9hUizS9cheTAYEH1NhkUDA.png?q=20" width="700" height="400" role="presentation"></div><img class="ft fu cp t u fv ak gf" width="700" height="400" role="presentation"><noscript><img class="cp t u fv ak" src="https://miro.medium.com/max/1400/1*9hUizS9cheTAYEH1NhkUDA.png" width="700" height="400" role="presentation"></noscript></div></div></div></figure><p id="a0db" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Ever felt frustrated at how long your web scraping script takes to complete the task? Have you ever wished there was a faster way to do your web scraping?</p><p id="a5c7" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Well, there is. And I’m going to show you today how you can increase the performance of your scraper in a very beginner friendly way.</p><p id="cc6f" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">In this post we will also talk about asynchronous programming in Python. And then apply that knowledge to optimize web scraping.</p><p id="9656" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Let’s dive in!</p><h2 id="b269" class="gu gv dc bk bj gw gx gy gz ha hb hc hd he hf hg hh"><strong class="az">What is Asynchronous execution? And why would you want it?</strong></h2><p id="6512" class="gg gh dc bk gi b gj hi gl hj gn hk gp hl gr hm gt cu">If you’re a beginner in web scraping, then I assume you’ve worked with <code class="ga hn ho hp hq b">requests</code> and <code class="ga hn ho hp hq b">BeautifulSoup</code> modules in python. And what you generally do while writing your scraper is as follows —</p><pre class="fn fo fp fq fr hr hs ht"><span id="37fe" class="gu gv dc bk hq b eg hu hv r hw">def parse(soup):<br>    # Extract data<br>    # return data</span><span id="3bf5" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">urls = [...]</span><span id="daaa" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">results = []<br>for url in urls:<br>    r = requests.get(url)<br>    soup = BeautifulSoup(r.content, 'lxml')<br>    results.append(parse(soup))</span></pre><p id="1946" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Or you might use a different structure than this. But the end result is same. The way you code your scraper is in a <em class="ic">synchronous</em> fashion.</p><p id="f4f6" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">What it means is that your program goes through the target URLs one by one, in a synchronized way. You send a GET request to the server and the server takes some time to send a response. But what do you suppose is happening while your program is waiting for a response from the server?</p><p id="ffde" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu"><strong class="gi id">Nothing!</strong></p><p id="3097" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">That’s right. The network request is the instruction that takes the most time in your script. And when you’re doing it in a synchronous way, your script remains idle a large amount of time which is spent waiting for the server response. How would you make use of that free time?</p><p id="7301" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">It’s quite obvious. We do not want our program to remain idle while one of the GET requests is waiting for server’s response. We want our program to move ahead with other URLS and their processing without being blocked due to one sluggish network request.</p><blockquote class="ie if ig"><p id="5109" class="gg gh dc ic gi b gj gk gl gm gn go gp gq gr gs gt ih cu">A<!-- -->synchronous programming is simply executing multiple instructions simultaneously.</p></blockquote><p id="61c9" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So we need a way to process multiple URLs simultaneously and independent of one another. Let’s see how we can achieve this in Python.</p><h2 id="4872" class="gu gv dc bk bj gw gx gy gz ha hb hc hd he hf hg hh">How asynchronous execution is achieved?</h2><p id="ff62" class="gg gh dc bk gi b gj hi gl hj gn hk gp hl gr hm gt cu">In this section, I will discuss different strategies of asynchronous execution. If you’re just interested in the asynchronous python code, you can skip this part.</p><p id="da97" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">There are many ways in which asynchronous execution is implemented. Three broad categories of multi-processing can be given as —</p><ul class=""><li id="bee7" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt ii ij ik">Process level multi-processing.</li><li id="1448" class="gg gh dc bk gi b gj il gl im gn in gp io gr ip gt ii ij ik">Thread level multi-processing.</li><li id="8cab" class="gg gh dc bk gi b gj il gl im gn in gp io gr ip gt ii ij ik">Application level multi-processing.</li></ul><p id="5ca6" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">If you have some background in Unix operating system, you would be familiar with these concepts. Still, I will do my best to explain them as concisely and cogently as possible.</p><p id="05ab" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">In Process level multi-processing, you can achieve asynchronous execution by dividing the total work across separate processes. Each process running on a different processor core. In this way, your original task is divided into number of chunks and all of these chunks are being processed simultaneously. This level of multi-processing is in-built in an OS. So all you have to do is utilize this and let the kernel worry about process scheduling.</p><p id="3571" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Thread level multi-processing is almost same as the previous one. Except in this case, we are dividing the task across multiple threads. A thread is like a process but a lightweight process. And we can add multiple threads under a single process context. So all of these thread would belong to the same process. This feature is also implemented in the OS itself. We just need to utilize this using Python and we will see how it is done.</p><p id="e433" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Application level multi-processing is somewhat different than the previous two. Here the OS is under the impression that it is executing only one process with a single thread. But our application itself schedules different tasks on that thread for execution. So the asynchronous nature of execution is implemented in our application program itself.</p><p id="5a97" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">These are the main ways to handle parallel execution on a traditional Unix system. Now we will see how we can use the <code class="ga hn ho hp hq b">concurrent</code> module in Python to utilize these concepts and to boost our scraping speed.</p><h2 id="bb08" class="gu gv dc bk bj gw gx gy gz ha hb hc hd he hf hg hh">Implementing asynchronous execution:</h2><p id="130d" class="gg gh dc bk gi b gj hi gl hj gn hk gp hl gr hm gt cu">Okay, so you must be itching to get started. Let’s start coding —</p><pre class="fn fo fp fq fr hr hs ht"><span id="ddeb" class="gu gv dc bk hq b eg hu hv r hw">from concurrent.futures import ProcessPoolExecutor<br>from concurrent.futures import ThreadPoolExecutor<br>from concurrent.futures import Future </span></pre><p id="f4e2" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So we first import the things we require. You will observe that we imported <code class="ga hn ho hp hq b">ProcessPoolExecutor</code> and <code class="ga hn ho hp hq b">ThreadPoolExecutor.</code> Both of these classes correspond to Process level and Thread level multi-processing respectively. We only need to use one of these. And for our use case i.e web scraping, both of these will be effective.</p><p id="7da5" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So the multi-processing features in the OS are abstracted and we can directly do parallel processing using the above classes.</p><blockquote class="ie if ig"><p id="deef" class="gg gh dc ic gi b gj gk gl gm gn go gp gq gr gs gt cu">The <code class="ga hn ho hp hq b"><a href="https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures" class="at cg iq ir is it" target="_blank" rel="noopener nofollow">concurrent.futures</a></code> module provides a high-level interface for asynchronously executing callables.</p><p id="99bb" class="gg gh dc ic gi b gj gk gl gm gn go gp gq gr gs gt cu">The asynchronous execution can be performed with threads, using <code class="ga hn ho hp hq b"><a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor" class="at cg iq ir is it" target="_blank" rel="noopener nofollow">ThreadPoolExecutor</a></code>, or separate processes, using <code class="ga hn ho hp hq b"><a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor" class="at cg iq ir is it" target="_blank" rel="noopener nofollow">ProcessPoolExecutor</a></code>.</p></blockquote><p id="0d75" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">The way it works is that we have a <strong class="gi id">pool </strong>of threads or processes. And we can assign some task to each of them and they will start executing independently of each other.</p><p id="5c05" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">We can create a pool —</p><pre class="fn fo fp fq fr hr hs ht"><span id="580a" class="gu gv dc bk hq b eg hu hv r hw">pool = ThreadPoolExecutor(3) # This means a pool of 3 threads<br>            OR<br>pool = ProcessPoolExecutor(3) # This means a pool of 3 processes </span></pre><p id="0031" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Now we can <code class="ga hn ho hp hq b">submit</code> or <code class="ga hn ho hp hq b">map</code> different tasks to each individual thread or process.</p><p id="3519" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Suppose we have a list of 100 URLs and we want to download the HTML page for each URL and do some post-processing and extract data.</p><pre class="fn fo fp fq fr hr hs ht"><span id="92a3" class="gu gv dc bk hq b eg hu hv r hw">def download_and_extract(url):<br>    r = requests.get(url)<br>    soup = BeautifulSoup(r.content, 'lxml')<br>    # Some data extraction logic<br>    return data</span></pre><p id="a4a3" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">We have a function <code class="ga hn ho hp hq b">download_and_extract</code> which will gather our data and we want to gather data from the 100 URLs previously mentioned.</p><p id="ae52" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">If we were to do this synchronously, it would take 100 multiplied by average time for one GET request ( assuming post-processing time is trivial ). But instead if we divide the 100 URLs on 4 separate threads/processes, then the time required would be 1/4th the original time, at least theoretically.</p><p id="4480" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So let us try this —</p><pre class="fn fo fp fq fr hr hs ht"><span id="022c" class="gu gv dc bk hq b eg hu hv r hw">URLs = [...]</span><span id="041e" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">def d_and_e(url): # Our download and extract function<br>    ....</span><span id="3abd" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">with ProcessPoolExecutor(max_workers=4) as executor:<br>    futures = [ executor.submit(d_and_e, url) for url in URLs]</span></pre><p id="9bd7" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Here we have slightly modified the Pool initialization to suit our use case but it does the same thing when we initialized it previously.</p><p id="dc42" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu"><code class="ga hn ho hp hq b">executor.submit</code> function takes two parameters in our code. The first one is the task we want to perform Or more technically, the function we want to execute and the parameters for the execution of our function. The executor will distribute the work across 4 different processes with each process executing one instance of <code class="ga hn ho hp hq b">download_and_extract</code> for the given URL.</p><p id="e554" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">But how do we know when the tasks are done? And what about the data that we wanted?</p><p id="76b9" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu"><code class="ga hn ho hp hq b">executor.submit</code> returns a <code class="ga hn ho hp hq b">Future</code> object.</p><blockquote class="ie if ig"><p id="70a9" class="gg gh dc ic gi b gj gk gl gm gn go gp gq gr gs gt cu">(Future) Encapsulates the asynchronous execution of a callable.</p></blockquote><p id="2f7a" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">This object represents the asynchronous execution of a specific function. You can read more about its properties in the <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Future" class="at cg iq ir is it" target="_blank" rel="noopener nofollow">documentation</a>. We will only focus on two main functions for this object that we will require viz. <code class="ga hn ho hp hq b">done</code> and <code class="ga hn ho hp hq b">result.</code></p><p id="4e3c" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu"><code class="ga hn ho hp hq b">done()</code> function returns the bool value <code class="ga hn ho hp hq b">True</code> if the function has finished executing or if there was some exception in it. And when it has finished execution, we can retrieve the result using the <code class="ga hn ho hp hq b">result()</code> function.</p><pre class="fn fo fp fq fr hr hs ht"><span id="b25f" class="gu gv dc bk hq b eg hu hv r hw">URLs = [...]</span><span id="37cf" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">def d_and_e(url): # Our download and extract function<br>    ....</span><span id="f7f0" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">with ProcessPoolExecutor(max_workers=4) as executor:<br>    futures = [ executor.submit(d_and_e, url) for url in URLs]</span><span id="8b5a" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">    results = []<br>    for result in concurrent.futures.as_completed(futures):<br>        results.append(result)</span></pre><p id="a20e" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">Here’s another new thing — <code class="ga hn ho hp hq b">as_completed().</code></p><p id="df86" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">The function <code class="ga hn ho hp hq b">as_completed()</code> simply determines the order of the results that are returned by the future. Using this function, we avoid having to write a block of code where we keep checking whether a given <code class="ga hn ho hp hq b">Future</code> is <code class="ga hn ho hp hq b">done()</code> or not.</p><p id="8111" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">The function will start generating results as soon as any one of the functions being executed yields some result. And then we simply append that result to our main collection of data.</p><p id="a681" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">And that’s it! Using these simple concepts you can make your program multi-processing capable. Web scraping is just a simple example to illustrate the concept. You can apply this concept anywhere you want.</p><p id="aded" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">We will look at a fully coded and working example below —</p><pre class="fn fo fp fq fr hr hs ht"><span id="f9fd" class="gu gv dc bk hq b eg hu hv r hw">import time<br>import requests<br>from bs4 import BeautifulSoup<br>from concurrent.futures import ProcessPoolExecutor, as_completed</span><span id="907a" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">URLs = [ ... ] # A long list of URLs.</span><span id="8fa1" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">def parse(url):<br>    r = requests.get(url)<br>    soup = BeautifulSoup(r.content, 'lxml')<br>    return soup.find_all('a')</span><span id="d4c0" class="gu gv dc bk hq b eg hx hy hz ia ib hv r hw">with ProcessPoolExecutor(max_workers=4) as executor:<br>    start = time.time()<br>    futures = [ executor.submit(parse, url) for url in URLs ]<br>    results = []<br>    for result in as_completed(futures):<br>        results.append(result)<br>    end = time.time()<br>    print("Time Taken: {:.6f}s".format(end-start))</span></pre><p id="a48c" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">You can now experiment using this example with URLs of your choice and different degrees of parallelization. See what conclusions you can draw from this.</p><h2 id="9dad" class="gu gv dc bk bj gw gx gy gz ha hb hc hd he hf hg hh">What next?</h2><p id="d77c" class="gg gh dc bk gi b gj hi gl hj gn hk gp hl gr hm gt cu">In this post, I demonstrated how to divide a particular task across multiple threads and process. And we achieved asynchronous execution of a specific task in this way.</p><p id="5b04" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">But think about this, the task we are doing i.e downloading data from the network, it is admittedly being done across multiple processes but on any one process the task is still being done synchronously.</p><p id="93b5" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">What I mean is that we are simply performing the task in a parallel fashion. So in any one of the threads/processes, that one process or thread still remains idle for some time until server responds.</p><p id="0ae2" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">There is a way in which we can overcome this and make our scraping truly asynchronous. We would have to use Application level multi-processing to accomplish this.</p><p id="a889" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">We want our program to send a GET request and while the server is processing that request, we want our program to suspend that request and move on to next requests. When the server finally responds, we want that data to be mapped to the correct request. In this way, we do not allow our program to remain idle at all. It is always doing something.</p><p id="c11b" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">This is possible in python using <code class="ga hn ho hp hq b">asyncio</code> and <code class="ga hn ho hp hq b">aiohttp</code> modules. I will explore both of those modules in the context of web scraping in a future post.</p><p id="2048" class="gg gh dc bk gi b gj gk gl gm gn go gp gq gr gs gt cu">So stay tuned!</p></div></div></section></div><div><div class="dq u dr ds dt du"></div><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><div><div id="646b" class="eb ec cw bi ed b ee ef eg eh ei ej ek el em en eo"><h1 class="ed b ee ep eg eq ei er ek es em et cw">master web scraping : understand the big picture</h1></div><div class="eu"><div class="n ev ew ex ey"><div class="o n"><div><a rel="noopener" href="/@phamtan500?source=post_page-----68540a66ec7f----------------------"><div class="ez fa fb"><div class="fc n fd o p dq fe ff fg fh fi du"><svg width="57" height="57" viewbox="0 0 57 57"><path fill-rule="evenodd" clip-rule="evenodd" d="M28.5 1.2A27.45 27.45 0 0 0 4.06 15.82L3 15.27A28.65 28.65 0 0 1 28.5 0C39.64 0 49.29 6.2 54 15.27l-1.06.55A27.45 27.45 0 0 0 28.5 1.2zM4.06 41.18A27.45 27.45 0 0 0 28.5 55.8a27.45 27.45 0 0 0 24.44-14.62l1.06.55A28.65 28.65 0 0 1 28.5 57 28.65 28.65 0 0 1 3 41.73l1.06-.55z"></path></svg></div><img alt="tan pham" class="r fj fb fa" src="https://miro.medium.com/fit/c/96/96/1*9tc04KWdvWh1HdM4eui2Vw.jpeg" width="48" height="48"></div></a></div><div class="fk ak r"><div class="n"><div style="flex:1"><span class="au b bi bj bk bl r cw q"><div class="fl n o fm"><span class="au fn fo bj ay fp ba bb bc bd cw"><a class="fq fr bq br bs bt bu bv bw bx fs ca cb ft fu" rel="noopener" href="/@phamtan500?source=post_page-----68540a66ec7f----------------------">tan pham</a></span><div class="fv r bf h"><button class="fw cw q cf fx fy fz ga bx ft gb gc gd ge gf gg ci au b bi gh gi bl cj ck cl cm cn ca">Follow</button></div></div></span></div></div><span class="au b bi bj bk bl r bm bn"><span class="au fn fo bj ay fp ba bb bc bd bm"><div><a class="fq fr bq br bs bt bu bv bw bx fs ca cb ft fu" rel="noopener" href="/datamadeeasy/master-web-scraping-understand-the-big-picture-68540a66ec7f?source=post_page-----68540a66ec7f----------------------">Jul 14, 2019</a> <!-- -->·<!-- --> <!-- -->2<!-- --> min read</div></span></span></div></div><div class="n gj gk gl gm gn go gp gq ab"><div class="n o"><div class="gr r bf"><a href="//medium.com/p/68540a66ec7f/share/twitter?source=post_actions_header---------------------------" class="fq fr bq br bs bt bu bv bw bx gs gt ca cb ft fu" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gr r bf"><a href="//medium.com/p/68540a66ec7f/share/facebook?source=post_actions_header---------------------------" class="fq fr bq br bs bt bu bv bw bx gs gt ca cb ft fu" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gu r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fdatamadeeasy%2Fmaster-web-scraping-understand-the-big-picture-68540a66ec7f&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="fq fr bq br bs bt bu bv bw bx gs gt ca cb ft fu" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="3114" class="gv gw cw bi gx b gy gz ha hb hc hd he hf hg hh hi dv">In this post we will explain how to do web scraping with beautiful soup and selenium.</p><figure class="hk hl hm hn ho hp dm dn paragraph-image"><div class="hq hr ez hs ak"><div class="dm dn hj"><div class="hy r ez hz"><div class="ia r"><div class="ht hu dq t u hv ak ay hw hx"><img class="dq t u hv ak ib ic id" src="https://miro.medium.com/max/60/1*8B-fOXenzIDWg-CXTTwK9g.png?q=20" width="1203" height="316" role="presentation"></div><img class="ht hu dq t u hv ak ie" width="1203" height="316" role="presentation"><noscript><img class="dq t u hv ak" src="https://miro.medium.com/max/2406/1*8B-fOXenzIDWg-CXTTwK9g.png" width="1203" height="316" role="presentation"></noscript></div></div></div></div></figure><h1 id="8bb2" class="if ig cw bi au av ee ih eg ii ij ik il im in io ip">selenium web driver</h1><p id="5a42" class="gv gw cw bi gx b gy iq ha ir hc is he it hg iu hi dv">Any data scraping task start with a url to page which contain data need to be scraped. Selenium web driver will take input url and produce content in html.</p><p id="72df" class="gv gw cw bi gx b gy gz ha hb hc hd he hf hg hh hi dv">Some people will ask, why we need selenium ? because we could simply use package like <code class="hz iv iw ix iy b">requests</code> to download html from input url.</p><p id="423a" class="gv gw cw bi gx b gy gz ha hb hc hd he hf hg hh hi dv">First reason is now a day, a lot of modern web page is dynamic meain contain javascript. Actual html content only be created when javascript code running through browser.</p><p id="29aa" class="gv gw cw bi gx b gy gz ha hb hc hd he hf hg hh hi dv">For example if you run following code, console will print out only js code due to <code class="hz iv iw ix iy b">requests</code> could not handle js</p><pre class="hk hl hm hn ho iz ja dc"><span id="ceea" class="jb ig cw bi iy b fo jc jd r je">import  requests</span><span id="4d49" class="jb ig cw bi iy b fo jf jg jh ji jj jd r je">url = '<a href="https://www.youtube.com/'" class="fq cn jk jl jm jn" target="_blank" rel="noopener nofollow">https://www.youtube.com/'</a><br>response = requests.get(url)<br>print(response.content)</span></pre><p id="8dbe" class="gv gw cw bi gx b gy gz ha hb hc hd he hf hg hh hi dv">Second reason to use selenium is some time in order to go to page contain needed information, we need to do some action on browser like login, click to access some where.</p><h1 id="15fd" class="if ig cw bi au av ee ih eg ii ij ik il im in io ip">beautiful soup</h1><p id="df05" class="gv gw cw bi gx b gy iq ha ir hc is he it hg iu hi dv">After html content is render with selenium web driver, we need <code class="hz iv iw ix iy b">beautiful soup</code> to parse this html to pull out target data.</p><p id="73fd" class="gv gw cw bi gx b gy gz ha hb hc hd he hf hg hh hi dv">Before use beautiful soup we need to know where our data located inside html structure. Normally we will use chrome developer tool to do this. Then finally we could pull out texts or links from html.</p><p id="ff70" class="gv gw cw bi gx b gy gz ha hb hc hd he hf hg hh hi dv">Access my full course on <a href="https://www.datamadeeasy.co/courses/master-web-scraping-with-python-do-16-projects" class="fq cn jk jl jm jn" target="_blank" rel="noopener nofollow">master web scraping with python</a></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="f01e" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How to do web scraping with python</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@itylergarrett.tag?source=post_page-----e6c4d5e860f6----------------------"><img alt="Tyler Garrett" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/2*H0Y15YLZacoZTnkRFsSFdQ.png" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@itylergarrett.tag?source=post_page-----e6c4d5e860f6----------------------">Tyler Garrett</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@itylergarrett.tag/using-pip-to-install-requests-and-lxml-on-python-3-7-mac-os-e6c4d5e860f6?source=post_page-----e6c4d5e860f6----------------------">Jul 5, 2018</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/e6c4d5e860f6/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/e6c4d5e860f6/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40itylergarrett.tag%2Fusing-pip-to-install-requests-and-lxml-on-python-3-7-mac-os-e6c4d5e860f6&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="b425" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Hey, <a href="http://bit.ly/Web-scraping" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">web scraping</a> is easy with python 3.7 —the way I was doing it before this tutorial was overly complex and extremely inefficient.</p><blockquote class="ge gf gg"><p id="7709" class="fm fn dc gh fo b fp fq fr fs ft fu fv fw fx fy fz cu">I wrote this blog in July/2018, when I was still learning how to program in Python. This particular version is not as complete or easy as my future version on web scraping.</p><p id="b392" class="fm fn dc gh fo b fp fq fr fs ft fu fv fw fx fy fz cu">It was not my best blog but it does show a quick way to do some web scraping basics, like grabbing numbers off a website. However, I reblogged this topic in a more straight forward example.</p><p id="cced" class="fm fn dc gh fo b fp fq fr fs ft fu fv fw fx fy fz cu">Please — if you’re interested in <a class="at cg ga gb gc gd" target="_blank" rel="noopener" href="/@itylergarrett.tag/learning-web-scraping-with-python-requests-beautifulsoup-936e6445312">learning web scraping with python</a>, check out the blog I released on Dec.25,2018!</p></blockquote><div class="gi gj gk gl gm gn"><a rel="noopener" href="/@itylergarrett.tag/learning-web-scraping-with-python-requests-beautifulsoup-936e6445312"><div class="gq n ar"><div class="gr n co p gs gt"><h2 class="bj gu gv bl dc"><div class="eh go ej ek gp em">Learning Web Scraping with Python, Requests, &amp; BeautifulSoup</div></h2><div class="gw r"><h3 class="bj ef eg bl bo"><div class="eh go ej ek gp em">Did you know learning web scraping w/ Python, Requests, and Beautiful Soup is easy...</div></h3></div><div class="gx r"><h4 class="bj ef fb bl bo"><div class="eh go ej ek gp em">medium.com</div></h4></div></div><div class="gy r"><div class="gz r ha hb hc gy hd he hf"></div></div></div></a></div><p id="03a1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">I was trying to make an a drag and drop ETL handle web scraping but it isn’t designed for parsing HTML.</p><p id="1ce3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Meet Python, lxml, requests, beautifulsoup4, etc… throw away the paid for services, throw away third party vendors, start web scraping on your own, on your computer, now!</p><p id="6d32" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Share this with your friends: <a href="http://tinyurl.com/yaupbwv8" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">http://tinyurl.com/yaupbwv8</a></p></div></div><div class="hg ak"><figure class="hh hi hj hk hl hg ak paragraph-image"><div class="hm hn ho hp ak"><div class="hv r ho hw"><div class="hx r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*mWK4ePw6uECtp6YMYYVQQw.png?q=20" width="1454" height="116" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="1454" height="116" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/2908/1*mWK4ePw6uECtp6YMYYVQQw.png" width="1454" height="116" role="presentation"></noscript></div></div></div><figcaption class="bo eg ic id ie cn cl cm if ig bj ef">Learn how I made this blog URL into a tinyurl with 2 lines of python code! Follow along here @ <a class="at cg ga gb gc gd" target="_blank" rel="noopener" href="/@itylergarrett.tag/how-to-build-shortlinks-with-python-on-mac-ef6fec7cc1b1">how to make tinyurls with python</a>.</figcaption></figure></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><h1 id="6974" class="ih ii dc bk bj gu de ij dg ik il im in io ip iq ir">Web scraping is easy in Python….</h1><p id="bf1d" class="fm fn dc bk fo b fp is fr it ft iu fv iv fx iw fz cu">Web scrapping is easy in python but you need to ramp up. It won’t take long, and let me know if you get stuck, I sure as hell did a lot.</p><p id="5bae" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">So above, Python, lxml, requests, etc… Speaking gibberish, well I explain everything in tutorials/blogs, without a single funnel or recommendation to buy anything! You’re welcome.</p><h1 id="31d5" class="ih ii dc bk bj gu de ij dg ik il im in io ip iq ir">Using Pip to install Requests and lxml on python 3.7 — MAC OS</h1><p id="943a" class="fm fn dc bk fo b fp is fr it ft iu fv iv fx iw fz cu">Found a blog about web <a href="http://docs.python-guide.org/en/latest/scenarios/scrape/" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">scraping</a> and it had a little bit of python, not much explanation, per the usual programmer blog, a bunch of short hand written stuff as if we speak this language… Hours of troubleshooting, digging through SEO’ed websites, and finally…. I think we have some cool content. Btw, the blog mentioned about scraping — it also has a bit of an incomplete tutorial surrounding this process/method. I will continue to clean this up, and maybe reblog it on my website at <a href="http://tylergarrett.com" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">tylergarret.com</a>.</p><p id="c8a3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Python is extremely efficient at handling web parsing, I’m blown away. I was trying to do this in softwares and it was a massive work-around/waste of time… This is exciting, but what is it.</p></div></div><div class="hg ak"><figure class="hh hi hj hk hl hg ak paragraph-image"><div class="hm hn ho hp ak"><div class="hv r ho hw"><div class="ix r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*qDOcZEgJD0gEX7p12UtOWg.png?q=20" width="1584" height="398" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="1584" height="398" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/3168/1*qDOcZEgJD0gEX7p12UtOWg.png" width="1584" height="398" role="presentation"></noscript></div></div></div></figure></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="c117" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Did you miss that? In 6 lines of code, we are getting prices…</p></div></div><div class="hg ak"><figure class="hh hi hj hk hl hg ak paragraph-image"><div class="hv r ho hw"><div class="iy r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*K0ogX5MKMSax9hsqtbZvaw.png?q=20" width="1346" height="422" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="1346" height="422" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/2692/1*K0ogX5MKMSax9hsqtbZvaw.png" width="1346" height="422" role="presentation"></noscript></div></div></figure></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="0a93" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">And boom prices… from a website…</p></div></div><div class="hg ak"><figure class="hh hi hj hk hl hg ak paragraph-image"><div class="hv r ho hw"><div class="iz r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*5cG5ZTjlUPIN98fcbjbV1Q.png?q=20" width="1370" height="570" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="1370" height="570" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/2740/1*5cG5ZTjlUPIN98fcbjbV1Q.png" width="1370" height="570" role="presentation"></noscript></div></div></figure></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="aca3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">One more line of code, and boom, buyers + prices… Now we are looking at prices online, instantly, loop this and you have price analysis… Push into a database, you have prices over time… Here we go…</p><p id="f087" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Python… What is it though?</p></div></div><div class="hg ak"><figure class="hh hi hj hk hl hg ak paragraph-image"><div class="hm hn ho hp ak"><div class="hv r ho hw"><div class="ja r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*0wjrtrO21Tmoom840YxQvQ.png?q=20" width="1162" height="878" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="1162" height="878" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/2324/1*0wjrtrO21Tmoom840YxQvQ.png" width="1162" height="878" role="presentation"></noscript></div></div></div><figcaption class="bo eg ic id ie cn cl cm if ig bj ef">Learning python is like space force. Everyone has an opinion, but none of it is factual, true, or exactly the truth. Like politics.</figcaption></figure></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><figure class="jc jd je jf jg hg cl cm paragraph-image"><div class="hm hn ho hp ak"><div class="cl cm jb"><div class="hv r ho hw"><div class="jh r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*nan1kle3WubmFbEobeJnmg.png?q=20" width="1210" height="914" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="1210" height="914" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/2420/1*nan1kle3WubmFbEobeJnmg.png" width="1210" height="914" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg ic id ie cn cl cm if ig bj ef">lol. Let me explain below.</figcaption></figure><h1 id="3f02" class="ih ii dc bk bj gu de ij dg ik il im in io ip iq ir">Setting up pip to install requests and lxml</h1><p id="7e27" class="fm fn dc bk fo b fp is fr it ft iu fv iv fx iw fz cu">Below I’m going to show you how to setup your requests and lxml on python 3.7 on mac os. <a class="at cg ga gb gc gd" target="_blank" rel="noopener" href="/@tyler_48883/trying-to-learn-python-from-scratch-6ab60bf08907">Trying to learn python from scratch</a> is a lot of fun, appears to be a bit of a ramp up, <strong class="fo ji">but that’s why I’m blogging about it every day</strong>.</p><p id="960b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">It’s easy, fun, and user friendly, don’t be discouraged trying to figure out how to get it working, keep it up, maybe give <a href="https://www.jetbrains.com/pycharm/download/" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">pycharm</a> a visit too.</p><h2 id="0d40" class="jj ii dc bk bj gu jk jl jm jn jo jp jq jr js jt ju">Installing python is important for any data related guru.</h2><p id="ba96" class="fm fn dc bk fo b fp is fr it ft iu fv iv fx iw fz cu"><a class="at cg ga gb gc gd" target="_blank" rel="noopener" href="/@tyler_48883/how-to-install-python-blogged-by-a-technical-non-developer-38e90347bc89">Learning how to install python</a> seems to be critical for the future of my career, I’m tired of spending countless hours making a software do what code has done for decades… Time to grow a pair. I don’t know if homebrew helped me but I wrote about <a class="at cg ga gb gc gd" target="_blank" rel="noopener" href="/@tyler_48883/my-miserable-path-to-python-expertise-continues-on-the-macbook-homebrew-for-dumbies-f414768c1dbf">how to setup homebrew for python</a> too.</p><p id="81da" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">A quick video on setting up pip on your mac. And I cover <a class="at cg ga gb gc gd" target="_blank" rel="noopener" href="/@itylergarrett.tag/setting-up-python-3-7-on-windows-10-a-non-developer-tutorial-e836639ca16">how to setup pip on your windows 10</a> too. Be sure to catch up, and install python, etc… Let me know if you get stuck, I’m still learning myself and want to know if I’m getting you past the point that I was stuck, trying to dig through….</p><h2 id="76ef" class="jj ii dc bk bj gu jk jl jm jn jo jp jq jr js jt ju">Learning how to do web scraping with python!</h2><p id="7b65" class="fm fn dc bk fo b fp is fr it ft iu fv iv fx iw fz cu">When I first started learning about web scraping, no one wanted to help me and I was stuck figuring out how to parse HTML with a tool 100% not designed to handle the task… So, when you hit this bridge, I hope more than anything my blog ranks half decent and you don’t waste any time trying to do web scraping with random tools, paid services, or third part vendors.</p><p id="f506" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">So, here we go! Web scraping is fun, you need to dig through a bunch of tabs if you ignore my blogs.</p><p id="8ab1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">If you made it this far… You’re clearly really intelligent and enjoy learning. Please follow along below, so you don’t have to open 20 tabs and spin your mother flipping wheels off. This should be easy! It’s just a bunch of junk in google searches right now.</p><figure class="hh hi hj hk hl hg cl cm paragraph-image"><div class="hm hn ho hp ak"><div class="cl cm jv"><div class="hv r ho hw"><div class="jw r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*x2O2t-rs2IxcttO5uEyUTA.png?q=20" width="1346" height="70" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="1346" height="70" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/2692/1*x2O2t-rs2IxcttO5uEyUTA.png" width="1346" height="70" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg ic id ie cn cl cm if ig bj ef">You feel me? Anyways, ping me if you need any help, I will likely be very far ahead of this point when this article begins ranking… I don’t want you struggling to get ahead, please ping me if you want source code to any projects I’m blogging about. Enjoy! And thanks for the follows.</figcaption></figure><p id="7201" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Follow along w/ this video to get pip working on your mac, before you begin.</p><figure class="hh hi hj hk hl hg"><div class="hv r ho"><div class="jx r"><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FyBdZZGPpYxg%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DyBdZZGPpYxg&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FyBdZZGPpYxg%2Fhqdefault.jpg&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=youtube" allowfullscreen="" frameborder="0" height="480" width="854" title="Installing Python Pip on Mac OSX" class="cp t u hs ak" scrolling="auto"></iframe></div></div></figure><p id="5097" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Let’s start with the imports:</p><pre class="hh hi hj hk hl jy jz ka"><span id="68f3" class="jj ii dc bk kb b eg kc kd r ke"><strong class="kb ji">from</strong> lxml <strong class="kb ji">import</strong> html<br><strong class="kb ji">import</strong> requests</span></pre><p id="c4a8" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Well these imports will not just work out of the box. Sorry. Which throws a big loop in the ramp up, also there’s some syntax that’s incorrect <a href="http://docs.python-guide.org/en/latest/scenarios/scrape/" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">here</a>, that I will update below.</p><p id="0a32" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">First you need to install requests. Below ensures you’re installing pip installs in python3, VS other python installs on your mac. Like 2.7, which comes with your mac, don’t uninstall or break that too… <a href="https://stackoverflow.com/questions/3819449/how-to-uninstall-python-2-7-on-a-mac-os-x-10-6-4" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">leave it alone</a>. Or reinstall everything.</p><p id="9537" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Install requests with this code in your terminal, ensure pip is function on this machine by typing “pip” in your CMD/terminal.</p><pre class="hh hi hj hk hl jy jz ka"><span id="9dc7" class="jj ii dc bk kb b eg kc kd r ke">python3 -m pip install requests --user</span></pre><p id="96f9" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Above code offers access to pushing a new installation. You can learn a little more about some of these pieces of code <a href="https://packaging.python.org/tutorials/installing-packages/#ensure-you-can-run-pip-from-the-command-line" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">here</a>.</p><p id="b68c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Python3 has another install called lxml, make sure you install it to python3 if you want to use the 3.7 python install.</p><pre class="hh hi hj hk hl jy jz ka"><span id="8c50" class="jj ii dc bk kb b eg kc kd r ke">python3 -m pip install lxml</span></pre><p id="0b6e" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Installing lxml took me a little bit because I kept typing xmlx. Be sure you’re not installing weird stuff.</p><figure class="hh hi hj hk hl hg cl cm paragraph-image"><div class="hm hn ho hp ak"><div class="cl cm kf"><div class="hv r ho hw"><div class="kg r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*ByIlRCPi0F-xhZUlacdFWw.png?q=20" width="1608" height="1298" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="1608" height="1298" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/3216/1*ByIlRCPi0F-xhZUlacdFWw.png" width="1608" height="1298" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg ic id ie cn cl cm if ig bj ef">here’s the rest of the code working plus using ()… in the code, which the tutorial does not include.</figcaption></figure><p id="812f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Now we want to “get” the HTML, and parse through looking for buyers and prices.</p><pre class="hh hi hj hk hl jy jz ka"><span id="10cc" class="jj ii dc bk kb b eg kc kd r ke">page = requests.get<strong class="kb ji">(</strong>'http://econpy.pythonanywhere.com/ex/001.html'<strong class="kb ji">)</strong><br>tree = html.fromstring<strong class="kb ji">(</strong>page.content<strong class="kb ji">)</strong></span></pre><p id="b9ce" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">After a quick analysis, we see that in our page the data is contained in two elements — one is a div with title ‘buyer-name’ and the other is a span with class ‘item-price’:</p><p id="8599" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">HTML looks like this:</p><pre class="hh hi hj hk hl jy jz ka"><span id="14d0" class="jj ii dc bk kb b eg kc kd r ke"><strong class="kb ji">&lt;div</strong> title="buyer-name"<strong class="kb ji">&gt;</strong>Carson Busses<strong class="kb ji">&lt;/div&gt;</strong><br><strong class="kb ji">&lt;span</strong> class="item-price"<strong class="kb ji">&gt;</strong>$29.95<strong class="kb ji">&lt;/span&gt;</strong></span></pre><p id="b0d7" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Knowing this we can create the correct XPath query and use the lxml <code class="hw kh ki kj kb b">xpath</code> function like this:</p><p id="0b32" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Here’s the code to capture the values in the html.</p><pre class="hh hi hj hk hl jy jz ka"><span id="f204" class="jj ii dc bk kb b eg kc kd r ke"><em class="gh">#This will create a list of buyers:</em><br>buyers = tree.xpath<strong class="kb ji">(</strong>'//div[@title="buyer-name"]/text()'<strong class="kb ji">)</strong><br><em class="gh">#This will create a list of prices</em><br>prices = tree.xpath<strong class="kb ji">(</strong>'//span[@class="item-price"]/text()'<strong class="kb ji">)</strong></span></pre><p id="03d5" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Let’s see what we got exactly:</p><pre class="hh hi hj hk hl jy jz ka"><span id="4cb6" class="jj ii dc bk kb b eg kc kd r ke"><strong class="kb ji">print</strong> 'Buyers: '<strong class="kb ji">,</strong> buyers<br><strong class="kb ji">print</strong> 'Prices: '<strong class="kb ji">,</strong> prices</span></pre><p id="68b9" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Boom.</p><p id="6ce1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Now you have your next step, time to start learning how to push this into a database!</p><p id="1faf" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Oh you’re still here…</p><p id="d927" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">DO you want to <a class="at cg ga gb gc gd" target="_blank" rel="noopener" href="/@itylergarrett.tag/how-to-build-shortlinks-with-python-on-mac-ef6fec7cc1b1">automate building tinyurls</a>? It’s super important for SEO, so head over here.</p><figure class="hh hi hj hk hl hg cl cm paragraph-image"><div class="hm hn ho hp ak"><div class="cl cm kk"><div class="hv r ho hw"><div class="kl r"><div class="hq hr cp t u hs ak eh ht hu"><img class="cp t u hs ak hy hz ia" src="https://miro.medium.com/max/60/1*Hr3EucT35S93tQUTpK5hXA.png?q=20" width="708" height="118" role="presentation"></div><img class="hq hr cp t u hs ak ib" width="708" height="118" role="presentation"><noscript><img class="cp t u hs ak" src="https://miro.medium.com/max/1416/1*Hr3EucT35S93tQUTpK5hXA.png" width="708" height="118" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg ic id ie cn cl cm if ig bj ef">Too easy right!?</figcaption></figure><p id="47af" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">typos by <a href="http://tylergarrett.com" class="at cg ga gb gc gd" target="_blank" rel="noopener nofollow">tyler garrett</a></p><div class="gi gj gk gl gm gn"><a rel="noopener" href="/@itylergarrett.tag/pip-install-on-python2-break-python3-heres-a-solution-to-pip-install-version-problems-5e05c9d808f2"><div class="gq n ar"><div class="gr n co p gs gt"><h2 class="bj gu gv bl dc"><div class="eh go ej ek gp em">Pip install on python2 break python3? Here’s a solution to pip install version problems.</div></h2><div class="gw r"><h3 class="bj ef eg bl bo"><div class="eh go ej ek gp em">I rushed ahead, broke shit, here’s the explanation…If you kick your python off in your CMD or terminal, you will want…</div></h3></div><div class="gx r"><h4 class="bj ef fb bl bo"><div class="eh go ej ek gp em">medium.com</div></h4></div></div><div class="gy r"><div class="km r ha hb hc gy hd he hf"></div></div></div></a></div><div class="gi gj gk gl gm gn"><a rel="noopener" href="/@itylergarrett.tag/setting-up-python-3-7-on-windows-10-a-non-developer-tutorial-e836639ca16"><div class="gq n ar"><div class="gr n co p gs gt"><h2 class="bj gu gv bl dc"><div class="eh go ej ek gp em">Setting up Pip on Python 3.7 in Windows 10 — A non-developer version</div></h2><div class="gw r"><h3 class="bj ef eg bl bo"><div class="eh go ej ek gp em">Install python, or do my how to install python 3.7 windows 10. This can be accomplished using the python installer at…</div></h3></div><div class="gx r"><h4 class="bj ef fb bl bo"><div class="eh go ej ek gp em">medium.com</div></h4></div></div><div class="gy r"><div class="kn r ha hb hc gy hd he hf"></div></div></div></a></div><div class="gi gj gk gl gm gn"><a rel="noopener" href="/@itylergarrett.tag/zomg-doing-basic-math-in-python-or-jump-in-and-do-https-requests-mac-users-imnotadeveloper-a4c1df9c5735"><div class="gq n ar"><div class="gr n co p gs gt"><h2 class="bj gu gv bl dc"><div class="eh go ej ek gp em">{zomg} Doing basic math in Python, or jump in and do HTTPS requests — mac users. #imnotadeveloper</div></h2><div class="gw r"><h3 class="bj ef eg bl bo"><div class="eh go ej ek gp em">Python is finally moving a bit now, I’ve been blogging about this randomly from both PC and MAC… Finally made some…</div></h3></div><div class="gx r"><h4 class="bj ef fb bl bo"><div class="eh go ej ek gp em">medium.com</div></h4></div></div><div class="gy r"><div class="ko r ha hb hc gy hd he hf"></div></div></div></a></div><div class="gi gj gk gl gm gn"><a rel="noopener" href="/@itylergarrett.tag/how-to-install-python-3-7-on-windows-10-pc-the-non-developer-version-b063e1913b39"><div class="gq n ar"><div class="gr n co p gs gt"><h2 class="bj gu gv bl dc"><div class="eh go ej ek gp em">How to install python 3.7 on windows 10 PC , The non-developer version.</div></h2><div class="gw r"><h3 class="bj ef eg bl bo"><div class="eh go ej ek gp em">Installing python on windows 10 PC is the end goal, the long term project is to learn python, to build an app that…</div></h3></div><div class="gx r"><h4 class="bj ef fb bl bo"><div class="eh go ej ek gp em">medium.com</div></h4></div></div><div class="gy r"><div class="kp r ha hb hc gy hd he hf"></div></div></div></a></div><div class="gi gj gk gl gm gn"><a rel="noopener" href="/@tyler_48883/my-miserable-path-to-python-expertise-continues-on-the-macbook-homebrew-for-dumbies-f414768c1dbf"><div class="gq n ar"><div class="gr n co p gs gt"><h2 class="bj gu gv bl dc"><div class="eh go ej ek gp em">My miserable path to python expertise, continues, on the Macbook — Homebrew for dumbies.</div></h2><div class="gw r"><h3 class="bj ef eg bl bo"><div class="eh go ej ek gp em">After digging around trying to understand what was the next step, everything leads me to setting up Homebrew. Thanks…</div></h3></div><div class="gx r"><h4 class="bj ef fb bl bo"><div class="eh go ej ek gp em">medium.com</div></h4></div></div><div class="gy r"><div class="kq r ha hb hc gy hd he hf"></div></div></div></a></div><div class="gi gj gk gl gm gn"><a rel="noopener" href="/@tyler_48883/how-to-install-python-blogged-by-a-technical-non-developer-38e90347bc89"><div class="gq n ar"><div class="gr n co p gs gt"><h2 class="bj gu gv bl dc"><div class="eh go ej ek gp em">How to Install Python — Blogged by a technical non-developer.</div></h2><div class="gw r"><h3 class="bj ef eg bl bo"><div class="eh go ej ek gp em">What’s up, let’s talk about how to install python, blogged by a technical non-developer. I can dabble in code, have…</div></h3></div><div class="gx r"><h4 class="bj ef fb bl bo"><div class="eh go ej ek gp em">medium.com</div></h4></div></div></div></a></div><div class="gi gj gk gl gm gn"><a rel="noopener" href="/@tyler_48883/trying-to-learn-python-from-scratch-6ab60bf08907"><div class="gq n ar"><div class="gr n co p gs gt"><h2 class="bj gu gv bl dc"><div class="eh go ej ek gp em">Trying to learn python from scratch</div></h2><div class="gw r"><h3 class="bj ef eg bl bo"><div class="eh go ej ek gp em">We can only imagine trying to learn python from scratch is not something anyone will be able to do without a nice…</div></h3></div><div class="gx r"><h4 class="bj ef fb bl bo"><div class="eh go ej ek gp em">medium.com</div></h4></div></div></div></a></div><p id="8967" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Cheers.</p></div></div></section></div><div><div class="ds u dt du dv dw"></div><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><div><div id="38ce" class="ed ee ef at eg b eh ei ej ek el em en eo ep eq er"><h1 class="eg b eh es ej et el eu en ev ep ew ef">Coupling Web Scraping with Functional programming in R for Scale</h1></div><div class="ex"><div class="n ey ez fa fb"><div class="o n"><div><a href="/@amrwrites?source=post_page-----1bc4509eef29----------------------" rel="noopener"><img alt="AbdulMajedRaja RS" class="r fc fd fe" src="https://miro.medium.com/fit/c/96/96/0*w0fQtGt26KJqthRE.jpg" width="48" height="48"></a></div><div class="ff ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r ef q"><div class="fg n o fh"><span class="as cx fi au cd fj fk fl fm fn ef"><a href="/@amrwrites?source=post_page-----1bc4509eef29----------------------" class="dc dd bb bc bd be bf bg bh bi fo bl bm fp fq" rel="noopener">AbdulMajedRaja RS</a></span><div class="fr r ap h"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoupling-web-scraping-with-functional-programming-in-r-for-scale-1bc4509eef29&amp;source=-bee33e964a63-------------------------follow_byline-" class="fs ef q bq ft fu fv fw bi fp fx fy fz ga gb gc bt as b at gd cy aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cx fi au cd fj fk fl fm fn ax"><div><a class="dc dd bb bc bd be bf bg bh bi fo bl bm fp fq" rel="noopener" href="/coupling-web-scraping-with-functional-programming-in-r-for-scale-1bc4509eef29?source=post_page-----1bc4509eef29----------------------">Feb 11, 2019</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n ge gf gg gh gi gj gk gl ab"><div class="n o"><div class="gm r ap"><a href="//medium.com/p/1bc4509eef29/share/twitter?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gm r ap"><a href="//medium.com/p/1bc4509eef29/share/facebook?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gp r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcoupling-web-scraping-with-functional-programming-in-r-for-scale-1bc4509eef29&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="dc dd bb bc bd be bf bg bh bi gn go bl bm fp fq" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div><div class="gq"><div class="n p"><div class="gr gs gt gu gv gw ag gx ah gy aj ak"><figure class="ha hb hc hd he gq hf hg paragraph-image"><div class="hh hi hj hk ak"><div class="do dp gz"><div class="hq r hj hr"><div class="hs r"><div class="hl hm ds t u hn ak cd ho hp"><img class="ds t u hn ak ht hu hv" src="https://miro.medium.com/max/60/1*aPhII8tpOj9XsXXQNngPzw.jpeg?q=20" width="5184" height="3456" role="presentation"></div><img class="hl hm ds t u hn ak hw" width="5184" height="3456" role="presentation"><noscript><img class="ds t u hn ak" src="https://miro.medium.com/max/10368/1*aPhII8tpOj9XsXXQNngPzw.jpeg" width="5184" height="3456" role="presentation"></noscript></div></div></div></div><figcaption class="ax fi hx hy hz dq do dp ia ib as cx"><a href="https://unsplash.com/photos/-lp8sTmF9HA" class="dc by ic id ie if" target="_blank" rel="noopener nofollow">https://unsplash.com/photos/-lp8sTmF9HA</a></figcaption></figure></div></div></div><div class="n p"><div class="ac ae af ag ah ec aj ak"><p id="c774" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">In this article, we will see how to do web scraping with R while doing so, we’ll leverage functional programming in R to scale it up. The nature of the article is more like a cookbook-format rather than a documentation/tutorial-type, because the objective here is to explain how effectively web scraping can be coupled with Functional Programming</p><h2 id="4dfd" class="iu iv ef at as iw ix iy iz ja jb jc jd je jf jg jh"><strong class="bf">Web Scraping in R</strong></h2><p id="de47" class="ig ih ef at ii b ij ji il jj in jk ip jl ir jm it dx">Web scraping needs no introduction among Data enthusiasts. It’s one of the most viable and most essential ways of collecting Data when the data itself isn’t available.</p><p id="2026" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">Knowing web scraping comes very handy when you are in shortage of data or in need of Macroeconomics indicators or simply no data available for a particular project like a Word2vec / Language with a custom text dataset.</p><p id="73c1" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx"><code class="hr jn jo jp jq b">rvest</code> a beautiful (like BeautifulSoup in Python) package in R for web scraping. It also goes very well with the universe of <code class="hr jn jo jp jq b">tidyverse</code> and the super-handy <code class="hr jn jo jp jq b">%&gt;%</code> pipe operator.</p><h2 id="e1cc" class="iu iv ef at as iw ix iy iz ja jb jc jd je jf jg jh"><strong class="bf">Sample Use-case</strong></h2><p id="2a8d" class="ig ih ef at ii b ij ji il jj in jk ip jl ir jm it dx">Text Analysis of how customers feel about Etsy.com. For this, we are going to extract reviews data from <a href="http://trustpilot.com" class="dc by ic id ie if" target="_blank" rel="noopener nofollow">trustpilot.com</a>.</p><p id="1479" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">Below is the R code for scraping reviews from the first page of Trustpilot’s Etsy page. <a href="https://www.trustpilot.com/review/www.etsy.com?page=1" class="dc by ic id ie if" target="_blank" rel="noopener nofollow">URL: https://www.trustpilot.com/review/www.etsy.com?page=1</a></p><pre class="ha hb hc hd he jr js cm"><span id="d822" class="iu iv ef at jq b fi jt ju r jv">library(tidyverse) #for data manipulation - here for pipe<br>library(rvest) - for web scraping</span><span id="6d78" class="iu iv ef at jq b fi jw jx jy jz ka ju r jv">#single-page scraping</span><span id="22d8" class="iu iv ef at jq b fi jw jx jy jz ka ju r jv">url &lt;- "<a href="https://www.trustpilot.com/review/www.etsy.com?page=1" class="dc by ic id ie if" target="_blank" rel="noopener nofollow">https://www.trustpilot.com/review/www.etsy.com?page=1</a>"</span><span id="15eb" class="iu iv ef at jq b fi jw jx jy jz ka ju r jv">url %&gt;% <br>  read_html() %&gt;% <br>  html_nodes(".review-content__text") %&gt;% <br>  html_text() -&gt; reviews</span></pre><p id="022d" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">This is fairly a straightforward code where we pass on the URL to read the html content. Once the content is read, we use <code class="hr jn jo jp jq b">html_nodes</code> function to get the reviews text based on its <code class="hr jn jo jp jq b">css selector property</code> and finally just taking the text out of it <code class="hr jn jo jp jq b">html_text()</code> and assigning it to the R object <code class="hr jn jo jp jq b">reviews</code> .</p><p id="a97a" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">Below is the sample output of <code class="hr jn jo jp jq b">reviews</code>:</p><figure class="ha hb hc hd he gq do dp paragraph-image"><div class="hh hi hj hk ak"><div class="do dp kb"><div class="hq r hj hr"><div class="kc r"><div class="hl hm ds t u hn ak cd ho hp"><img class="ds t u hn ak ht hu hv" src="https://miro.medium.com/max/60/1*NfimrAbyjQFZ7icNctmTrw.png?q=20" width="2554" height="1770" role="presentation"></div><img class="hl hm ds t u hn ak hw" width="2554" height="1770" role="presentation"><noscript><img class="ds t u hn ak" src="https://miro.medium.com/max/5108/1*NfimrAbyjQFZ7icNctmTrw.png" width="2554" height="1770" role="presentation"></noscript></div></div></div></div></figure><p id="92ea" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">Well and Good. We’ve successfully scraped the reviews we wanted for our Analysis.</p><figure class="ha hb hc hd he gq hw ke bv kf kg kh ki kj bg kk kl km kn ko kp paragraph-image"><div class="do dp kd"><div class="hq r hj hr"><div class="kq r"><div class="hl hm ds t u hn ak cd ho hp"><img class="ds t u hn ak ht hu hv" src="https://miro.medium.com/max/60/1*zeuLPCYv_9AIn7Gz6VpeNQ.png?q=20" width="474" height="188" role="presentation"></div><img class="hl hm ds t u hn ak hw" width="474" height="188" role="presentation"><noscript><img class="ds t u hn ak" src="https://miro.medium.com/max/948/1*zeuLPCYv_9AIn7Gz6VpeNQ.png" width="474" height="188" role="presentation"></noscript></div></div></div></figure><p id="2d22" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">But the catch is the amount of reviews we’ve got is just 20 reviews — in that as we can see in the screenshot we’ve already got a non-English review that we might have to exclude in the data cleaning process.</p><p id="55eb" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">This all puts us in a situation to collect more data to compensate the above mentioned data loss and make the analysis more effective.</p><h2 id="1d42" class="iu iv ef at as iw ix iy iz ja jb jc jd je jf jg jh">Need for Scale</h2><p id="46ea" class="ig ih ef at ii b ij ji il jj in jk ip jl ir jm it dx">With the above code, we had scraped only from the first page (which is the most recent). So, Due to the need for more data, we have to expand our search to further pages, let’s say 10 other pages which will give us 200 raw reviews to work with before data processing.</p><h2 id="097c" class="iu iv ef at as iw ix iy iz ja jb jc jd je jf jg jh">Conventional Way</h2><p id="fdf0" class="ig ih ef at ii b ij ji il jj in jk ip jl ir jm it dx">The very conventional way of doing this is to use a loop — typically <code class="hr jn jo jp jq b">for</code>loop to iterate the URL from 1 to 20 to create 20 different URLs (String Concatenation at work) based on a base url. As we all know that’s more computationally intensive and the code wouldn’t be compact either.</p><h2 id="c4aa" class="iu iv ef at as iw ix iy iz ja jb jc jd je jf jg jh"><strong class="bf">The Functional Programming way</strong></h2><p id="a2e1" class="ig ih ef at ii b ij ji il jj in jk ip jl ir jm it dx">This is where we are going to use R’s functional programming support from the package <code class="hr jn jo jp jq b">purrr</code> to perform the same iteration but quite in R’s <code class="hr jn jo jp jq b">tidy</code> way within the same data pipeline as the above code. We’re going to use two functions from <code class="hr jn jo jp jq b">purrr</code> ,</p><ol class=""><li id="9b52" class="ig ih ef at ii b ij ik il im in io ip iq ir is it kr ks kt"><code class="hr jn jo jp jq b">map()</code> is the typical map from the functional programming paradigm, that takes a function and maps onto a series of values.</li><li id="a294" class="ig ih ef at ii b ij ku il kv in kw ip kx ir ky it kr ks kt"><code class="hr jn jo jp jq b">map2_chr()</code> is the evolution of map that takes additional arguments for the function and formats the output as a character.</li></ol><p id="021e" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx"><strong class="ii kz">Below is our Functional Programming Code</strong></p><pre class="ha hb hc hd he jr js cm"><span id="dad6" class="iu iv ef at jq b fi jt ju r jv">library(tidyverse)<br>library(rvest)<br>library(purrr)</span><span id="5c57" class="iu iv ef at jq b fi jw jx jy jz ka ju r jv">#multi-page</span><span id="c63b" class="iu iv ef at jq b fi jw jx jy jz ka ju r jv">url &lt;- "<a href="https://www.trustpilot.com/review/www.etsy.com?page=" class="dc by ic id ie if" target="_blank" rel="noopener nofollow">https://www.trustpilot.com/review/www.etsy.com?page=</a>" #base URL without the page number</span><span id="eadf" class="iu iv ef at jq b fi jw jx jy jz ka ju r jv">url %&gt;% <br>  map2_chr(1:10,paste0) %&gt;% #for building 20 URLs <br>  map(. %&gt;% <br>    read_html() %&gt;% <br>      html_nodes(".review-content__text") %&gt;% <br>      html_text()<br>  ) %&gt;% <br>  unlist() -&gt; more_reviews</span></pre><p id="c81f" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">As you can see, this code is very similar to the above single-page code and hence it makes it easier for anyone who understand the previous code to read this through with minimal prior knowledge.</p><p id="37e7" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">The additional operations in this code is that we build 20 new URLs (by changing the query value of the URL) and pass on those 20 URLs one-by-one for web scraping and finally as we’d get a list in return, we use <code class="hr jn jo jp jq b">unlist</code> to save all the reviews whose count must be 200 (20 reviews per page x 10 pages).</p><p id="fe5f" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">Let’s check how the output looks:</p><figure class="ha hb hc hd he gq do dp paragraph-image"><div class="hh hi hj hk ak"><div class="do dp la"><div class="hq r hj hr"><div class="lb r"><div class="hl hm ds t u hn ak cd ho hp"><img class="ds t u hn ak ht hu hv" src="https://miro.medium.com/max/60/1*gb-oTtYRXoBMBQ4_j83-dA.png?q=20" width="2586" height="912" role="presentation"></div><img class="hl hm ds t u hn ak hw" width="2586" height="912" role="presentation"><noscript><img class="ds t u hn ak" src="https://miro.medium.com/max/5172/1*gb-oTtYRXoBMBQ4_j83-dA.png" width="2586" height="912" role="presentation"></noscript></div></div></div></div></figure><p id="6758" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">Yes, 200 reviews it is. That fulfills our goal of collecting (fairly) sufficient data for performing the text analysis use-case we mentioned above.</p><p id="9ad1" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx">But the point of this article is to introduce you to the world of functional programming in R and to show how easily it fits in with the existing data pipeline / workflow and how compact it is and with a pinch of doubt, how efficient it is (than a typical for-loop). Hope, the article served its purpose.</p><ul class=""><li id="1654" class="ig ih ef at ii b ij ik il im in io ip iq ir is it lc ks kt"><strong class="ii kz">If you are more interested, Check out this </strong><a href="https://www.datacamp.com/courses/foundations-of-functional-programming-with-purrr?tap_a=5644-dce66f&amp;tap_s=210728-e54afe" class="dc by ic id ie if" target="_blank" rel="noopener nofollow"><strong class="ii kz">Datacamp course on Functional Programming with purrr</strong></a></li><li id="7828" class="ig ih ef at ii b ij ku il kv in kw ip kx ir ky it lc ks kt">The complete code used here is available <a href="https://github.com/amrrs/blogpost_codes/blob/master/rvest_purrr_scraping_at_scale.R" class="dc by ic id ie if" target="_blank" rel="noopener nofollow">here on github</a></li></ul><p id="51a0" class="ig ih ef at ii b ij ik il im in io ip iq ir is it dx"><em class="ld">Thanks: This entire article and code was inspired by the Session that Saurav Ghosh took in the Bengaluru R user group meetup</em></p></div></div></section></div><div><div class="dq u dr ds dt du"></div><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><div><div id="1f38" class="eb ec ed at ee b ef eg eh ei ej ek el em en eo ep"><h1 class="ee b ef eq eh er ej es el et en eu ed">Hands-On Web Scraping With Python</h1></div><div class="ev"><div class="n ew ex ey ez"><div class="o n"><div><a rel="noopener" href="/@rouvenglauert?source=post_page-----2dff4d1bf7be----------------------"><img alt="Rouven" class="r fa fb fc" src="https://miro.medium.com/fit/c/96/96/2*gZLiLCvc1Tmo8zVvTvhslA.jpeg" width="48" height="48"></a></div><div class="fd ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r ed q"><div class="fe n o ff"><span class="as cv fg au cd fh fi fj fk fl ed"><a class="da db bb bc bd be bf bg bh bi fm bl bm fn fo" rel="noopener" href="/@rouvenglauert?source=post_page-----2dff4d1bf7be----------------------">Rouven</a></span><div class="fp r ap h"><button class="fq ed q bq fr fs ft fu bi fn fv fw fx fy fz ga bt as b at gb cw aw bu bv bw bx by bl">Follow</button></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cv fg au cd fh fi fj fk fl ax"><div><a class="da db bb bc bd be bf bg bh bi fm bl bm fn fo" rel="noopener" href="/swlh/hands-on-web-scraping-with-python-2dff4d1bf7be?source=post_page-----2dff4d1bf7be----------------------">Nov 6, 2019</a> <!-- -->·<!-- --> <!-- -->8<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewbox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div><div class="n gc gd ge gf gg gh gi gj ab"><div class="n o"><div class="gk r ap"><a href="//medium.com/p/2dff4d1bf7be/share/twitter?source=post_actions_header---------------------------" class="da db bb bc bd be bf bg bh bi gl gm bl bm fn fo" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gk r ap"><a href="//medium.com/p/2dff4d1bf7be/share/facebook?source=post_actions_header---------------------------" class="da db bb bc bd be bf bg bh bi gl gm bl bm fn fo" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gn r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Fhands-on-web-scraping-with-python-2dff4d1bf7be&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="da db bb bc bd be bf bg bh bi gl gm bl bm fn fo" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="e8f9" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb hc dv"><span class="r hd he hf hg hh hi hj hk hl hm">W</span>eb scraping is inherently useful for many people, in particular those who do not know how to do it. I have written many web scraping scripts for friends. None of them had any programming or computer science related background. This tutorial is for all the Sociologists, Business Analysts, Literature Researcher and all other people sometimes need to automatically collect data from the web.</p><p id="83d1" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">At the end of this tutorial we will have a little script, which if you run it automatically collects an article from medium.com which you could for instance store in a .csv file. You then can process with another software of your choice. The goal is also to teach fundamentals you need in order to do simple website scraping. This guide will not make you a web developer or anything close to that, but I provide some references in the last section, if you are interested in deeper knowledge. For the sake of making this guide understandable for readers without technical background, I decided to oversimplify in some parts as well as I did not use exact terminology.</p><h1 id="df30" class="hn ho ed at as hp ef hq eh hr hs ht hu hv hw hx hy">The Things That I Will Not Explain</h1><p id="7ffb" class="go gp ed at gq b gr hz gt ia gv ib gx ic gz id hb dv">Here are listed the things which you need to know in order to follow this guide. In the scope of this tutorial there in no space to explain them, but I will add some resources so you can learn them beforehand.</p><ol class=""><li id="8775" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb ie if ig">What is Python and how do I use it?</li><li id="9f57" class="go gp ed at gq b gr ih gt ii gv ij gx ik gz il hb ie if ig">How do I install Python packages?</li></ol><h1 id="b37f" class="hn ho ed at as hp ef hq eh hr hs ht hu hv hw hx hy">Basics 1: See Websites From A Different Perspective</h1><p id="9896" class="go gp ed at gq b gr hz gt ia gv ib gx ic gz id hb dv">The first thing we have to understand in order to perform web scraping is the function of the browser. In order to do so, press F12 on the keyboard. If you are using Firefox or Chrome you see the developer tools popping up. One of the things you can see in the developer tools is the plain HTML text which your browser interprets into a website (see Fig. 1). We will need the developer tools later, in order to locate the data that we want to extract in the HTML text of the website. For now, close the developer tools again and lets discuss where the HTML text comes from in first place.</p><figure class="in io ip iq ir is dm dn paragraph-image"><div class="it iu hm iv ak"><div class="dm dn im"><div class="jb r hm jc"><div class="jd r"><div class="iw ix dq t u iy ak cd iz ja"><img class="dq t u iy ak je jf jg" src="https://miro.medium.com/max/60/1*Yv-nhzSOCr4VkCop2kV12A.png?q=20" width="1400" height="297" role="presentation"></div><img class="iw ix dq t u iy ak jh" width="1400" height="297" role="presentation"><noscript><img class="dq t u iy ak" src="https://miro.medium.com/max/2800/1*Yv-nhzSOCr4VkCop2kV12A.png" width="1400" height="297" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg ji jj jk do dm dn jl jm as cv">Fig 1. Your Browser interprets the HTML text, the Style Sheets (CSS) and the JavaScript code into a beautiful website.</figcaption></figure><p id="726f" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">The browser requests the data that it needs to display a website from a web server via a HTTP request. Such a requests gets then answered from a web server via a HTTP response (See Fig. 2). All this happens in the background while you are clicking through the web. For this guide, we are mostly interested in the body of the response, this is where the HTML text is located.</p><figure class="in io ip iq ir is dm dn paragraph-image"><div class="it iu hm iv ak"><div class="dm dn jn"><div class="jb r hm jc"><div class="jo r"><div class="iw ix dq t u iy ak cd iz ja"><img class="dq t u iy ak je jf jg" src="https://miro.medium.com/max/60/1*4jCsz3axu7pmKnDRcbH9IA.png?q=20" width="1401" height="301" role="presentation"></div><img class="iw ix dq t u iy ak jh" width="1401" height="301" role="presentation"><noscript><img class="dq t u iy ak" src="https://miro.medium.com/max/2802/1*4jCsz3axu7pmKnDRcbH9IA.png" width="1401" height="301" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg ji jj jk do dm dn jl jm as cv">Fig. 2 Your browser sends a HTTP request for the information it needs to display a website. The server answers with the demanded content.</figcaption></figure><h1 id="a66d" class="hn ho ed at as hp ef hq eh hr hs ht hu hv hw hx hy">Basics 2: HTML</h1><p id="cd82" class="go gp ed at gq b gr hz gt ia gv ib gx ic gz id hb dv">HTML is a language which allows to structure the content of a website. It is part of what we get as a response from the HTTP request. The HTML code consists of tags and content in between the tags. The tags are like markers for content on the website. The browser then knows in which style sheet (CSS) it has to look to into in order to display the content correctly.<br>A tag <em class="jp">&lt;h1&gt;Something&lt;/h1&gt; </em>for instance is interpreted by the browser as a heading of type 1. In the corresponding Style Sheet, it could for instance be noted that h1 heading have to be displayed in blue or a certain font.</p><p id="1c5b" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">We will use such tags later in order to find the information we want to extract. It is important to note that the &lt;h1&gt; tag for instance is in between the body tag. The tags have an hierarchical order, the inner tags are called children and the outer ones are called parents. In our case the &lt;body&gt; tag is the parent of the &lt;h1&gt; and the &lt;p&gt; tag. Later, when extracting a link from a heading we will come back to this concept.</p><figure class="in io ip iq ir is"><div class="jb r hm"><div class="jq r"><iframe src="https://medium.com/media/6eff4f71f0a3de4ab9d4170658082b31" allowfullscreen="" frameborder="0" height="0" width="0" title="" class="dq t u iy ak" scrolling="auto"></iframe></div></div><figcaption class="ax fg ji jj jk do dm dn jl jm as cv">Code 1: A simple example of an HTML structured document. You can run this HTML in your browser.</figcaption></figure><p id="3d88" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">We learned about interpretation of the HTML into a beautiful website and the request of the HTML text from the web server are two different things, that the browser does for us. Therefor, we can conclude, that we can write our own file containing only HTML text and have the browser interpret it. And this is what we are going to do now.</p><p id="9f0b" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">Create an empty text file on your desktop. Open the file and copy the text from (Code 1) into the file and save it. Right click on the file, click on <em class="jp">Open with </em>with any browser of your choice. If everything worked out, you should now see only the text in between the tags formatted in two different ways.</p></div></div></section><hr class="jr cv js jt ju jk jv jw jx jy jz"><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><p id="5621" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">After we got the basics we can now start to use Python to replace the browser. First we are going to make the HTTP request with the help of Python and then we are going to use also Python to actually extract the information from the webserver’s response.</p><h1 id="c572" class="hn ho ed at as hp ef hq eh hr hs ht hu hv hw hx hy">Get the request</h1><p id="b15d" class="go gp ed at gq b gr hz gt ia gv ib gx ic gz id hb dv">For this tutorial, our job will be to extract the headings from medium.com and put them into a .csv file. Then select one article and also save the text of the article.</p><p id="2cd5" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">We start to use Python in order to make a HTTP request without using the browser. Therefor, we will use the Python package called<a href="https://2.python-requests.org/en/master/" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow"><em class="jp"> </em>requests</a>. Requests allows us in a very simple manner to formulate an HTTP request and store the response so we can use it later for the extraction of our data.</p><figure class="in io ip iq ir is"><div class="jb r hm"><div class="jq r"><iframe src="https://medium.com/media/65078b92cfbe6f6aaeeaa6bc4149303b" allowfullscreen="" frameborder="0" height="0" width="0" title="" class="dq t u iy ak" scrolling="auto"></iframe></div></div><figcaption class="ax fg ji jj jk do dm dn jl jm as cv">Code 2: Most simple HTTP GET request in python with the help of requests</figcaption></figure><p id="5e1d" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">In code to we first make a request with the<em class="jp"> get()</em> function and we store the response into the variable <em class="jp">r</em>. Then we print the HTML text of the response which is located in the attribute <em class="jp">text </em>of the response object.</p><h1 id="9ea4" class="hn ho ed at as hp ef hq eh hr hs ht hu hv hw hx hy">Find the data on the website</h1><p id="7715" class="go gp ed at gq b gr hz gt ia gv ib gx ic gz id hb dv">We will work with the non personalized version of the medium.com start page. The easiest way to see how it looks is to open a browser window in <a href="https://support.mozilla.org/en-US/kb/private-browsing-use-firefox-without-history" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">incognito mode</a> and go to medium.com. Then open the developer tools again with F12. And use the inspection tool to find the corresponding HTML part for the headings you want to extract. In our case click with the inspection tool activated onto the main heading. The developer tools will now jump to the part of the HTML text which is responsible for the main heading.</p><figure class="in io ip iq ir is dm dn paragraph-image"><div class="dm dn ke"><div class="jb r hm jc"><div class="kf r"><div class="iw ix dq t u iy ak cd iz ja"><img class="dq t u iy ak je jf jg" src="https://miro.medium.com/max/60/1*dnj_TkWzjwu80ImOOET5Ww.png?q=20" width="23" height="20" role="presentation"></div><img class="iw ix dq t u iy ak jh" width="23" height="20" role="presentation"><noscript><img class="dq t u iy ak" src="https://miro.medium.com/max/46/1*dnj_TkWzjwu80ImOOET5Ww.png" width="23" height="20" role="presentation"></noscript></div></div></div><figcaption class="ax fg ji jj jk do dm dn jl jm as cv">Inspection tool to find HTML text passage corresponding to the content on the website.</figcaption></figure><figure class="in io ip iq ir is dm dn paragraph-image"><div class="it iu hm iv ak"><div class="dm dn kg"><div class="jb r hm jc"><div class="kh r"><div class="iw ix dq t u iy ak cd iz ja"><img class="dq t u iy ak je jf jg" src="https://miro.medium.com/max/60/1*8RUMQaHQ7S68gZC07zmVug.png?q=20" width="814" height="264" role="presentation"></div><img class="iw ix dq t u iy ak jh" width="814" height="264" role="presentation"><noscript><img class="dq t u iy ak" src="https://miro.medium.com/max/1628/1*8RUMQaHQ7S68gZC07zmVug.png" width="814" height="264" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg ji jj jk do dm dn jl jm as cv">Fig. 3 Part of the HTML text of medium.com. The blue highlighted text is what you marked with the inspector tool.</figcaption></figure><p id="0382" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">We can identify which HTML tag medium has used for the main heading, by looking into the blue marked part and check for the tag which is written in the smaller and larger signs <em class="jp">&lt;&gt;</em> . As we can see in Fig. 3 medium.com has used an <em class="jp">&lt;h1&gt;</em> tag for the main heading. In the next step we will find all<em class="jp"> &lt;h1&gt;</em> tags used on the website and extract the heading itself.</p><h1 id="703b" class="hn ho ed at as hp ef hq eh hr hs ht hu hv hw hx hy">Extract the data from the website</h1><p id="83a3" class="go gp ed at gq b gr hz gt ia gv ib gx ic gz id hb dv">To extract the heading we have to first find it in the response, that we got from the HTTP request and secondly we have to remove the surrounding HTML structures. The package we are going to use for this job is called <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">beautiful soup</a>.</p><p id="6535" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">Beautiful Soup makes it possible to search through the the different HTML tags, but before we extract certain HTML tags we have to decide which tags we are interested in. The biggest helper here will be the developer tools of your browser, which we have seen before.</p><figure class="in io ip iq ir is"><div class="jb r hm"><div class="jq r"><iframe src="https://medium.com/media/999c6cc10b3732faf2a0f1d34144ea1c" allowfullscreen="" frameborder="0" height="0" width="0" title="" class="dq t u iy ak" scrolling="auto"></iframe></div></div><figcaption class="ax fg ji jj jk do dm dn jl jm as cv">Code 3 Making a requests, parsing the response, searching for &lt;h1&gt; tags, print the search result, print the text of the first result</figcaption></figure><p id="63b0" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">In order to extract the first heading we will</p><p id="2ce3" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv"><strong class="gq ki">Make a request (1), </strong>which we learned before. The response of the server will be stores in a variable called <em class="jp">r</em>. The HTML text of the response will be <strong class="gq ki">parsed (2) </strong>the into a Beautiful Soup object, we called it <em class="jp">soup</em> here. The parsing basically means that the whole website will be stored in a structure which makes it easy to search through. The BeautifulSoup object additionally gives us some new functions which makes our life easier such as removes the tags from the content. The<strong class="gq ki"> search for all &lt;h1&gt; tags in the response (3)</strong> is performed by calling the function <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">find_all() </a>function on the BeautifulSoup object <em class="jp">soup. </em>You can find out how find_all() works in detail by clicking on the link, but for now it is enough to know that it returns a <a href="https://www.w3schools.com/python/python_lists.asp" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">list </a>of all tags which match the search condition. The first element of the list is our heading. For better understanding, we will <strong class="gq ki">display the whole list (4) </strong>first and then we only print out the first element without the HTML tags by accessing its .text attribute.</p><p id="8975" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">The next step is to open the link behind the text and to extract the content. In Code 4 we can see how this is done. The new line here is line 8, we formulate a new HTTP get request as we have learned before, but this time we will use the link behind the heading in order to get the request. In Fig. 3 we can see that the link is a parent of the heading node, this means we can acces the link by accesing the parent of the heading. In line 8, we acces the first element of the list containing all headings by using <em class="jp">headings[0]</em>, then we acces its parent tag <em class="jp">headings[0].parent</em> and lastly we get what is written in between the <em class="jp">&lt;href&gt;</em> tag. <a href="https://www.w3schools.com/tags/att_a_href.asp" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow"><em class="jp">&lt;href&gt;</em></a> stands for hyper reference and is basically a link as you know it.</p><p id="0375" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">As before, we make a Beautiful Soup object from the response (line 9) and then we are going to find the text of the article which is placed in a couple of &lt;p&gt; tags, but how do we know, that the articles main text is located in &lt;p&gt; tags. We again checked this with the help of the developer tools of the browser. We first click on the link and then mark the text body with the inspection tool.</p><p id="eb28" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">Since the text is written in different paragraphs .find_all() returns a list of all &lt;p&gt; tags. In line 12 we remove the tags from the elements so that only the plain text is which we put back into a list. In line 13 we use the <a href="https://thepythonguru.com/python-builtin-functions/reduce/" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">reduce function,</a> which makes it easy for us to combine the different paragraphs into one single string.</p><figure class="in io ip iq ir is"><div class="jb r hm"><div class="jq r"><iframe src="https://medium.com/media/205828a2cddb9a48f4096271f93f4c9d" allowfullscreen="" frameborder="0" height="0" width="0" title="" class="dq t u iy ak" scrolling="auto"></iframe></div></div><figcaption class="ax fg ji jj jk do dm dn jl jm as cv">Code 4</figcaption></figure><h1 id="911a" class="hn ho ed at as hp ef hq eh hr hs ht hu hv hw hx hy">And what can we do now?</h1><p id="290c" class="go gp ed at gq b gr hz gt ia gv ib gx ic gz id hb dv">Probably you have some application with scraped data in mind. Either you just want to save it, because the website you are looking at in your work tends to go offline from time to time or you want to to analytics.</p><p id="11bb" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">One great case is to observe changes in the content of a newspaper over time <a href="https://www.youtube.com/watch?v=-YpwsdRKt8Q" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">(David Kriesel, CCC, 2017)</a>. If we have scraped a couple of articles from, e.g. a news site, we could search for certain key words or <a href="https://www.w3resource.com/python-exercises/string/python-data-type-string-exercise-12.php" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">count their appearance</a>. We could use also different Machine Learning tools to classify text, which I will cover in a different article.</p><h1 id="6845" class="hn ho ed at as hp ef hq eh hr hs ht hu hv hw hx hy">Further reading</h1><p id="8e31" class="go gp ed at gq b gr hz gt ia gv ib gx ic gz id hb dv"><a href="https://www.w3schools.com/tags/ref_httpmethods.asp" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">HTTP</a></p><p id="1a10" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv"><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">Beautiful Soup Documentation</a></p><p id="418a" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv"><a href="https://requests.kennethreitz.org/en/master/" class="da by ka kb kc kd" target="_blank" rel="noopener nofollow">requests Documentation</a></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="2774" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">EZ Web Scraping</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@chris.d.marker?source=post_page-----fd39ef1c1f50----------------------"><img alt="Chris Marker" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/1*zD9OL5i9q25HU_7ZGDSrUA.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@chris.d.marker?source=post_page-----fd39ef1c1f50----------------------">Chris Marker</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@chris.d.marker/ez-web-scraping-fd39ef1c1f50?source=post_page-----fd39ef1c1f50----------------------">Apr 18, 2018</a> <!-- -->·<!-- --> <!-- -->1<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/fd39ef1c1f50/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/fd39ef1c1f50/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40chris.d.marker%2Fez-web-scraping-fd39ef1c1f50&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="9f25" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Before I actually learned how to do it the concept of web scraping seemed like something extremely complicated and advanced. Using a few simple packages in python, however, it turns out its something that can be learned in an afternoon.</p><p id="93d5" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">To accomplish some basic web scraping tasks, all you really need is requests and BeautifulSoup.</p><pre class="ga gb gc gd ge gf gg gh"><span id="84ae" class="gi gj dc bk gk b eg gl gm r gn">import requests<br>from bs4 import BeautifulSoup</span></pre><p id="adab" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">From there, you can simply request a url (after checking that the website allows you to do so, of course) to get the html:</p><pre class="ga gb gc gd ge gf gg gh"><span id="cd1b" class="gi gj dc bk gk b eg gl gm r gn">url = "<a href="https://www.datatau.com/" class="at cg go gp gq gr" target="_blank" rel="noopener nofollow">https://www.datatau.com/</a>"<br>response = requests.get(url)</span></pre><p id="07ad" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">From there you can use BeautifulSoup to organize the content in a manageable way and search through it by element.</p><pre class="ga gb gc gd ge gf gg gh"><span id="5f58" class="gi gj dc bk gk b eg gl gm r gn">html = response.text<br>soup = BeautifulSoup(html, 'lxml')<br>all_td = soup.find_all('td', {'class':'title'})</span></pre><p id="0367" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">The above will return all of the ‘td’ elements with the class ‘title’ which you can then further refine.</p><p id="239c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Using the above basic skills and something like Chrome’s developer tools, you can learn to scrape almost any basic information off of a website.</p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu"><div class="n p"><div class="ac ae af ag ah cv aj ak"><div class="cw"><div class="n cx cy cz da"><div class="o n"><div><a rel="noopener" href="/@websiteparsing?source=post_page-----815c3838fe1a----------------------"><img alt="Web Parsing" class="r db dc dd" src="https://miro.medium.com/fit/c/96/96/0*vcqLwB8kAKqVxYbu.png" width="48" height="48"></a></div><div class="de ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r df q"><div class="dg n o dh"><span class="bj di dj bl dk dl dm dn do dp df"><a class="at au av aw ax ay az ba bb bc dq bf bg bh bi" rel="noopener" href="/@websiteparsing?source=post_page-----815c3838fe1a----------------------">Web Parsing</a></span><div class="dr r ar h"><button class="ds df q by dt du dv dw bc bh dx dy dz ea eb ec cb bj b bk ed ee bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj di dj bl dk dl dm dn do dp bo"><div><a class="at au av aw ax ay az ba bb bc dq bf bg bh bi" rel="noopener" href="/@websiteparsing/how-to-improve-your-business-using-web-data-extraction-815c3838fe1a?source=post_page-----815c3838fe1a----------------------">Nov 16, 2015</a> <!-- -->·<!-- --> <!-- -->3<!-- --> min read</div></span></span></div></div><div class="n ef eg eh ei ej ek el em ab"><div class="n o"><div class="en r ar"><a href="//medium.com/p/815c3838fe1a/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="en r ar"><a href="//medium.com/p/815c3838fe1a/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="eo r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40websiteparsing%2Fhow-to-improve-your-business-using-web-data-extraction-815c3838fe1a&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></section><section class="ep eq er es et"><div class="n p"><div class="ac ae af ag ah cv aj ak"><h2 id="6ea9" class="eu ev df bk bj ew ex ey ez fa fb fc fd fe ff fg fh">How to Improve Your Business Using Web Data Extraction</h2><p id="0647" class="fi fj df bk fk b fl fm fn fo fp fq fr fs ft fu fv ep">If you own a business, you need to monitor your competitors’ move so as to remain ahead of the game. However, you need to do a market research so as to gather useful information that will help you determine your position in the online business. The easiest and convenient way to gather data is through <a href="http://www.web-parsing.com/Web-Scraping-Services.php" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow"><strong class="fk ga">web data extraction</strong></a>. This can be done manually or by using data extraction software. Most businesses employ manual methods by browsing the web in order to gather useful information. While this method is reliable, it is time consuming and expensive. For effective results it is advisable to use automated data mining software which is faster and cheaper.</p><figure class="gc gd ge gf gg gh cl cm paragraph-image"><div class="cl cm gb"><div class="gn r go gp"><div class="gq r"><div class="gi gj cp t u gk ak dk gl gm"><img class="cp t u gk ak gr gs gt" src="https://miro.medium.com/max/60/1*UTy3AdgMBVTlWO237XIQbA.png?q=20" width="636" height="278" role="presentation"></div><img class="gi gj cp t u gk ak gu" width="636" height="278" role="presentation"><noscript><img class="cp t u gk ak" src="https://miro.medium.com/max/1272/1*UTy3AdgMBVTlWO237XIQbA.png" width="636" height="278" role="presentation"></noscript></div></div></div></figure><p id="08d6" class="fi fj df bk fk b fl gv fn gw fp gx fr gy ft gz fv ep">Today, <a href="http://www.web-parsing.com" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow"><strong class="fk ga">data mining companies</strong></a> have developed web harvesting software which you could buy and install in your computers or you can outsource the services from a qualified company. However, outsourcing the services will help you cut on costs which come with the installation, maintenance and running of the software. All you need to do is specify the type of information you require and the web scraping company will do the searching. According to your specifications, you’ll get customized data scraping services where web crawler show up data that matches your specifications. The information collected is readable, easy to understand and transferable.</p><p id="86d6" class="fi fj df bk fk b fl gv fn gw fp gx fr gy ft gz fv ep">There are different reasons why you should seek web data extraction services. May be you want to monitor your performance in the online market compared to your competitors. This may be in terms of sales and marketing strategies. You need to do a competitor price monitoring to know the products your competitors have and their rates. This will help you set the right prices for your products or services so as to attract more customers. Setting higher prices than your competitors will scare away customers. Competitor price monitoring software will also help you keep track on price changes. However you can only achieve this by getting the right data harvesting specialists. You need to work with a company that will give you value for your money.</p><p id="7662" class="fi fj df bk fk b fl gv fn gw fp gx fr gy ft gz fv ep">Due to the fast growth in the eCommerce market, business trends keep on changing so you need to check what your competitors are doing in order to remain relevant in business. The web harvesting company should provide you with up to date information for the businesses you’re monitoring. Ensure you get the right data that you can use to formulate a good pricing strategy for your business. Clients will be prompted to order from businesses that offer the right prices for their products. As a business owner you need to ensure you remain ahead of your competition.</p><p id="cf33" class="fi fj df bk fk b fl gv fn gw fp gx fr gy ft gz fv ep">If you’re doing business online it’s mandatory to invest in <a href="http://www.web-parsing.com/Web-Scraping-Services.php" class="at cg fw fx fy fz" target="_blank" rel="noopener nofollow"><strong class="fk ga">web data extraction services</strong></a> so as to enhance business growth. Contact a reputable web data mining company and they’ll do the donkey work as you enjoy the benefits. They have a team of experts who will work closely with your company. They’ll assess your business needs and help you improve your presence in the online market.</p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="3fa4" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How can Competitive Business Intelligence (BI) escalate your Success?</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@3idatascraping?source=post_page-----e8ff70bc64e4----------------------"><img alt="3i Data Scraping" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/1*pTBf2MTUyiDWXDDY4GglGg.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@3idatascraping?source=post_page-----e8ff70bc64e4----------------------">3i Data Scraping</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@3idatascraping/how-can-competitive-business-intelligence-bi-escalate-your-success-e8ff70bc64e4?source=post_page-----e8ff70bc64e4----------------------">Nov 16, 2017</a> <!-- -->·<!-- --> <!-- -->3<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/e8ff70bc64e4/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/e8ff70bc64e4/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%403idatascraping%2Fhow-can-competitive-business-intelligence-bi-escalate-your-success-e8ff70bc64e4&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm fm"><div class="gc r fv gd"><div class="ge r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*U5ozk2TKyZkA2hq-LjAIlw.jpeg?q=20" width="730" height="448" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="730" height="448" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/1460/1*U5ozk2TKyZkA2hq-LjAIlw.jpeg" width="730" height="448" role="presentation"></noscript></div></div></div></div></figure><p id="715f" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You wish to ace it, but is it so easy to do so? You recognize your market; however, do they recognize you as well? You have a massive variety of items for your customers; however, are they of real worth to your clients or consumers? How to really defeat your competition at this game?</p><p id="5a3b" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">A universal answer to all these questions is <a href="http://www.3idatascraping.com/business-intelligence-web-scraping-services.php" class="at cg gx gy gz ha" target="_blank" rel="noopener nofollow"><strong class="gl hb">Competitive Business Intelligence obtained from web data scraping</strong></a>!!! If you typically aren’t leveraging big information to your benefit, then you are missing out on those instrumental data mining benefits that your competitors are already doing. You need to understand that there are rivals keeping a competitive eye on you.</p><h2 id="71bf" class="hc hd dc bk bj he hf hg hh hi hj hk hl hm hn ho hp">What can Big Information deliver to a business?</h2><p id="85d7" class="gj gk dc bk gl b gm hq go hr gq hs gs ht gu hu gw cu">For beginners, it could aid you to snoop your competitors. Considering that today’s economic situation has become a lot fiercer, businesses, as well as vendors have actually been aiming to ace the race. Currently, <a href="https://en.wikipedia.org/wiki/Big_data" class="at cg gx gy gz ha" target="_blank" rel="noopener nofollow">big data</a> simply makes it all that easier. With the right tools in place, you not just understand exactly what your rivals do daily, weekly, monthly, or yearly; but even discover exactly what they are doing now.</p><h2 id="9115" class="hc hd dc bk bj he hf hg hh hi hj hk hl hm hn ho hp">Big data analytics offers actual real-time understandings on Market Intelligence, which gives the capability to:</h2><ul class=""><li id="e3ea" class="gj gk dc bk gl b gm hq go hr gq hs gs ht gu hu gw hv hw hx">Determine successful prices that could help you get an edge over others</li><li id="757b" class="gj gk dc bk gl b gm hy go hz gq ia gs ib gu ic gw hv hw hx">Projection of future strategies by determining the influence of cost modifications</li><li id="8e1d" class="gj gk dc bk gl b gm hy go hz gq ia gs ib gu ic gw hv hw hx">Automate the prices to guarantee uniformity of costs while removing error-prone jobs</li><li id="c932" class="gj gk dc bk gl b gm hy go hz gq ia gs ib gu ic gw hv hw hx">Carry out product positioning while optimizing revenue possibilities</li><li id="e9bc" class="gj gk dc bk gl b gm hy go hz gq ia gs ib gu ic gw hv hw hx">Mimic real-time “what-if” circumstances for forecasting alternative method results</li><li id="22a6" class="gj gk dc bk gl b gm hy go hz gq ia gs ib gu ic gw hv hw hx">Localize rates based upon consumer needs as well as affordable habits</li></ul><p id="7b9d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Regardless of this, 75% sellers do not make use of real-time affordable analytics.</p><h2 id="96a2" class="hc hd dc bk bj he hf hg hh hi hj hk hl hm hn ho hp">What is more than likely to be exposed after evaluation of such details?</h2><ul class=""><li id="b6ff" class="gj gk dc bk gl b gm hq go hr gq hs gs ht gu hu gw hv hw hx"><strong class="gl hb">Action-Based Insights</strong>: Knowledge about the kind of activities that must be undertaken.</li><li id="e860" class="gj gk dc bk gl b gm hy go hz gq ia gs ib gu ic gw hv hw hx"><strong class="gl hb">Anticipative Insights</strong>: Circumstances that may take place.</li><li id="941d" class="gj gk dc bk gl b gm hy go hz gq ia gs ib gu ic gw hv hw hx"><strong class="gl hb">Analytical Insights</strong>: What occurred in the past, and why?</li><li id="c48c" class="gj gk dc bk gl b gm hy go hz gq ia gs ib gu ic gw hv hw hx"><strong class="gl hb">Detailed Insights</strong>: A ‘what’s taking place currently’ perspective based on real-time information.</li></ul><h2 id="ae9c" class="hc hd dc bk bj he hf hg hh hi hj hk hl hm hn ho hp">How it’s done?</h2><p id="c79b" class="gj gk dc bk gl b gm hq go hr gq hs gs ht gu hu gw cu">There is a remarkable quantity of understandings that big information could discover, yet to be able to utilize it to your advantage needs a framework. Below are a couple of points to watch out for:</p><ul class=""><li id="2107" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw hv hw hx"><strong class="gl hb">Automation Possibilities need to be scrutinized</strong></li></ul><p id="cf6b" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Constantly inspect that the procedures you are preparing to use could be automated or not. In spite of the fact that today everything could be automated, it’s best to be assured. You do not wish to end up doing such a stressful job manually.</p><ul class=""><li id="8a74" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw hv hw hx"><strong class="gl hb">Resources need to be Validated</strong></li></ul><p id="c9c0" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Be specific that the details you are placing for evaluation is exact and also from a qualified resource. Guarantee that you feed the appropriate kind of information for the most precise outcomes.</p><ul class=""><li id="154d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw hv hw hx"><strong class="gl hb">Insights need to be Immediate</strong></li></ul><p id="7a77" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Produce applications that make it simpler for team members to draw out real-time details while placing the same for evaluation. When dealing with challenging clients, this could be extremely useful.</p><ul class=""><li id="b8af" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw hv hw hx"><strong class="gl hb">Collect Responses for both Performing &amp; Non-Performing Locations</strong></li></ul><p id="626b" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">The most effective feature of competitive business intelligence is the location-specific understandings. These kinds of understandings could be several of one of the most important little bits of information you will certainly stumble upon.</p><ul class=""><li id="4f00" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw hv hw hx"><strong class="gl hb">Identify Violations taking place within your Approach</strong></li></ul><p id="832b" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Make your approaches smarter. Your approach can not be as easy as decreasing the cost each time your rival does. Your method needs to make it possible for constant growth as well as most valuable activities based on your sales technique.</p><ul class=""><li id="c9af" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw hv hw hx"><strong class="gl hb">Develop plans leading to advance preparation for Longer Terms</strong></li></ul><p id="c80d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Constantly think of long-term preparation for cost reduction by matching your rival’s activities. You require taking into consideration whether decreasing the cost of a provided product diminish inventory too rapidly or not.</p><p id="aaa7" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Want Competitive Business Intelligence (BI) solutions at cost-effective or cheaper rates to leverage your company growth? See us at <a href="http://www.3idatascraping.com/" class="at cg gx gy gz ha" target="_blank" rel="noopener nofollow">3i Data Scraping</a>, your ultimate Big Data outsourcing partners.</p></div></div></section><hr class="id ef ie if ig ih ii ij ik il im"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="7de3" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><em class="in">Originally published at </em><a href="http://www.3idatascraping.com/how-can-competitive-business-intelligence-bi-escalate-your-success.php" class="at cg gx gy gz ha" target="_blank" rel="noopener nofollow"><em class="in">www.3idatascraping.com</em></a><em class="in"> on November 16, 2017.</em></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="5409" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Web scraping with Python(using BeautifulSoup)</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@kenechiojukwu?source=post_page-----bf677172d6ba----------------------"><img alt="Kenechi Ojukwu" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/1*GPU32yt1uCsPXcBSTLfESw.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@kenechiojukwu?source=post_page-----bf677172d6ba----------------------">Kenechi Ojukwu</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@kenechiojukwu/web-scraping-using-python-bf677172d6ba?source=post_page-----bf677172d6ba----------------------">Mar 24, 2019</a> <!-- -->·<!-- --> <!-- -->5<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/bf677172d6ba/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/bf677172d6ba/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40kenechiojukwu%2Fweb-scraping-using-python-bf677172d6ba&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fn fo fp fq fr fs cl cm paragraph-image"><div class="ft fu fv fw ak"><div class="cl cm fm"><div class="gc r fv gd"><div class="ge r"><div class="fx fy cp t u fz ak eh ga gb"><img class="cp t u fz ak gf gg gh" src="https://miro.medium.com/max/60/1*a4xWpmTJ14YsMVDg0Wwlbw.jpeg?q=20" width="720" height="566" role="presentation"></div><img class="fx fy cp t u fz ak gi" width="720" height="566" role="presentation"><noscript><img class="cp t u fz ak" src="https://miro.medium.com/max/1440/1*a4xWpmTJ14YsMVDg0Wwlbw.jpeg" width="720" height="566" role="presentation"></noscript></div></div></div></div></figure><p id="c5a4" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">As usual the first set of questions always go like this, what is web scraping? What is the usefulness? And how do I do it? Now, to answer the first two questions with the simplest of words, web scraping is simply the collection of specific data or information from a web site or a simple web page, to which this information or data could be used for analysis or whatever the web scraper needs such information for. Several programming languages can be used for web scraping, but as stated above we would be using the python programming language to scrape a web site. How do I do it? lets get right to it with a simple example. First, it would be a good thing to note that one of the languages used in building a website is the Hyper Text Mark-up Language(HTML). HTML contains large amount of data in text form. To scrape data from a web site, we would use the beautifulsoup4 from the bs4 python library and the lxml parser( there are other types of parsers but we would be using ‘lxml’ for this example). These tools are way more preferable, very helpful and easy to use when it comes to web scraping.</p><p id="ee7d" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Getting Started:</strong></p><p id="f761" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You would need to create a folder, after which you create a virtual environment in that folder, then install these tools and libraries in the virtual environment. All these steps would be done in the command prompt using pip.</p><pre class="fn fo fp fq fr gy gz ha"><span id="9890" class="hb hc dc bk hd b eg he hf r hg">#on the command prompt<br>1) mkdir work<br>2) cd work <br>3) virtualenv work_flow_env <br>4) work_flow_env\Scripts\activate<br>5) pip install beautifulsoup4<br>6) pip install lxml<br>7) pip install html5lib #optional<br>8) pip install requests</span><span id="c3fd" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg"># explanation of code lines:</span><span id="89b1" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">line 1 creates a folder named work<br>line 2 opens the folder<br>line 3 creates a virtual environment named work_flow_env in that folder<br>line 4 activates the virtual environment(this code is for a windows os)<br>line 5,6,7 and 8 installs the tools and libraries needed for web scraping.</span></pre><p id="0ab1" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">I will assume that most of the readers have an idea even if it’s a little knowledge on HTML, but if you have none, you could always skim through a good free source website, in which I will recommend “w3schools.com”.</p><p id="5f6c" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now if you are using sublime text, all you have to do is drag your folder named “work’’ into the sublime text application and you are ready to code. If you have a well downloaded anaconda application, jupyter notebook has all these installed, so you would not have to go through the ‘Getting started’ phase.</p><p id="ef37" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Our task is really simple, we are to get the name of movies from ‘<a href="http://toxicwap.com/New_Movies/" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow">http://toxicwap.com/New_Movies/</a>’ and their links. In your already set compiler, you import the libraries.</p><p id="8bce" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Importing Libraries:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="970c" class="hb hc dc bk hd b eg he hf r hg">from bs4 import BeautifulSoup<br>import requests<br></span></pre><p id="5d38" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Getting the raw data:</strong></p><p id="26ed" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">After importing the libraries, you get the information in text format from the web page using requests.get().text</p><pre class="fn fo fp fq fr gy gz ha"><span id="209b" class="hb hc dc bk hd b eg he hf r hg">source = requests.get('<a href="http://toxicwap.com/New_Movies/').text" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow">http://toxicwap.com/New_Movies/').text</a> </span></pre><p id="d459" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Parsing:</strong></p><p id="2132" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now, you have gotten the information needed, so you parse through the text using lxml, you make it clean and readable using prettify().</p><pre class="fn fo fp fq fr gy gz ha"><span id="1e7e" class="hb hc dc bk hd b eg he hf r hg">soup = BeautifulSoup(source, 'lxml')<br>print(soup.prettify())</span></pre><p id="2c73" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Inspecting the web page:</strong></p><p id="d321" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Go back to the web page and inspect it( you right click and and select inspect), you then navigate the web page from the source code seen, you navigate to the point you are able to highlight the part you want to scrape. When you have gotten all you need, you go back to your code then look through your “prettified” text. When going through your cleaned up data(prettified text) you would see the code you highlighted from the web page, depending on the site you are scraping you may have to dig a lot deeper before getting to what you want.</p><p id="9183" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Digging(navigating) through the text data:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="e272" class="hb hc dc bk hd b eg he hf r hg">div = soup.find('div', attrs={'data-role':'content'})<br>ul = div.find('ul')<br>li = ul.find('li')<br>print(li)</span></pre><p id="88f4" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">To explain the code. First, if you are doing exactly what I am doing, when you inspect the web page you would notice the tags are mostly “div”, now line 1 selects the particular div that holds the content you want to scrape. Line 2 digs deeper into the div to the ul(unordered list) and line 3 digs into the ul to the li(list). Well, We all know what line 4 does( it displays the list).</p><p id="e4c1" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Note: some sources online use ‘class_=()’ when trying to get to a particular div, but you would notice that in this particular case the prettified text did not display the class of the div, hence resulting to the use of ‘attrs ={}’.</p><pre class="fn fo fp fq fr gy gz ha"><span id="ac83" class="hb hc dc bk hd b eg he hf r hg">title = li.a.text<br>print(title)<br>link = li.a<br>print(link['href'])</span></pre><p id="ac17" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Now, the first line says put in the variable named title the text which can be found in the ‘a’ tag(&lt;a&gt;: link tag in HTML), which is also found in the ‘li’ tag. It is literally just digging from ‘li’ into ‘a’ to the ‘text’. From what I have said, you should be able to interpret the third line. Basically, you are already done, but this written code will get you just the first title and the first link, to get all the titles and links you use a for loop.</p><p id="c1d2" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Displaying the whole output:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="ec8f" class="hb hc dc bk hd b eg he hf r hg">for li in ul.find_all('li'):<br>    title = li.a.text<br>    print(title)<br>    <br>    link =li.a<br>    print(link['href'])</span></pre><p id="4df0" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">Notice how .find() changed to .find_all()? That’s what you do when you want to get all the data needed. It is good practice to use .find() first when trying to navigate, then when the code format is gotten you use the .find_all() to get the data remaining. So now the whole code should look like this.</p><p id="7fa8" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Complete code:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="6ee6" class="hb hc dc bk hd b eg he hf r hg">from bs4 import BeautifulSoup<br>import requests</span><span id="8d5b" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">source = requests.get('<a href="http://toxicwap.com/New_Movies/').text" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow">http://toxicwap.com/New_Movies/').text</a><br>soup = BeautifulSoup(source, 'lxml')</span><span id="28cb" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">div = soup.find('div', attrs={'data-role':'content'})<br>ul = div.find('ul')<br>li = ul.find('li')</span><span id="4bdb" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">for li in ul.find_all('li'):<br>    title = li.a.text<br>    print(title)<br>    <br>    link = li.a<br>    print(link['href'])<br>    print()<br>    <br> </span></pre><p id="a6be" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">You are done, but if you want to save the scrapped data into a text file or csv file, you can. I’ll be saving this into a csv file.</p><p id="bd7b" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu"><strong class="gl gx">Saving in a format:</strong></p><pre class="fn fo fp fq fr gy gz ha"><span id="f4a3" class="hb hc dc bk hd b eg he hf r hg">from bs4 import BeautifulSoup<br>import requests<br>import csv</span><span id="9fe5" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">source = requests.get('<a href="http://toxicwap.com/New_Movies/').text" class="at cg hm hn ho hp" target="_blank" rel="noopener nofollow">http://toxicwap.com/New_Movies/').text</a><br>soup = BeautifulSoup(source, 'lxml')</span><span id="971e" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">div = soup.find('div', attrs={'data-role':'content'})<br>ul = div.find('ul')<br>li = ul.find('li')</span><span id="0c7c" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">file = open('s_data.csv', 'w')<br>file_writer = csv.writer(file)<br>file_writer.writerow(['Titles','Links'])</span><span id="d0a7" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">for li in ul.find_all('li'):<br>    title = li.a.text<br>    print(title)<br>    <br>    link = li.a<br>    print(link['href'])<br>    print()<br>    <br>    file_writer.writerow([title, link ])</span><span id="1f55" class="hb hc dc bk hd b eg hh hi hj hk hl hf r hg">file.close()</span></pre><p id="0274" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">I would like to add that some websites make it really hard to scrape their page and for some it is illegal to scrape their page.</p><p id="885c" class="gj gk dc bk gl b gm gn go gp gq gr gs gt gu gv gw cu">That is it. It is all done. I guess I could say you just learnt how to scrape a website.</p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="9ab9" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How to Run JavaScript in Python | Web Scraping | Web Testing</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@mahmudahsan?source=post_page-----16bd04894360----------------------"><div class="dz ea eb"><div class="bs n ec o p cp ed ee ef eg eh ct"><svg width="57" height="57" viewbox="0 0 57 57"><path fill-rule="evenodd" clip-rule="evenodd" d="M28.5 1.2A27.45 27.45 0 0 0 4.06 15.82L3 15.27A28.65 28.65 0 0 1 28.5 0C39.64 0 49.29 6.2 54 15.27l-1.06.55A27.45 27.45 0 0 0 28.5 1.2zM4.06 41.18A27.45 27.45 0 0 0 28.5 55.8a27.45 27.45 0 0 0 24.44-14.62l1.06.55A28.65 28.65 0 0 1 28.5 57 28.65 28.65 0 0 1 3 41.73l1.06-.55z"></path></svg></div><img alt="Mahmud Ahsan" class="r ei eb ea" src="https://miro.medium.com/fit/c/96/96/2*NweAj3YbA04w9moXyaubdQ.jpeg" width="48" height="48"></div></a></div><div class="ej ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ek n o el"><span class="bj em en bl eo ep eq er es et dc"><a class="at au av aw ax ay az ba bb bc eu bf bg bh bi" rel="noopener" href="/@mahmudahsan?source=post_page-----16bd04894360----------------------">Mahmud Ahsan</a></span><div class="ev r ar h"><button class="ew dc q by ex ey ez fa bc bh fb fc fd fe ff fg cb bj b bk fh fi bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj em en bl eo ep eq er es et bo"><div><a class="at au av aw ax ay az ba bb bc eu bf bg bh bi" rel="noopener" href="/@mahmudahsan/how-to-run-javascript-in-python-web-scraping-web-testing-16bd04894360?source=post_page-----16bd04894360----------------------">May 2, 2018</a> <!-- -->·<!-- --> <!-- -->5<!-- --> min read</div></span></span></div></div><div class="n fj fk fl fm fn fo fp fq ab"><div class="n o"><div class="fr r ar"><a href="//medium.com/p/16bd04894360/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fr r ar"><a href="//medium.com/p/16bd04894360/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fs r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40mahmudahsan%2Fhow-to-run-javascript-in-python-web-scraping-web-testing-16bd04894360&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div><div class="ft"><div class="n p"><div class="fu fv fw fx fy fz ag ga ah gb aj ak"><figure class="gd ge gf gg gh ft gi gj paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm gc"><div class="gs r dz gt"><div class="gu r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*R7Fvc3uJkHaaWvvgJVlx-Q.png?q=20" width="1920" height="1080" role="presentation"></div><img class="gn go cp t u gp ak gy" width="1920" height="1080" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/3840/1*R7Fvc3uJkHaaWvvgJVlx-Q.png" width="1920" height="1080" role="presentation"></noscript></div></div></div></div></figure></div></div></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="3155" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">When we develop web application sometimes <strong class="hb hn">we need to test the UX</strong>. Most of the time we do it manually. For example, after a form submission what happen, which a person check it manually. In future, if another coder wrongly modified the form code it may creates a bug which may be skipped by manual tester.</p><p id="0997" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">Sometimes <strong class="hb hn">we want to scrap some webpage’s information</strong> but which is fully loaded by JavaScript framework. In normal scraping techniques it’s not possible to scrap data as the data is loaded lazily.</p><p id="729b" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">We <strong class="hb hn">can solve both webpage testing and dynamic web page scraping</strong> by running <strong class="hb hn">JavaScript code using </strong><a href="https://www.seleniumhq.org/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="hb hn">Selenium</strong></a><strong class="hb hn"> library. </strong>Which is called automate the web browser.</p><p id="7f32" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">In this post I will discuss about:</p><ol class=""><li id="15f2" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm hs ht hu">Installing <a href="http://selenium-python.readthedocs.io/getting-started.html" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">Selenium library in Mac and Windows</a></li><li id="e6be" class="gz ha dc bk hb b hc hv he hw hg hx hi hy hk hz hm hs ht hu">Install Headless <a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">Google Chrome driver</a> in Mac and Windows</li><li id="b467" class="gz ha dc bk hb b hc hv he hw hg hx hi hy hk hz hm hs ht hu">A Python script to run <a href="https://github.com/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">github.com</a> site in headless browser</li><li id="54e9" class="gz ha dc bk hb b hc hv he hw hg hx hi hy hk hz hm hs ht hu">Using Python selenium library to run JavaScript code</li><li id="448f" class="gz ha dc bk hb b hc hv he hw hg hx hi hy hk hz hm hs ht hu">Scraping <a href="http://github.com/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="hb hn">github.com</strong></a><strong class="hb hn"> webpage data</strong> after it loaded</li><li id="d8f3" class="gz ha dc bk hb b hc hv he hw hg hx hi hy hk hz hm hs ht hu"><strong class="hb hn">Filling</strong> the <a href="http://github.com/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="hb hn">github.com</strong></a><strong class="hb hn"> search form</strong> and <strong class="hb hn">submit by code</strong></li><li id="3080" class="gz ha dc bk hb b hc hv he hw hg hx hi hy hk hz hm hs ht hu">Finally <strong class="hb hn">taking the invisible browsers screenshot programmatically</strong></li></ol><p id="1f59" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">We have a Bangla narrated video tutorial for this solution:</p><figure class="gd ge gf gg gh ft"><div class="gs r dz"><div class="gu r"><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F9ff6at9ONWA&amp;src_secure=1&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D9ff6at9ONWA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F9ff6at9ONWA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" allowfullscreen="" frameborder="0" height="720" width="1280" title="Execute JavaScript in Python with Selenium Webdriver | Web Automation Bangla Tutorial ## 10" class="cp t u gp ak" scrolling="auto"></iframe></div></div></figure><h1 id="9d4d" class="ia ib dc bk bj ic de id dg ie if ig ih ii ij ik il">Setup</h1><p id="8637" class="gz ha dc bk hb b hc im he in hg io hi ip hk iq hm cu">We need <a href="https://thinkdiff.net/python/python-official-pipenv-packaging-tool-for-virtualenv-and-pip-in-mac-and-windows/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">pipenv</a> to install Selenium library for this project. <strong class="hb hn">If you don’t know how to install Pipenv then please </strong><a href="https://thinkdiff.net/python/python-official-pipenv-packaging-tool-for-virtualenv-and-pip-in-mac-and-windows/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="hb hn">check my other tutorial</strong></a><strong class="hb hn">.</strong></p><h1 id="7979" class="ia ib dc bk bj ic de id dg ie if ig ih ii ij ik il">1. Installing Selenium library in Mac and Windows</h1><p id="9ccf" class="gz ha dc bk hb b hc im he in hg io hi ip hk iq hm cu">First in terminal go to a directory. In my case I am in this directory:</p><figure class="gd ge gf gg gh ft cl cm paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm ir"><div class="gs r dz gt"><div class="is r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*-Rt2mw8xG2brxylbi92J4A.png?q=20" width="926" height="304" role="presentation"></div><img class="gn go cp t u gp ak gy" width="926" height="304" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/1852/1*-Rt2mw8xG2brxylbi92J4A.png" width="926" height="304" role="presentation"></noscript></div></div></div></div></figure><p id="2a14" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu"><code class="gt it iu iv iw b">/Users/mahmud/Desktop/demo/sel1</code></p><p id="c916" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">Now open the Terminal in Mac or PowerShell in Windows and run the following commands:</p><p id="0d30" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu"><code class="gt it iu iv iw b">pipenv install selenium</code></p><figure class="gd ge gf gg gh ft cl cm paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm ix"><div class="gs r dz gt"><div class="iy r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*FTvrCwBNDs54-FfpDa_A1A.png?q=20" width="1592" height="330" role="presentation"></div><img class="gn go cp t u gp ak gy" width="1592" height="330" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/3184/1*FTvrCwBNDs54-FfpDa_A1A.png" width="1592" height="330" role="presentation"></noscript></div></div></div></div></figure><p id="aec9" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">It will create 2 files, Pipfile and Pipfile.lock</p><p id="da40" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">Now run the following command to activate sel1 project’s virtualenv.</p><pre class="gd ge gf gg gh iz ja jb"><span id="f923" class="jc ib dc bk iw b en jd je r jf">pipenv shell</span></pre><h1 id="5b78" class="ia ib dc bk bj ic de id dg ie if ig ih ii ij ik il">2. Install Headless Google Chrome driver</h1><p id="2766" class="gz ha dc bk hb b hc im he in hg io hi ip hk iq hm cu">To automate web browser, which is done in invisible way, we need to install Google Chrome driver. Please visit <a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">the following website</a> and download the latest released driver for your mac or windows or linux operating system.</p><figure class="gd ge gf gg gh ft cl cm paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm jg"><div class="gs r dz gt"><div class="jh r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*CAXyYYp-sqJlf46c_i0j8g.png?q=20" width="930" height="530" role="presentation"></div><img class="gn go cp t u gp ak gy" width="930" height="530" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/1860/1*CAXyYYp-sqJlf46c_i0j8g.png" width="930" height="530" role="presentation"></noscript></div></div></div></div></figure><p id="41d9" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">Now <strong class="hb hn">unzip the downloaded file</strong>, and c<strong class="hb hn">opy the chromedriver.exe file</strong> in our project directory <strong class="hb hn">sel1</strong>.</p><h1 id="aaec" class="ia ib dc bk bj ic de id dg ie if ig ih ii ij ik il">3. Run the python script</h1><p id="f5a5" class="gz ha dc bk hb b hc im he in hg io hi ip hk iq hm cu">Now in the sel1 directory, create a python script named <strong class="hb hn">chapter9.py</strong> and paste the following codes. <a href="https://git.io/vpCuV" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">Github Source</a></p><pre class="gd ge gf gg gh iz ja jb"><span id="e1b6" class="jc ib dc bk iw b en jd je r jf"><em class="ji"># author: Mahmud Ahsan</em><br><em class="ji"># github: https://github.com/mahmudahsan</em><br><em class="ji"># blog: http://thinkdiff.net</em><br><em class="ji"># Web: http://pythonbangla.com</em><br><em class="ji"># youtube: https://www.youtube.com/c/banglaprogramming</em><br><em class="ji"># License: MIT License</em><br><em class="ji"># https://github.com/mahmudahsan/thinkdiff/blob/master/LICENSE </em><br><br><em class="ji"># --------------------------</em><br><em class="ji">#      Execute JavaScript</em><br><em class="ji"># --------------------------</em><br><br>from selenium import webdriver<br>from selenium.webdriver.chrome.options import Options<br>from selenium.webdriver.common.keys import Keys<br>import os<br><br>def main():<br>    chrome_options = Options()<br>    chrome_options.add_argument("--headless")<br>    chrome_options.add_argument("--window-size=1024x1400")<br><br>    <em class="ji"># download Chrome Webdriver  </em><br>    <em class="ji"># https://sites.google.com/a/chromium.org/chromedriver/download</em><br>    <em class="ji"># put driver executable file in the script directory</em><br>    chrome_driver = os.path.join(os.getcwd(), "chromedriver")<br><br>    driver = webdriver.Chrome(chrome_options=chrome_options, executable_path=chrome_driver)<br><br>    driver.get("https://github.com")<br>    assert "GitHub".lower() in driver.title.lower()<br>    <br>    <em class="ji"># scrap info</em><br>    h1_elem = driver.find_element_by_tag_name("h1")<br>    print(h1_elem.text)<br><br>    <em class="ji"># fill and submit form</em><br>    elem = driver.find_element_by_name("q")<br>    elem.clear()<br>    elem.send_keys("python")<br>    elem.send_keys(Keys.RETURN)<br><br>    <em class="ji"># screenshot capture</em><br>    driver.get_screenshot_as_file("python-github.png")<br>    driver.close()<br><br>if __name__ == '__main__' : main()</span></pre><h1 id="5994" class="ia ib dc bk bj ic de id dg ie if ig ih ii ij ik il">4. Run the program</h1><p id="835a" class="gz ha dc bk hb b hc im he in hg io hi ip hk iq hm cu">In macOS terminal run the following command:</p><pre class="gd ge gf gg gh iz ja jb"><span id="1cef" class="jc ib dc bk iw b en jd je r jf">python3 chapter9.py</span></pre><p id="ffef" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">In windows 10 power shell run the following command. Just use Python instead of Python3</p><pre class="gd ge gf gg gh iz ja jb"><span id="5ace" class="jc ib dc bk iw b en jd je r jf">python chapter9.py</span></pre><figure class="gd ge gf gg gh ft cl cm paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm jj"><div class="gs r dz gt"><div class="jk r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*nRz-blrMVBWgWBfIM2P08w.png?q=20" width="1250" height="288" role="presentation"></div><img class="gn go cp t u gp ak gy" width="1250" height="288" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/2500/1*nRz-blrMVBWgWBfIM2P08w.png" width="1250" height="288" role="presentation"></noscript></div></div></div></div></figure><p id="9f84" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">After successfully run the program, you will get a png file named python-github.png.</p><figure class="gd ge gf gg gh ft cl cm paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm jl"><div class="gs r dz gt"><div class="jm r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*2_GofjsRy38xfD4SUYQnAw.png?q=20" width="1991" height="988" role="presentation"></div><img class="gn go cp t u gp ak gy" width="1991" height="988" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/3982/1*2_GofjsRy38xfD4SUYQnAw.png" width="1991" height="988" role="presentation"></noscript></div></div></div></div></figure><h1 id="a03a" class="ia ib dc bk bj ic de id dg ie if ig ih ii ij ik il">5. Python script analysis</h1><p id="da8e" class="gz ha dc bk hb b hc im he in hg io hi ip hk iq hm cu">It is a very simple script. At first we import python selenium libraries in our script. Then we create a webdriver object based on some options we provided also we mentioned the google chrome browser driver location via <strong class="hb hn">chrome_driver</strong> object.</p><p id="8872" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">Then by <strong class="hb hn">driver.get()</strong> method we load <a href="http://github.com/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">github.com</a> website.</p><p id="6403" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">In the <strong class="hb hn">#scrap info section </strong>we <strong class="hb hn">scrap HTML h1 tag data</strong> and <strong class="hb hn">print it in the console</strong>. <strong class="hb hn">This is how we scrap</strong> via selenium and headless web driver.</p><p id="b14f" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">In the #scrap info section we scrap HTML h1 tag data and print it in the console. This is how we scrap via selenium and headless web driver.</p><pre class="gd ge gf gg gh iz ja jb"><span id="6006" class="jc ib dc bk iw b en jd je r jf"><em class="ji"># scrap info</em><br> h1_elem = driver.find_element_by_tag_name("h1")<br> print(h1_elem.text)</span></pre><p id="16d6" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">We see “Built for developers” is printed in the terminal.</p><figure class="gd ge gf gg gh ft cl cm paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm jj"><div class="gs r dz gt"><div class="jk r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*nRz-blrMVBWgWBfIM2P08w.png?q=20" width="1250" height="288" role="presentation"></div><img class="gn go cp t u gp ak gy" width="1250" height="288" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/2500/1*nRz-blrMVBWgWBfIM2P08w.png" width="1250" height="288" role="presentation"></noscript></div></div></div></div></figure><p id="ae0d" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">Finally we fill and submit the form by code. To select the search form in the webpage by javascript, we use Google Chrome Browser’s Inspect code option to check the form element name.</p><figure class="gd ge gf gg gh ft cl cm paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm jn"><div class="gs r dz gt"><div class="jo r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*AOHcPNi_-CQSH8fVV_PUEg.png?q=20" width="2092" height="1472" role="presentation"></div><img class="gn go cp t u gp ak gy" width="2092" height="1472" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/4184/1*AOHcPNi_-CQSH8fVV_PUEg.png" width="2092" height="1472" role="presentation"></noscript></div></div></div></div></figure><p id="4b13" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">This is the code that automates search and submit the form:</p><pre class="gd ge gf gg gh iz ja jb"><span id="0953" class="jc ib dc bk iw b en jd je r jf"><em class="ji"># fill and submit form</em><br>elem = driver.find_element_by_name("q")<br>elem.clear()<br>elem.send_keys("python")<br>elem.send_keys(Keys.RETURN)</span></pre><p id="fd64" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">To take screenshot of the final page we write the following code:</p><pre class="gd ge gf gg gh iz ja jb"><span id="aed1" class="jc ib dc bk iw b en jd je r jf"># screenshot capture<br>  driver.get_screenshot_as_file("python-github.png")<br>  driver.close()</span></pre><figure class="gd ge gf gg gh ft cl cm paragraph-image"><div class="gk gl dz gm ak"><div class="cl cm jp"><div class="gs r dz gt"><div class="jq r"><div class="gn go cp t u gp ak eo gq gr"><img class="cp t u gp ak gv gw gx" src="https://miro.medium.com/max/60/1*0ZVoK4jdithY6obzXU4c8A.png?q=20" width="1298" height="228" role="presentation"></div><img class="gn go cp t u gp ak gy" width="1298" height="228" role="presentation"><noscript><img class="cp t u gp ak" src="https://miro.medium.com/max/2596/1*0ZVoK4jdithY6obzXU4c8A.png" width="1298" height="228" role="presentation"></noscript></div></div></div></div></figure><p id="7449" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">This is one of the way we can use selenium library in Python to execute JavaScript to test webpage or scrap dynamic or static website information.</p><p id="08d4" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu"><strong class="hb hn">Reference:</strong></p><ol class=""><li id="25ad" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm hs ht hu">Selenium: <a href="https://www.seleniumhq.org/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">https://www.seleniumhq.org</a></li></ol><p id="8be7" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">2. Selenium Python Docs: <a href="http://selenium-python.readthedocs.io/getting-started.html" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">http://selenium-python.readthedocs.io/getting-started.html</a></p><p id="a8bd" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">3. Google Chrome Web Driver: <a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">https://sites.google.com/a/chromium.org/chromedriver/downloads</a></p><p id="bf42" class="gz ha dc bk hb b hc hd he hf hg hh hi hj hk hl hm cu">Source:<a href="http://thinkdiff.net/python/how-to-run-javascript-in-python-web-scraping-testing/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"> Thinkdiff.net</a></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="7b29" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Scraping Data from Website to Excel</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@skeer834?source=post_page-----ee9cb6fe5fb----------------------"><img alt="Alen Cooper" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/2*HHO9R-4kQGPTFsqqrHwFIw.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@skeer834?source=post_page-----ee9cb6fe5fb----------------------">Alen Cooper</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@skeer834/scraping-data-from-website-to-excel-ee9cb6fe5fb?source=post_page-----ee9cb6fe5fb----------------------">Apr 22, 2019</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/ee9cb6fe5fb/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/ee9cb6fe5fb/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40skeer834%2Fscraping-data-from-website-to-excel-ee9cb6fe5fb&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><figure class="fm fn fo fp fq fr cl cm paragraph-image"><div class="fs ft fu fv ak"><div class="cl cm cz"><div class="gb r fu gc"><div class="gd r"><div class="fw fx cp t u fy ak eh fz ga"><img class="cp t u fy ak ge gf gg" src="https://miro.medium.com/max/60/1*bF0NxOqVHkNXSDqpcf7jUg.png?q=20" width="680" height="456" role="presentation"></div><img class="fw fx cp t u fy ak gh" width="680" height="456" role="presentation"><noscript><img class="cp t u fy ak" src="https://miro.medium.com/max/1360/1*bF0NxOqVHkNXSDqpcf7jUg.png" width="680" height="456" role="presentation"></noscript></div></div></div></div></figure><p id="571a" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">You probably know how to use basic functions in Excel. It’s easy to do things like sorting, applying filters, making charts, and outlining data with Excel. You even can perform advanced data analysis using pivot and regression models. It becomes an easy job when the live data turns into a structured format. The problem is, how can we extract scalable data and put it into Excel? This can be tedious if you doing it manually by typing, searching, copying and pasting repetitively. Instead, you can achieve automated data scraping from websites to excel.</p><p id="1ad4" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">In this article, I will introduce several ways to save your time and energy to scrape web data into Excel.</p><p id="a8c8" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu"><strong class="gk gw">Disclaimer</strong>: There many other ways to scrape from websites using programming languages like PHP, Python, Perl, Ruby and etc. Here we just talk about how to scrape data from websites into excel for non-coders.</p><h1 id="0bf6" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi"><strong class="az">Getting web data using Excel Web Queries</strong></h1><p id="5c1d" class="gi gj dc bk gk b gl hj gn hk gp hl gr hm gt hn gv cu">Except for transforming data from a web page manually by copying and pasting, Excel Web Queries is used to quickly retrieve data from a standard web page into an Excel worksheet. It can automatically detect tables embedded in the web page’s HTML. Excel Web queries can also be used in situations where a standard ODBC(Open Database Connectivity) connection gets hard to create or maintain. You can directly scrape a table from any website using Excel Web Queries.</p><p id="86f7" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">The process boils down to several simple steps (Check out <a href="http://www.excel-university.com/pull-external-data-into-excel/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">this article</a>):</p><p id="52e2" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">1. Go to Data &gt; Get External Data &gt; From Web</p><p id="3cda" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">2. A browser window named “New Web Query” will appear</p><p id="7b82" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">3. In the address bar, write the web address</p><figure class="fm fn fo fp fq fr cl cm paragraph-image"><div class="fs ft fu fv ak"><div class="cl cm hs"><div class="gb r fu gc"><div class="ht r"><div class="fw fx cp t u fy ak eh fz ga"><img class="cp t u fy ak ge gf gg" src="https://miro.medium.com/max/60/0*RNSiEzMrSHQCIUJp.png?q=20" width="645" height="417" role="presentation"></div><img class="fw fx cp t u fy ak gh" width="645" height="417" role="presentation"><noscript><img class="cp t u fy ak" src="https://miro.medium.com/max/1290/0*RNSiEzMrSHQCIUJp.png" width="645" height="417" role="presentation"></noscript></div></div></div></div></figure><p id="89bf" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">(picture from excel-university.com)</p><p id="f44a" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">4. The page will load and will show yellow icons against data/tables.</p><p id="f300" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">5. Select the appropriate one</p><p id="a90a" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">6. Press the Import button.</p><p id="7ca6" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">Now you have the web data scraped into the Excel Worksheet — perfectly arranged in rows and columns as you like.</p><figure class="fm fn fo fp fq fr cl cm paragraph-image"><div class="fs ft fu fv ak"><div class="cl cm hu"><div class="gb r fu gc"><div class="hv r"><div class="fw fx cp t u fy ak eh fz ga"><img class="cp t u fy ak ge gf gg" src="https://miro.medium.com/max/60/0*p5egK5jHH64SYo-X.png?q=20" width="845" height="432" role="presentation"></div><img class="fw fx cp t u fy ak gh" width="845" height="432" role="presentation"><noscript><img class="cp t u fy ak" src="https://miro.medium.com/max/1690/0*p5egK5jHH64SYo-X.png" width="845" height="432" role="presentation"></noscript></div></div></div></div></figure><h1 id="6848" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi"><strong class="az">Getting web data using Excel VBA</strong></h1><p id="5b66" class="gi gj dc bk gk b gl hj gn hk gp hl gr hm gt hn gv cu">Most of us would use formula’s in Excel(e.g. =avg(…), =sum(…), =if(…), etc.) a lot, but less familiar with the built-in language — Visual Basic for Application a.k.a VBA. It’s commonly known as “Macros” and such Excel files are saved as a **.xlsm. Before using it, you need to first enable the Developer tab in the ribbon (right click File -&gt; Customize Ribbon -&gt; check Developer tab). Then set up your layout. In this developer interface, you can write VBA code attached to various events. Click HERE (https://msdn.microsoft.com/en-us/library/office/ee814737(v=office.14).aspx) to getting started with VBA in excel 2010.</p><figure class="fm fn fo fp fq fr cl cm paragraph-image"><div class="fs ft fu fv ak"><div class="cl cm hw"><div class="gb r fu gc"><div class="hx r"><div class="fw fx cp t u fy ak eh fz ga"><img class="cp t u fy ak ge gf gg" src="https://miro.medium.com/max/60/0*_D0KxpQjrhh0hqTz.jpg?q=20" width="742" height="167" role="presentation"></div><img class="fw fx cp t u fy ak gh" width="742" height="167" role="presentation"><noscript><img class="cp t u fy ak" src="https://miro.medium.com/max/1484/0*_D0KxpQjrhh0hqTz.jpg" width="742" height="167" role="presentation"></noscript></div></div></div></div></figure><p id="c86f" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">Using Excel VBA is going to be a bit technical — this is not very friendly for non-programmers among us. VBA works by running macros, step-by-step procedures written in Excel Visual Basic. To scrape data from websites to Excel using VBA, we need to build or get some VBA script to send some request to web pages and get returned data from these web pages. It’s common to use VBA with XMLHTTP and regular expressions to parse the web pages. For Windows, you can use VBA with WinHTTP or InternetExplorer to scrape data from websites to Excel.</p><p id="d943" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">With some patience and some practice, you would find it worthwhile to learn some Excel VBA code and some HTML knowledge to make your web scraping into Excel much easier and more efficient for automating the repetitive work. There’s a plentiful amount of material and forums for you to learn how to write VBA code.</p><h1 id="1a8f" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi"><strong class="az">Automated Web Scraping Tools</strong></h1><p id="d2bf" class="gi gj dc bk gk b gl hj gn hk gp hl gr hm gt hn gv cu">For someone who is looking for a quick tool to scrape data off pages to Excel and doesn’t want to set up the VBA code yourself, I strongly recommend automated web scraping tools <a href="http://www.octoparse.com" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">(https://www.octoparse.com/) </a>to scrape data for your Excel Worksheet directly or via API. There is no need to learn programming. You can pick one of those web scraping freeware from the list, and get started with extracting data from websites immediately and exporting the scraped data into Excel. Different web scraping tool has its pros and cons and you can choose the perfect one to fit your needs.</p><p id="eb70" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu">Check out <a href="https://www.octoparse.com/blog/top-30-free-web-scraping-software/" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow">this post</a> and try out these TOP 30 free web scraping tools</p><h1 id="67df" class="gx gy dc bk bj gz de ha dg hb hc hd he hf hg hh hi"><strong class="az">Outsource Your Web Scraping Project</strong></h1><p id="21d7" class="gi gj dc bk gk b gl hj gn hk gp hl gr hm gt hn gv cu">If time is your most valuable asset and you want to focus on your core businesses, outsourcing such complicated web scraping work to a proficient web scraping team that has experience and expertise would be the best option. It’s difficult to scrape data from websites due to the fact that the presence of anti-scraping bots will restrain the practice of web scraping. A proficient web scraping team would help you get data from websites in a proper way and deliver structured data to you in an Excel sheet, or in any format you need.</p></div></div></section><hr class="hy ef hz ia ib ic id ie if ig ih"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="3e06" class="gi gj dc bk gk b gl gm gn go gp gq gr gs gt gu gv cu"><strong class="gk gw">Don’t hesitate if you have things to say. I am a passionate web scraper. </strong>Welcome to read more articles, and learn web scraping at <a href="http://www.octoparse.com" class="at cg ho hp hq hr" target="_blank" rel="noopener nofollow"><strong class="gk gw">Octoparse</strong></a><strong class="gk gw">.</strong></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><figure class="db dc dd de df dg cl cm paragraph-image"><div class="dh di dj dk ak"><div class="cl cm da"><div class="dr r dj ds"><div class="dt r"><div class="dl dm cp t u dn ak do dp dq"><img class="cp t u dn ak du dv dw" src="https://miro.medium.com/max/60/0*Y0HlJutBZuGHoibP?q=20" width="5184" height="3456" role="presentation"></div><img class="dl dm cp t u dn ak dx" width="5184" height="3456" role="presentation"><noscript><img class="cp t u dn ak" src="https://miro.medium.com/max/10368/0*Y0HlJutBZuGHoibP" width="5184" height="3456" role="presentation"></noscript></div></div></div></div><figcaption class="bo dy dz ea eb cn cl cm ec ed bj ee">“A close-up of a frozen spider web in Neumühl.” by <a href="https://unsplash.com/@artnok?utm_source=medium&amp;utm_medium=referral" class="at cg ef eg eh ei" target="_blank" rel="noopener nofollow">Nicolas Picard</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" class="at cg ef eg eh ei" target="_blank" rel="noopener nofollow">Unsplash</a></figcaption></figure><div><div id="b17d" class="ej ek el bk em b en eo ep eq er es et eu ev ew ex"><h1 class="em b en ey ep ez er fa et fb ev fc el">Web Crawling? eh.. What is it?</h1></div><div class="fd"><div class="n fe ff fg fh"><div class="o n"><div><a rel="noopener" href="/@siddharthalibra13?source=post_page-----fdfb91e946f7----------------------"><img alt="Siddhartha Anand" class="r fi fj fk" src="https://miro.medium.com/fit/c/96/96/1*2V5VvaF6S8rBAnjstr-unQ@2x.jpeg" width="48" height="48"></a></div><div class="fl ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r el q"><div class="fm n o fn"><span class="bj ee dy bl do fo fp fq fr fs el"><a class="at au av aw ax ay az ba bb bc ft bf bg bh bi" rel="noopener" href="/@siddharthalibra13?source=post_page-----fdfb91e946f7----------------------">Siddhartha Anand</a></span><div class="fu r ar h"><button class="fv el q by fw fx fy fz bc bh ga gb gc gd ge gf cb bj b bk gg gh bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ee dy bl do fo fp fq fr fs bo"><div><a class="at au av aw ax ay az ba bb bc ft bf bg bh bi" rel="noopener" href="/@siddharthalibra13/web-crawling-eh-what-is-it-fdfb91e946f7?source=post_page-----fdfb91e946f7----------------------">Apr 17, 2015</a> <!-- -->·<!-- --> <!-- -->2<!-- --> min read</div></span></span></div></div><div class="n gi gj gk gl gm gn go gp ab"><div class="n o"><div class="gq r ar"><a href="//medium.com/p/fdfb91e946f7/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gq r ar"><a href="//medium.com/p/fdfb91e946f7/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gr r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40siddharthalibra13%2Fweb-crawling-eh-what-is-it-fdfb91e946f7&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="7231" class="gs gt el bk gu b gv gw gx gy gz ha hb hc hd he hf cu">Hey folks! This post is for all those who have always wondered what web crawling is, how do you do it but have never been able to understand it. Lo and behold! your search stops here..:) In layman’s terms, Web crawling is the art of extracting vast amounts of <em class="hg">information</em> from the world wide web.</p><p id="7e75" class="gs gt el bk gu b gv gw gx gy gz ha hb hc hd he hf cu">Hey so what.. what’s so different about it? Extracting data was done years ago(large data sheets, hand-written bank records, all people staying at a hotel, etc). Ahem!.. wait! Imagine the online register at the entrance of a hotel that was built a few years ago. Now, imagine one million such registers containing the exact same information of people who have visited the hotel since that time. You are the new supervisor and are supposed to draw out the details of all those people. That’s tough! right? A hard working guy would take a paper and a pen and manually do all the hard work. A smart guy would write a code that automatically does this for him. That’s one kind of web scraping and crawling.</p><p id="55f6" class="gs gt el bk gu b gv gw gx gy gz ha hb hc hd he hf cu"><strong class="gu hh">Web crawlers</strong> are also known as <strong class="gu hh"><em class="hg">spiders</em></strong> because of the their very nature to walk through the <strong class="gu hh"><em class="hg">world wide</em></strong> <strong class="gu hh"><em class="hg">web</em></strong>. Yes, they are the soul of online search engines that help you with relevant pages in barely a fraction of a second! How? These silent warriors pack their tools and tricks up their sleeves and go around the world downloading bulk of information from the online document store(i.e. WWW). Just to generate more interest among you guys I am sharing another link <a href="http://bostinno.streetwise.co/2015/04/16/recorded-future-has-raised-12m-for-its-cyber-threat-web-crawling-service/" class="at cg ef eg eh ei" target="_blank" rel="noopener nofollow">here</a>. Just read it and think about it. If you want to further try out something, then <a href="https://sidlearnstocrawl.wordpress.com/2015/06/27/first-chapter-learning-basics/" class="at cg ef eg eh ei" target="_blank" rel="noopener nofollow">y</a>ou can get your <a class="at cg ef eg eh ei" target="_blank" rel="noopener" href="/@siddharthalibra13/first-chapter-learning-basics-c14ef98230f3">feet wet here</a>.</p></div></div></section><hr class="hi ee hj hk hl eb hm hn ho hp hq"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="3631" class="gs gt el bk gu b gv gw gx gy gz ha hb hc hd he hf cu"><em class="hg">Originally published at </em><a href="https://sidlearnstocrawl.wordpress.com/2015/04/17/web-crawling-eh-what-is-it/" class="at cg ef eg eh ei" target="_blank" rel="noopener nofollow"><em class="hg">sidlearnstocrawl.wordpress.com</em></a><em class="hg"> on April 17, 2015.</em></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="0fac" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How Xpath Plays Vital Role In Web Scraping Part 2</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@sandra_21783?source=post_page-----fd32e6c45c65----------------------"><img alt="Sandra K" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/2*mIH5vdwhAOewjJjvRqoZ1Q.png" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@sandra_21783?source=post_page-----fd32e6c45c65----------------------">Sandra K</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@sandra_21783/how-xpath-plays-vital-role-in-web-scraping-part-2-fd32e6c45c65?source=post_page-----fd32e6c45c65----------------------">Aug 26, 2016</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/fd32e6c45c65/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/fd32e6c45c65/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40sandra_21783%2Fhow-xpath-plays-vital-role-in-web-scraping-part-2-fd32e6c45c65&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="3f21" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">To read the first part of this blog do read:</p><div class="ga gb gc gd ge gf"><a href="https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping/" rel="noopener nofollow"><div class="gi n ar"><div class="gj n co p gk gl"><h2 class="bj gm gn bl dc"><div class="eh gg ej ek gh em">How Xpath Plays Vital Role In Web Scraping - Data hut</div></h2><div class="go r"><h3 class="bj ef eg bl bo"><div class="eh gg ej ek gh em">XPath is a language for finding information in structured documents like XML or HTML. You can say that XPath is (sort…</div></h3></div><div class="gp r"><h4 class="bj ef fb bl bo"><div class="eh gg ej ek gh em">blog.datahut.co</div></h4></div></div><div class="gq r"><div class="gr r gs gt gu gq gv gw gx"></div></div></div></a></div><p id="f801" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Here is a piece of content on Xpaths which is the follow up of <a href="https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping/" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">How Xpath Plays Vital Role In Web Scraping</a></p><p id="f410" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Let’s dive into a real-world example of scraping amazon website for getting information about deals of the day. Deals of the day in amazon can be found at this . So navigate to the (deals of the day) in Firefox and find the XPath selectors. Right click on the deal you like and select “Inspect Element with Firebug”:</p><p id="7188" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">If you observe the image below keenly, there you can find the source of the image(deal) and the name of the deal in src, alt attribute’s respectively. So now let’s write a generic XPath which gathers the name and image source of the product(deal). //img[@role=”img”]/@src ## for image source //img[@role=”img”]/@alt ## for product name</p><p id="87f2" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">In this post, I’ll show you some tips we found valuable when using XPath in the trenches.</p><p id="094b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">If you have an interest in Python and web scraping, you may have already played with the nice <a href="http://docs.python-requests.org/" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">requests library </a>to get the content of pages from the Web. Maybe you have toyed around using <a href="http://doc.scrapy.org/en/latest/topics/selectors.html" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">Scrapy selector </a>or to make the content extraction easier. Well, now I’m going to show you some tips I found valuable when using XPath in the trenches and we are going to use both and <a href="http://doc.scrapy.org/en/latest/topics/selectors.html" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">Scrapy selector </a>for HTML parsing.</p><p id="20df" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Avoid using expressions which contains(.//text(), ‘search text’) in your XPath conditions. Use contains(., ‘search text’) instead.</p><p id="4019" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Here is why: the expression .//text() yields a collection of text elements — a node-set(collection of nodes).and when a node-set is converted to a string, which happens when it is passed as argument to a string function like contains() or starts-with(), results in the text for the first element only.</p><p id="d30f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo hc">Scrapy Code:</strong></p><p id="4a87" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">from scrapy import Selector<br> html_code = “””&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;”””<br> sel = Selector(text=html_code)<br> print xp(‘//a//text()’)<br> xp = lambda x: sel.xpath(x).extract() # Let’s type this only once # Take a peek at the node-set<br> [u’Click here to go to the ‘, u’Next Page’] # output of above command<br> print xp(‘string(//a//text())’) # convert it to a string # output of the above command<br> [u’Click here to go to the ‘]</p><p id="7c9f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Let’s do the above one by using lxml then you can implement XPath by both lxml or Scrapy selector as XPath expression is same for both methods.</p><p id="8134" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo hc">lxml code:</strong></p><p id="c85b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">from lxml import html <br> html_code = “””&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;””” # Parse the text into a tree<br> parsed_body = html.fromstring(html_code) # Perform xpaths on the tree<br> print parsed_body(‘//a//text()’) # take a peek at the node-set<br> [u’Click here to go to the ‘, u’Next Page’] # output<br> print parsed_body(‘string(//a//text())’) # convert it to a string<br> [u’Click here to go to the ‘] # output</p><p id="93f2" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">A node converted to a string, however, puts together the text of itself plus of all its descendants:</p><p id="c04d" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp(‘//a[1]’) # selects the first a node<br> [u’&lt;a href=”#”&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;’]</p><p id="fdb6" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp(‘string(//a[1])’) # converts it to string<br> [u’Click here to go to the Next Page’]</p><p id="e8e3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Beware of the difference between //node[1] and (//node)[1]//node[1] selects all the nodes occurring first under their respective parents and (//node)[1] selects all the nodes in the document, and then gets only the first of them.</p><p id="5985" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">from scrapy import Selector</p><p id="e668" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">sel = Selector(text=html_code) <br> xp = lambda x: sel.xpath(x).extract()</p><p id="ed97" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">xp(“//li[1]”) # get all first LI elements under whatever it is its parent</p><p id="6e2a" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">xp(“(//li)[1]”) # get the first LI element in the whole document</p><p id="1c09" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">xp(“//ul/li[1]”) # get all first LI elements under an UL parent</p><p id="c35c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">xp(“(//ul/li)[1]”) # get the first LI element under an UL parent in the document</p><p id="0f99" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">//a[starts-with(@href, ‘#’)][1] gets a collection of the local anchors that occur first under their respective parents and (//a[starts-with(@href, ‘#’)])[1] gets the first local anchor in the document.</p><p id="2399" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">When selecting by class, be as specific as necessary.</p><p id="f85c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">If you want to select elements by a CSS class, the XPath way to do the same job is the rather verbose:</p><p id="dd8e" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo hc">*[contains(concat(‘ ‘, normalize-space(@class), ‘ ‘), ‘ someclass ‘)]</strong></p><p id="7744" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Let’s cook up some examples:</p><p id="b2d6" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; sel = Selector(text=’&lt;p class=”content-author”&gt;Someone&lt;/p&gt;&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’)</p><p id="855f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp = lambda x: sel.xpath(x).extract()</p><p id="6015" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">BAD: because there are multiple classes in the attribute</p><p id="350f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">[]</p><p id="497f" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">BAD: gets more content than we need</p><p id="3836" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp(“//*[contains(@class,’content’)]”)</p><p id="5613" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">[u’&lt;p class=”content-author”&gt;Someone&lt;/p&gt;’, u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</p><p id="af4a" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; xp(“//*[contains(concat(‘ ‘, normalize-space(@class), ‘ ‘), ‘ content ‘)]”) <br> [u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</p><p id="7636" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">And many times, you can just use a CSS selector instead, and even combine the two of them if needed:</p><p id="1a5d" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; sel.css(“.content”).extract() <br> [u’&lt;p class=”content text-wrap”&gt;Some content&lt;/p&gt;’]</p><p id="783c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">&gt;&gt;&gt; sel.css(‘.content’).xpath(‘@class’).extract() <br> [u’content text-wrap’]</p><p id="b107" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Learn to use all the different axes.</p><p id="ef7b" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">It is handy to know how to use the axes, you can follow through these examples .</p><p id="f297" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">In particular, you should note that following and following-sibling are not the same thing, this is a common source of confusion. The same goes for preceding and preceding-sibling, and also ancestor and parent.</p><p id="c9ad" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><strong class="fo hc">Useful trick to get text content</strong></p><p id="cc03" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Here is another XPath trick that you may use to get the interesting text contents:</p><p id="8287" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">//*[not(self::script or self::style)]/text()[normalize-space(.)]</p><p id="f0a3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">This excludes the content from the script and style tags and also skip whitespace-only text nodes.</p><p id="0eb1" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Tools &amp; Libraries Used:</p><p id="f0d3" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Firefox<br> Firefox inspect element with firebug<br> Scrapy : 1.1.1<br> Python : 2.7.12<br> Requests : 2.11.0</p><p id="4d3c" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Have questions? Comment below. Please share if you found this helpful.</p><p id="7c95" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu">Read the original article here: <a href="https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping-part-2/" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow">https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping-part-2/</a></p></div></div></section><hr class="hd ef he hf hg hh hi hj hk hl hm"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="1b20" class="fm fn dc bk fo b fp fq fr fs ft fu fv fw fx fy fz cu"><em class="hn">Originally published at </em><a href="https://blog.datahut.co/how-xpath-plays-vital-role-in-web-scraping-part-2/" class="at cg gy gz ha hb" target="_blank" rel="noopener nofollow"><em class="hn">https://blog.datahut.co</em></a><em class="hn"> on August 26, 2016.</em></p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="7529" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Theory vs. The World: How Retrieving Links from Google Is not t<em class="du">hat Easy</em></h1></div><div class="dv"><div class="n dw dx dy dz"><div class="o n"><div><a rel="noopener" href="/@guglielmofeis?source=post_page-----3ab35c8ac92a----------------------"><img alt="Guglielmo Feis" class="r ea eb ec" src="https://miro.medium.com/fit/c/96/96/1*VJbR-mrPzjtpHVxaGqdIGg.jpeg" width="48" height="48"></a></div><div class="ed ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ee n o ef"><span class="bj eg eh bl ei ej ek el em en dc"><a class="at au av aw ax ay az ba bb bc eo bf bg bh bi" rel="noopener" href="/@guglielmofeis?source=post_page-----3ab35c8ac92a----------------------">Guglielmo Feis</a></span><div class="ep r ar h"><button class="eq dc q by er es et eu bc bh ev ew ex ey ez fa cb bj b bk fb fc bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj eg eh bl ei ej ek el em en bo"><div><a class="at au av aw ax ay az ba bb bc eo bf bg bh bi" rel="noopener" href="/@guglielmofeis/theory-vs-the-world-how-retrieving-links-from-google-is-not-hat-easy-3ab35c8ac92a?source=post_page-----3ab35c8ac92a----------------------">Jan 20</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div><div class="n fd fe ff fg fh fi fj fk ab"><div class="n o"><div class="fl r ar"><a href="//medium.com/p/3ab35c8ac92a/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fl r ar"><a href="//medium.com/p/3ab35c8ac92a/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fm r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40guglielmofeis%2Ftheory-vs-the-world-how-retrieving-links-from-google-is-not-hat-easy-3ab35c8ac92a&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><blockquote class="fn fo fp"><p id="2657" class="fq fr dc fs ft b fu fv fw fx fy fz ga gb gc gd ge cu">Do you want to prove a bit of coding helps in the Humanities? Easy!</p></blockquote><p id="bbfb" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge gf cu"><span class="r gg gh gi gj gk gl gm gn go gp">We</span> all use Google a lot in our research, what if you can store the links you get from search results? This looks like a super-easy task. It takes a second to figure out the steps you need to perform <em class="fs">by hand</em>: access Google, perform the search, get results, save data, move to the next page, iterate if needed.</p><p id="2845" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Plus the “extract the link” is quite a popular feature in variaty of packages that perform webscraping, you there should be a lot of documentation annd tutorials out there. Even better: the script we want to build is helpful for some colleagues (we’ll work with <strong class="ft gq">Python</strong> here).</p><p id="95d8" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">It looks like that’s an eay task to learn some new features of a library by putting it in practice. Further, it proves the point of <strong class="ft gq">coding helps in the humanities</strong>.</p><p id="ce3d" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Cool, so just go. It won’t take long, right? Spoiler: it was not that easy (hence the post).</p><figure class="gs gt gu gv gw gx cl cm paragraph-image"><div class="cl cm gr"><div class="hd r gp he"><div class="hf r"><div class="gy gz cp t u ha ak ei hb hc"><img class="cp t u ha ak hg hh hi" src="https://miro.medium.com/max/60/1*wgmjZE_Ex0XhiMLPqLvsDw.png?q=20" width="640" height="513" role="presentation"></div><img class="gy gz cp t u ha ak hj" width="640" height="513" role="presentation"><noscript><img class="cp t u ha ak" src="https://miro.medium.com/max/1280/1*wgmjZE_Ex0XhiMLPqLvsDw.png" width="640" height="513" role="presentation"></noscript></div></div></div><figcaption class="bo eh hk hl hm cn cl cm hn ho bj eg">[SPOILER: but we’ll make it a yes. Image by <a href="https://pixabay.com/users/GDJ-1086657/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2069850" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Gordon Johnson</a> from <a href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2069850" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Pixabay</a>]</figcaption></figure></div></div></section><hr class="ht eg hu hv hw hm hx hy hz ia ib"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><h1 id="8b78" class="ic id dc bk bj ie de if dg ig ih ii ij ik il im in">The Basic Idea: Requests and BeautifulSoup</h1><p id="9120" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">The project outline is easy to map and close to what we would do by hand:</p><ol class=""><li id="a0ca" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge it iu iv">reach a search engine;</li><li id="7dea" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge it iu iv">query it;</li><li id="c28b" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge it iu iv">get the results of the query;</li><li id="72ac" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge it iu iv">extract all the links;</li><li id="0488" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge it iu iv">save them;</li><li id="e8a8" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge it iu iv">move to the next page;</li><li id="0a78" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge it iu iv">rinse and repeat.</li></ol><p id="ed29" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Step 4 looks like the most scary one. We’ll have to inspect the html and get the right tag. But that’s part of the fun. Ok, there are issues lurking here like “how do I find out when I run out of results?”. But we can agree to have a fixed set of pages scraped or even stop a the first one.</p><p id="5953" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Armed with <strong class="ft gq">requests</strong> and <strong class="ft gq">BeautifulSoup</strong> library (if you don’t have them, get the instruction for installation <a href="https://2.python-requests.org/en/master/user/install/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">here</a> and <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-beautiful-soup" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">here</a>, respectively) we begin our journey with some standard imports:</p><pre class="gs gt gu gv gw jb jc jd"><span id="4556" class="je id dc bk jf b eh jg jh r ji"><strong class="jf gq">import</strong> requests<br><strong class="jf gq">from</strong> bs4 <strong class="jf gq">import</strong> BeautifulSoup <strong class="jf gq">as</strong> bs</span></pre><p id="7b08" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Next, we build our request to a search engige (Google here). To do that we note that all queries on Google have the url that goes as: ‘<a href="https://www.google.com/search?q=" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow"><em class="fs">https://www.google.com/search?q=</em></a>’ + ‘something to query’.</p><p id="f57e" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">As we don’t want to keep typing our query as an input, we’ll hard code it, i.e. search ‘Goofy’. Then, we check the status of our request to make sure everything is ok when we access the page.</p><pre class="gs gt gu gv gw jb jc jd"><span id="7ed4" class="je id dc bk jf b eh jg jh r ji"><strong class="jf gq">import</strong> requests<br><strong class="jf gq">from</strong> bs4 <strong class="jf gq">import</strong> BeautifulSoup <strong class="jf gq">as</strong> bs</span><span id="971f" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#search for our term with requests</em><br>searchreq = requests.get('https://www.google.com/search?q=Goofy')<br><em class="fs"><br>#ensure it works</em><br>searchreq.raise_for_status()</span></pre><p id="a783" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">If you want to input a different query everytime (i.e. not to hard code it) you may go with something like this:</p><pre class="gs gt gu gv gw jb jc jd"><span id="e446" class="je id dc bk jf b eh jg jh r ji"><strong class="jf gq">import</strong> requests<br><strong class="jf gq">from</strong> bs4 <strong class="jf gq">import</strong> BeautifulSoup <strong class="jf gq">as</strong> bs</span><span id="48b7" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs"># ask the user what to search</em><br>query = input('What do you want to search?')</span><span id="4c84" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#search for our term with requests</em><br>searchreq = requests.get('https://www.google.com/search?q=' + query)</span><span id="66a7" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#ensure it works</em><br>searchreq.raise_for_status()</span></pre><h1 id="03b5" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">Getting the Links</h1><p id="f406" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">We have done tasks 1, 2 and 3 from our sketch. Now comes the tricky part. We need to isolate the links that Google gives us. This means we need to create a BeautifulSoup object for each page returning the search results (i.e. what we called <strong class="ft gq">searchreq</strong>) and process them with BeautifulSoup.</p><p id="4b6e" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">We follow the standard practice and call this object ‘soup’. We also specify it’s html that we want to parse. Then in ‘results’ we are going to use our soup object to return what we need and print it. That’s what we add to our code:</p><pre class="gs gt gu gv gw jb jc jd"><span id="d0bf" class="je id dc bk jf b eh jg jh r ji"><em class="fs"># creating the Beautiful Soup object to parse html <br></em>soup = bs(searchreq.text, 'html.parser')</span><span id="6039" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#apply a find all method on our soup object to get the result</em><br>results = soup.find_all() <em class="fs">#but wait, what to we have to search?</em></span><span id="cc09" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#print them and be happy (if it works)</em><br>print(results)</span><span id="e591" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#SPOILER: it won't</em></span></pre><h1 id="03c0" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">Scraping the Links</h1><p id="7f06" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">To scrape the links we need to tell BeautifulSoup what we need it to extract. To find this out, we call the inspector mode from our web browser on one of the search results (right click and select inspect on Chrome).</p><p id="798f" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">From there we play a game of:</p><ol class=""><li id="62fd" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge it iu iv">finding the items we need;</li><li id="18df" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge it iu iv">extracting patterns or regularities for the items we care about (i.e. the links);</li><li id="0330" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge it iu iv">catch them all.</li></ol><figure class="gs gt gu gv gw gx cl cm paragraph-image"><div class="ju jv gp jw ak"><div class="cl cm jt"><div class="hd r gp he"><div class="jx r"><div class="gy gz cp t u ha ak ei hb hc"><img class="cp t u ha ak hg hh hi" src="https://miro.medium.com/max/60/1*NBLLEGDxvYM3iQOBpKih_Q.jpeg?q=20" width="1280" height="853" role="presentation"></div><img class="gy gz cp t u ha ak hj" width="1280" height="853" role="presentation"><noscript><img class="cp t u ha ak" src="https://miro.medium.com/max/2560/1*NBLLEGDxvYM3iQOBpKih_Q.jpeg" width="1280" height="853" role="presentation"></noscript></div></div></div></div><figcaption class="bo eh hk hl hm cn cl cm hn ho bj eg">[Sorry, this has to happen — Photo by <a href="https://www.pexels.com/@carocastilla?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Carolina Castilla Arias </a>from <a href="https://www.pexels.com/photo/close-up-photo-of-pokemon-pikachu-figurine-1716861/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Pexels</a>]</figcaption></figure><p id="b1ee" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Our first choice might be something like ‘http’, but this is going to catch a lot of extra stuff as well like links that are <strong class="ft gq">not</strong> search results.</p><p id="5289" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">You have to think about HTML patterns and tags. If you look at it (or Google around like crazy), you’ll find out that there’s a nice thing called <strong class="ft gq">div class=“r”</strong> that seems to have what you are looking for.</p><p id="cc7b" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">After a few extra minutes with the BeautifulSoup documentation page, we learn to get them from the soup with: <strong class="ft gq">soup.select(‘.r a’)</strong>.</p><p id="229f" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">So we put all together:</p><pre class="gs gt gu gv gw jb jc jd"><span id="2cd2" class="je id dc bk jf b eh jg jh r ji"><strong class="jf gq">import</strong> requests<br><strong class="jf gq">from</strong> bs4 <strong class="jf gq">import</strong> BeautifulSoup <strong class="jf gq">as</strong> bs<br><em class="fs"># ask the user for a the search</em><br>query = input('What do you want to search?')</span><span id="237b" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#search for our term with requests</em><br>searchreq = requests.get('https://www.google.com/search?q=' + query)</span><span id="2bb8" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#ensure it works</em><br>searchreq.raise_for_status()</span><span id="be53" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs"># creating the Beautiful Soup object</em><br>soup = bs(searchreq.text, 'html.parser')</span><span id="747a" class="je id dc bk jf b eh jj jk jl jm jn jh r ji"><em class="fs">#apply a find all method on our soup object to get the result</em><br>results = soup.select('.r a')</span><span id="25d5" class="je id dc bk jf b eh jj jk jl jm jn jh r ji">print(results)</span></pre><p id="ee45" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">We are ready to try this out!</p><h1 id="7d9f" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">Stuck: The World Strikes Back</h1><p id="a446" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu"><strong class="ft gq">[]</strong></p><p id="c6e6" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Exactly, watch that again. A pair of square brackets. That’s our output.</p><p id="6511" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu"><strong class="ft gq">[], i.e. </strong>an empty list.</p><p id="4df6" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">That’s our result. This is disappointing. Why is that? What’s happening? Let’s check what’s going on.</p><p id="bdac" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">The first we do is try to print our soup object (if you have Ipython, use the shell). Once we have the soup object printed, we try to search our beloved “r” class, the one we are trying to select with out soup object.</p><p id="4b09" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu"><strong class="ft gq">It’s not there!</strong></p><p id="8499" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">This is: <em class="fs">the world getting back on us</em>. In practice, theory is not enough. So, well, <em class="fs">now we can panic</em>. What’s going on? This was supposed to be an easy task.</p><h1 id="c6a7" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">Ways Out</h1><p id="cab4" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">We start googling more. I went out on Twitter and ask Al Sweigart (the author of <a href="https://automatetheboringstuff.com/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Automate the Boring Stuff with Python</a>, a book you should check if you are starting out with Python) about it. In fact, one of the programs in the book discusses the task of getting links.</p><p id="be7d" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Al was kind enough to let me know that’s common practice for Google to obscure its results. That’s why the soup doesn’t match what we looked at. He briefly reminded me there’s life out of Google, so there are chances to be better off searching on different search engines (he suggested duckduckgo).</p><p id="e679" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">That’s <em class="fs">reeeeally</em> important (hence the extra <em class="fs">Es</em>). Now we know the cause of the problem: <strong class="ft gq">the HTML we see on the Google is not the same we get with our request</strong>. And we already have a hint towards a solution: try asking to different search engines.</p><p id="7bf0" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">We can use these new knowledge to build alternative ways.</p><h1 id="6eb3" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">Rethinking the Issue</h1><p id="2b83" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">We have a new problem. The HTML that delivers our search results is partly out of our control. What can we do? Can we get it like we see? Are there ways around it? This depends on how we want to fight.</p><h1 id="08ad" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">1. Ways Around: Different Search Engines</h1><p id="618e" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">The first option is to circumvent the problem: we pick a different search engine. In practice, we go on <em class="fs">Wikipedia</em> and asks for search engines names. We then figure out how the query is asked and hope that the links extraction phase stays the same.</p><p id="5351" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Assuming this, that doesn’t look as a costly option. And we hope one of the engines gives us the same html we can inspect.</p><h1 id="a216" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">2. No Way(s): We Fight!</h1><p id="7b78" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">We know what we want to get. Despite the HTML tags being different, we know the links are still there. What about extracting them through <a href="https://docs.python.org/3/howto/regex.html" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow"><strong class="ft gq">regular expressions</strong></a>? It will be difficult and maybe sub-optmial, but rather than risking to fight again with HTML obfuscation, etc. we can tackle the issue once and forever.</p><p id="ef46" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">We’ll write a regular expression extracting all that <em class="fs">http-something</em>. We can predict we will:</p><ul class=""><li id="266a" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge jy iu iv">have double results or even more copies (which we’ll exclude by way of making a <strong class="ft gq">set</strong> out of our results)</li><li id="c76c" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge jy iu iv">have some bad results (like links to you Google account; or extra non-search related links).</li></ul><p id="9725" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Assuming you can identify the bad links, more links than required might be better than the [empty list] we got before.</p><h1 id="9e15" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">3. Rebuilding: from BeutifulSoup to Selenium</h1><p id="6b65" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">Maybe we can get around the HTML obfuscation and get the search results in a different way. <strong class="ft gq">Selenium</strong> is another popular Python library that allows us to automate our browsing.</p><p id="acb4" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">Selenium will open the browser for us and then we’ll have a look at the HTML. Should this fail, we may have Selenium inspect the page for us and copy and paste the inspected html.</p><p id="49cb" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">This seems something that can work <em class="fs">in theory</em>. But requires extra efforts.</p><h1 id="373d" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">4. Download the HTML in Different Ways</h1><p id="75bc" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">We know that obfuscation happens but we do not know how and when. Maybe we can try to download the page and save it on our desktop and operate from there.</p><p id="5950" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">This sounds both simple and complicated. Saving a file, easy. Still, we need to access it properly… Is request the way to go? This requires some extra efforts.</p><h1 id="4bdb" class="ic id dc bk bj ie de jo dg jp ih jq ij jr il js in">To Do:</h1><p id="00f6" class="fq fr dc bk ft b fu io fw ip fy iq ga ir gc is ge cu">Ok, there’s still a problem but the field looks clearer:</p><ul class=""><li id="8723" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge jy iu iv">the different ways need exploring;</li><li id="afc0" class="fq fr dc bk ft b fu iw fw ix fy iy ga iz gc ja ge jy iu iv">code should grow and make it to GitHub.</li></ul></div></div></section><hr class="ht eg hu hv hw hm hx hy hz ia ib"><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="006f" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">(This is an improved and reviewed version of a previous post that appeared here: <a href="http://www.thegui.eu/blog/scraping-links-from-google-part-1.htm" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">http://www.thegui.eu/blog/scraping-links-from-google-part-1.htm</a>).</p><p id="f44f" class="fq fr dc bk ft b fu fv fw fx fy fz ga gb gc gd ge cu">This work is carried out as part of a <strong class="ft gq">CAS Fellowship</strong> as <em class="fs">CAS-SEE Rijeka</em>. See more about the Fellowship <a href="http://cas.uniri.hr/cas-see-fellowship-application/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">here.</a></p></div></div></section></div><div><div class="dq u dr ds dt du"></div><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><div><div id="b746" class="eb ec ed bj ee b ef eg eh ei ej ek el em en eo ep"><h1 class="ee b ef eq eh er ej es el et en eu ed">ReactJS Examples</h1></div><div class="ev"><div class="n ew ex ey ez"><div class="o n"><div><a rel="noopener" href="/@makzan?source=post_page-----f46dc076886a----------------------"><img alt="makzan" class="r fa fb fc" src="https://miro.medium.com/fit/c/96/96/0*IGKg4bOHHmOslgKv.jpeg" width="48" height="48"></a></div><div class="fd ak r"><div class="n"><div style="flex:1"><span class="av b bj bk bl bm r ed q"><div class="fe n o ff"><span class="av fg fh bk az fi bb bc bd be ed"><a class="fj fk br bs bt bu bv bw bx by fl cb cc fm fn" rel="noopener" href="/@makzan?source=post_page-----f46dc076886a----------------------">makzan</a></span><div class="fo r bg h"><button class="fp ed q cg fq fr fs ft by fm fu fv fw fx fy fz cj av b bj ga gb bm ck cl cm cn co cb">Follow</button></div></div></span></div></div><span class="av b bj bk bl bm r bn bo"><span class="av fg fh bk az fi bb bc bd be bn"><div><a class="fj fk br bs bt bu bv bw bx by fl cb cc fm fn" rel="noopener" href="/makzan-scrapbook/reactjs-examples-f46dc076886a?source=post_page-----f46dc076886a----------------------">Nov 11, 2015</a> <!-- -->·<!-- --> <!-- -->1<!-- --> min read</div></span></span></div></div><div class="n gc gd ge gf gg gh gi gj ab"><div class="n o"><div class="gk r bg"><a href="//medium.com/p/f46dc076886a/share/twitter?source=post_actions_header---------------------------" class="fj fk br bs bt bu bv bw bx by gl gm cb cc fm fn" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gk r bg"><a href="//medium.com/p/f46dc076886a/share/facebook?source=post_actions_header---------------------------" class="fj fk br bs bt bu bv bw bx by gl gm cb cc fm fn" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gn r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fmakzan-scrapbook%2Freactjs-examples-f46dc076886a&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="fj fk br bs bt bu bv bw bx by gl gm cb cc fm fn" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="3bd7" class="go gp ed bj gq b gr gs gt gu gv gw gx gy gz ha hb dv">I’m writing some ReactJS examples to demonstrate how we use React as the view rendering library. It also shows how we do data-view separation.</p><p id="bb10" class="go gp ed bj gq b gr gs gt gu gv gw gx gy gz ha hb dv">You can find the examples in the following CodePen collection:</p><p id="4b00" class="go gp ed bj gq b gr gs gt gu gv gw gx gy gz ha hb dv"><a href="http://codepen.io/collection/XwaeGM/" class="fj co hc hd he hf" target="_blank" rel="noopener nofollow">http://codepen.io/collection/XwaeGM/</a></p><figure class="hh hi hj hk hl hm dm dn paragraph-image"><div class="dm dn hg"><div class="hs r ht hu"><div class="hv r"><div class="hn ho dq t u hp ak az hq hr"><img class="dq t u hp ak hw hx hy" src="https://miro.medium.com/max/58/0*LiuEqORBLJqLScO5.jpg?q=20" width="640" height="665" role="presentation"></div><img class="hn ho dq t u hp ak hz" width="640" height="665" role="presentation"><noscript><img class="dq t u hp ak" src="https://miro.medium.com/max/1280/0*LiuEqORBLJqLScO5.jpg" width="640" height="665" role="presentation"></noscript></div></div></div></figure></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="b71f" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">Need to know about the scrapping a car</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@WeBuyCarsToday?source=post_page-----700d60394613----------------------"><img alt="We Buy Cars Today" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/1*qbb2wzil0UVTI9AEo0DJvA.jpeg" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@WeBuyCarsToday?source=post_page-----700d60394613----------------------">We Buy Cars Today</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@WeBuyCarsToday/need-to-know-about-the-scrapping-a-car-700d60394613?source=post_page-----700d60394613----------------------">Mar 16, 2018</a> <!-- -->·<!-- --> <!-- -->1<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/700d60394613/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/700d60394613/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40WeBuyCarsToday%2Fneed-to-know-about-the-scrapping-a-car-700d60394613&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><div class="fm fn fo fp fq fr"><a href="http://buycarstoday.blogspot.com/2018/03/how-and-where-to-sell-car.html" rel="noopener nofollow"><div class="fu n ar"><div class="fv n co p fw fx"><h2 class="bj fy fz bl dc"><div class="eh fs ej ek ft em">Get to know where to sell your car with no difficulty</div></h2><div class="ga r"><h3 class="bj ef eg bl bo"><div class="eh fs ej ek ft em">Selling a car can be such a pain if you do not explore the web and not find out what to do with the car, how to sell it…</div></h3></div><div class="gb r"><h4 class="bj ef fb bl bo"><div class="eh fs ej ek ft em">buycarstoday.blogspot.com</div></h4></div></div></div></a></div><p id="2e24" class="gc gd dc bk ge b gf gg gh gi gj gk gl gm gn go gp cu">Selling a car can be such a pain if you do not explore the web and not find out what to do with the car, how to sell it and what price to demand the car itself.</p></div></div></section></div><div><div class="cp u cq cr cs ct"></div><section class="cu cv cw cx cy"><div class="n p"><div class="ac ae af ag ah cz aj ak"><div><div id="4085" class="da db dc bk dd b de df dg dh di dj dk dl dm dn do"><h1 class="dd b de dp dg dq di dr dk ds dm dt dc">How to link preview like Facebook, Twitter, Slack, and WhatsApp</h1></div><div class="du"><div class="n dv dw dx dy"><div class="o n"><div><a rel="noopener" href="/@adoolak?source=post_page-----549381fef40a----------------------"><img alt="Adel ak" class="r dz ea eb" src="https://miro.medium.com/fit/c/96/96/0*sSbgQI54wZC-y2Ki" width="48" height="48"></a></div><div class="ec ak r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r dc q"><div class="ed n o ee"><span class="bj ef eg bl eh ei ej ek el em dc"><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@adoolak?source=post_page-----549381fef40a----------------------">Adel ak</a></span><div class="eo r ar h"><button class="ep dc q by eq er es et bc bh eu ev ew ex ey ez cb bj b bk fa fb bn cc cd ce cf cg bf">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj ef eg bl eh ei ej ek el em bo"><div><a class="at au av aw ax ay az ba bb bc en bf bg bh bi" rel="noopener" href="/@adoolak/how-to-link-preview-like-facebook-twitter-slack-and-whatsapp-549381fef40a?source=post_page-----549381fef40a----------------------">Jan 10</a> <!-- -->·<!-- --> <!-- -->10<!-- --> min read</div></span></span></div></div><div class="n fc fd fe ff fg fh fi fj ab"><div class="n o"><div class="fk r ar"><a href="//medium.com/p/549381fef40a/share/twitter?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="fk r ar"><a href="//medium.com/p/549381fef40a/share/facebook?source=post_actions_header---------------------------" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="fl r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40adoolak%2Fhow-to-link-preview-like-facebook-twitter-slack-and-whatsapp-549381fef40a&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="at au av aw ax ay az ba bb bc bd be bf bg bh bi" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div><div class="fm"><div class="n p"><div class="fn fo fp fq fr fs ag ft ah fu aj ak"><figure class="fw fx fy fz ga fm gb gc paragraph-image"><div class="gd ge gf gg ak"><div class="cl cm fv"><div class="gm r gf gn"><div class="go r"><div class="gh gi cp t u gj ak eh gk gl"><img class="cp t u gj ak gp gq gr" src="https://miro.medium.com/max/60/1*IRKtkTpZ5B4ZXU0aLuQ9GA.jpeg?q=20" width="2952" height="1080" role="presentation"></div><img class="gh gi cp t u gj ak gs" width="2952" height="1080" role="presentation"><noscript><img class="cp t u gj ak" src="https://miro.medium.com/max/5904/1*IRKtkTpZ5B4ZXU0aLuQ9GA.jpeg" width="2952" height="1080" role="presentation"></noscript></div></div></div></div><figcaption class="bo eg gt gu gv cn cl cm gw gx bj ef">Link previews</figcaption></figure></div></div></div><div class="n p"><div class="ac ae af ag ah cz aj ak"><p id="6380" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Have you ever wondered how do web applications preview a link once you’ve posted it on your timeline or send a message ?, I’ve been to the sun and back multiple times trying to figure it out.</p><p id="bc50" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">I had many questions that needed to be answered, but it was either no one understood what I asked or I was asking the wrong questions.</p><p id="4c22" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Worrest answers I’ve received were “you can use a web scraper API tool to achieve it, that’s what I used in my project”.</p><p id="494d" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Services like</p><ul class=""><li id="870d" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl hm hn ho"><a href="https://www.linkpreview.net/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Linkpreview</a></li><li id="04e2" class="gy gz dc bk ha b hb ht hd hu hf hv hh hw hj hx hl hm hn ho"><a href="https://www.scraperapi.com/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Scraperapi</a></li><li id="6083" class="gy gz dc bk ha b hb ht hd hu hf hv hh hw hj hx hl hm hn ho"><a href="https://www.scrapesimple.com/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Scrapesimple</a></li><li id="a447" class="gy gz dc bk ha b hb ht hd hu hf hv hh hw hj hx hl hm hn ho"><a href="https://guteurls.de/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">Guteurls</a></li></ul><p id="9adf" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">A few more…..</p><p id="ce54" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Until one day I met a guardian angel and I was introduced to <a href="https://ogp.me/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">open graph protocol</a>.</p><p id="ab45" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Thank you, Emma 🤗.</p><p id="03f1" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">FYI — The correct word for what we are doing is called web scraping</p><h1 id="c978" class="hy hz dc bk bj ia de ib dg ic id ie if ig ih ii ij">What is open graph protocol ?</h1><blockquote class="ik il im"><p id="7ec7" class="gy gz dc in ha b hb hc hd he hf hg hh hi hj hk hl cu">The Open Graph protocol enables any web page to become a rich object in a social graph. For instance, this is used on Facebook to allow any web page to have the same functionality as any other object on Facebook.</p><p id="68b6" class="gy gz dc in ha b hb hc hd he hf hg hh hi hj hk hl cu">~ Someone from <a href="https://ogp.me/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">https://ogp.me/</a></p></blockquote><p id="abc7" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">In short, it describes a website with objects like title, description, images, and more with <code class="gn io ip iq ir b">&lt;meta&gt;</code> tags.</p><p id="21f3" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">I’m not here to talk about open graph protocol, I’m here to show you how to fetch those data to make your own link preview, so if you want to know more about OGP, here are a couple of links.</p><p id="e207" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">FYI — Twitter has its own meta tag, but they use the “twitter” prefix instead of “og”</p><div class="is it iu iv iw ix"><a href="https://www.computerhope.com/jargon/o/open-graph.htm" rel="noopener nofollow"><div class="ja n ar"><div class="jb n co p jc jd"><h2 class="bj ia je bl dc"><div class="eh iy ej ek iz em">What is Open Graph?</div></h2><div class="jf r"><h3 class="bj ef eg bl bo"><div class="eh iy ej ek iz em">Open Graph is a technology first introduced by Facebook in 2010 that allows integration between Facebook and its user…</div></h3></div><div class="jg r"><h4 class="bj ef fb bl bo"><div class="eh iy ej ek iz em">www.computerhope.com</div></h4></div></div><div class="jh r"><div class="ji r jj jk jl jh jm jn jo"></div></div></div></a></div><div class="is it iu iv iw ix"><a rel="noopener" href="/better-programming/getting-meta-why-does-my-social-post-not-show-an-image-when-i-share-a-link-316b35b311e2"><div class="ja n ar"><div class="jb n co p jc jd"><h2 class="bj ia je bl dc"><div class="eh iy ej ek iz em">Getting Meta: Why Does My Social Post Not Show an Image When I Share a Link?</div></h2><div class="jf r"><h3 class="bj ef eg bl bo"><div class="eh iy ej ek iz em">How to fix this with The Open Graph protocol</div></h3></div><div class="jg r"><h4 class="bj ef fb bl bo"><div class="eh iy ej ek iz em">medium.com</div></h4></div></div><div class="jh r"><div class="jp r jj jk jl jh jm jn jo"></div></div></div></a></div><div class="is it iu iv iw ix"><a href="https://ogp.me/" rel="noopener nofollow"><div class="ja n ar"><div class="jb n co p jc jd"><h2 class="bj ia je bl dc"><div class="eh iy ej ek iz em">Open Graph protocol</div></h2><div class="jf r"><h3 class="bj ef eg bl bo"><div class="eh iy ej ek iz em">The Open Graph protocol enables any web page to become a rich object in a social graph. For instance, this is used on…</div></h3></div><div class="jg r"><h4 class="bj ef fb bl bo"><div class="eh iy ej ek iz em">ogp.me</div></h4></div></div><div class="jh r"><div class="jq r jj jk jl jh jm jn jo"></div></div></div></a></div><div class="is it iu iv iw ix"><a href="https://css-tricks.com/essential-meta-tags-social-media/" rel="noopener nofollow"><div class="ja n ar"><div class="jb n co p jc jd"><h2 class="bj ia je bl dc"><div class="eh iy ej ek iz em">The Essential Meta Tags for Social Media | CSS-Tricks</div></h2><div class="jf r"><h3 class="bj ef eg bl bo"><div class="eh iy ej ek iz em">These days, almost every website encourages visitors to share its pages on social media. We’ve all seen the ubiquitous…</div></h3></div><div class="jg r"><h4 class="bj ef fb bl bo"><div class="eh iy ej ek iz em">css-tricks.com</div></h4></div></div><div class="jh r"><div class="jr r jj jk jl jh jm jn jo"></div></div></div></a></div><h1 id="07ef" class="hy hz dc bk bj ia de ib dg ic id ie if ig ih ii ij">How do we do it ?</h1><p id="0a7a" class="gy gz dc bk ha b hb js hd jt hf ju hh jv hj jw hl cu">It’s a simple process and doesn’t require much work, we will fetch the web page as text in our Node.js application. Then we will select the HTML elements we need and get the data/text it holds, save it to a JSON file then send the data back.</p><p id="ce08" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">“But how can we select the dom from the back end Adel ?”</p><p id="0537" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Easy, with the help of cheerio and other modules like it, cheerio is a <em class="in">Fast, flexible, and lean implementation of core jQuery designed specifically for the server.</em></p><h1 id="d439" class="hy hz dc bk bj ia de ib dg ic id ie if ig ih ii ij">Can we do it on the front end ?</h1><p id="00e8" class="gy gz dc bk ha b hb js hd jt hf ju hh jv hj jw hl cu">As far as I know, you cant, this cant be done in the front end script, when you try to fetch eg my portfolio or any other site in chrome’s console, it will throw a cors (Cross-Origin Resource Sharing) error.</p><figure class="fw fx fy fz ga fm cl cm paragraph-image"><div class="cl cm jx"><div class="gm r gf gn"><div class="jy r"><div class="gh gi cp t u gj ak eh gk gl"><img class="cp t u gj ak gp gq gr" src="https://miro.medium.com/max/60/1*rAU9xQYpm2MTc8ZMASHWNA.jpeg?q=20" width="522" height="310" role="presentation"></div><img class="gh gi cp t u gj ak gs" width="522" height="310" role="presentation"><noscript><img class="cp t u gj ak" src="https://miro.medium.com/max/1044/1*rAU9xQYpm2MTc8ZMASHWNA.jpeg" width="522" height="310" role="presentation"></noscript></div></div></div></figure><p id="9093" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">To bypass this issue, we will send the URL to the back end server, process the request then send back the data we scrapped.</p><h1 id="fc59" class="hy hz dc bk bj ia de ib dg ic id ie if ig ih ii ij">Prerequisite</h1><ul class=""><li id="bc00" class="gy gz dc bk ha b hb js hd jt hf ju hh jv hj jw hl hm hn ho">JQuery, if you know how to select an element and get its values, your good.</li><li id="ed9a" class="gy gz dc bk ha b hb ht hd hu hf hv hh hw hj hx hl hm hn ho">Async/Await</li><li id="5868" class="gy gz dc bk ha b hb ht hd hu hf hv hh hw hj hx hl hm hn ho">NodeJS/ExpressJS</li></ul><h1 id="3638" class="hy hz dc bk bj ia de ib dg ic id ie if ig ih ii ij">OK LETS CODE!!!</h1><p id="e0ca" class="gy gz dc bk ha b hb js hd jt hf ju hh jv hj jw hl cu">If you want to tag along, I’ve got starter files you can clone/download, and I’ll be adding the completed files too.</p><div class="is it iu iv iw ix"><a href="https://github.com/Adel-ak/web-scraping-101" rel="noopener nofollow"><div class="ja n ar"><div class="jb n co p jc jd"><h2 class="bj ia je bl dc"><div class="eh iy ej ek iz em">Adel-ak/web-scraping-101</div></h2><div class="jf r"><h3 class="bj ef eg bl bo"><div class="eh iy ej ek iz em">You can’t perform that action at this time. You signed in with another tab or window. You signed out in another tab or…</div></h3></div><div class="jg r"><h4 class="bj ef fb bl bo"><div class="eh iy ej ek iz em">github.com</div></h4></div></div><div class="jh r"><div class="jz r jj jk jl jh jm jn jo"></div></div></div></a></div><p id="d978" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">1 — Get to know the front end script</strong></p><p id="b5a5" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">In our front end script located in the public/javascript folder has a fairly small amount of code in it, we have a click event listener on our add button, which will</p><ul class=""><li id="a5c0" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl hm hn ho">prepend a loading preview card with an id.</li><li id="d962" class="gy gz dc bk ha b hb ht hd hu hf hv hh hw hj hx hl hm hn ho">send a post request to the backend with the URL link and the card id which was added to the page.</li><li id="7bce" class="gy gz dc bk ha b hb ht hd hu hf hv hh hw hj hx hl hm hn ho">await for the data to come back, then add the data to the correct preview card but searching the id of the card.</li></ul><pre class="fw fx fy fz ga kb kc kd"><span id="82dd" class="ke hz dc bk ir b eg kf kg r kh">const addButton = document.querySelector('button');</span><span id="4aa3" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">addButton.addEventListener('click', async (e) =&gt; {</span><span id="312c" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">e.preventDefault();</span><span id="4f67" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">try {<br>    let urlInput = document.querySelector('input');<br>    const id = await uuid();<br>    const fetchData = {<br>             method: 'POST',<br>             headers: {<br>                 'Content-Type': 'application/json;charset=utf-8'<br>             },<br>            body: JSON.stringify({ previewUrl: urlInput.value, id })<br>    };</span><span id="637d" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">urlInput.value = '';</span><span id="c983" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">prependLoadingPreview(id);</span><span id="4571" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">const data = await fetch('/get-preview', fetchData)<br>     .then(res =&gt; res.json());<br>     <br>     addDataToPreview(data);</span><span id="a579" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">} catch (err) {</span><span id="6b58" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">console.log(err);</span><span id="c99e" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">}<br>});</span></pre><p id="9eb8" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">This function accepts an id and will add the loading preview to the unordered list,</p><pre class="fw fx fy fz ga kb kc kd"><span id="1f4e" class="ke hz dc bk ir b eg kf kg r kh">function prependLoadingPreview(id) {</span><span id="b7f5" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">document.querySelector(`ul`).prepend(document.createRange()<br>  .createContextualFragment(`&lt;li class="preview-container loading" data-id="${id}"&gt;&lt;/li&gt;`,'text/html'));</span><span id="08a1" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">}</span></pre><p id="f398" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">This function will receive an object, it will get the loading list by its id which was previously added, remove the loading class name then append the data</p><pre class="fw fx fy fz ga kb kc kd"><span id="0f2e" class="ke hz dc bk ir b eg kf kg r kh">Object example<br>{<br>  id: "a60d491d-8c70-4620-aa18-111ae1abaea9", <br>  url: "https://www.adelak.me", <br>  img:"https://www.npmjs.com/package/mongoose", <br>  title: "Mongoose", <br>  description: "Mongoose is a <a href="https://www.mongodb.org/" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">MongoDB</a> object modeling tool designed to work in an asynchronous environment. Mongoose supports both promises and callbacks.", <br>  domain: "npmjs.com"<br>}</span><span id="cf46" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">function addDataToPreview({ id, url, img, title, description, domain }) {</span><span id="2057" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">const li = document.querySelector(`li[data-id="${id}"]`);</span><span id="de8a" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">li.classList.remove('loading');</span><span id="637a" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">li.innerHTML =</span><span id="02fc" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">`&lt;svg class="delete-button" viewBox="0 0 24 24" onClick="removePreview(this)"&gt;&lt;polygon points="17.8,16.7 16.6,17.9 12,13.3 7.4,17.9 6.2,16.7 10.8,12.1 6.2,7.5 7.4,6.3 12,11 16.6,6.4 17.8,7.6 13.2,12.2"&gt;&lt;/polygon&gt;&lt;/svg&gt;<br>&lt;a href="${url}" target="_blank"&gt;&lt;img class="preview-image" src="${img}" alt="preview image" onError="imgError(this)"/&gt;&lt;div class="preview-info"&gt;&lt;h5 class="preview-title"&gt;${title}&lt;/h5&gt;&lt;p class="preview-description"&gt;${description}&lt;/p&gt;&lt;span class="preview-url"&gt;${domain}&lt;/span&gt;&lt;/div&gt;&lt;/a&gt;`;</span><span id="3361" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">}</span></pre><p id="5364" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">This function removes a preview card</p><pre class="fw fx fy fz ga kb kc kd"><span id="d09f" class="ke hz dc bk ir b eg kf kg r kh">const removePreview = async (e) =&gt; {<br>  try{</span><span id="888c" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">let li = e.parentElement;</span><span id="86f0" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">li.parentElement.removeChild(li);</span><span id="ec42" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">const id = li.getAttribute('data-id');</span><span id="49e2" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">await fetch(`/remove/${id}`, { method: 'POST' });<br>  <br>  }catch(err){</span><span id="9394" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">console.log(err);</span><span id="4c99" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">}<br>}</span></pre><p id="1967" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">The UUID CDN was making the app load super slow, thanks to <a href="https://stackoverflow.com/users/109538" class="at cg hp hq hr hs" target="_blank" rel="noopener nofollow">broofa</a> who came up with this function, it will be creating our unique id for each preview card</p><pre class="fw fx fy fz ga kb kc kd"><span id="6f62" class="ke hz dc bk ir b eg kf kg r kh">function uuid(){</span><span id="07c5" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">let date = new Date().getTime();</span><span id="a21e" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">const randomStrings = (c) =&gt; {</span><span id="3859" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">const r = (date + Math.random()*16)%16 | 0;</span><span id="6a46" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">date = Math.floor(date/16);</span><span id="ce46" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">return (c=='x' ? r :(r&amp;0x3|0x8)).toString(16);</span><span id="a021" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">}</span><span id="466d" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">const id = 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, randomStrings);</span><span id="b037" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">return id;</span><span id="3d4f" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">}</span></pre><p id="c4dc" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">2 — Installing modules</strong></p><p id="f784" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">We need to install a few modules.</p><p id="e925" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">Cheerio </strong>to<strong class="ha ka"> </strong>load the source code of the webpage we want to crawl.</p><p id="fb8d" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">ExpressJS</strong> to create our HTTP server.</p><p id="c0cc" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">Express-handlebars</strong> a template engine that makes writing HTML code easier and renders out page.</p><p id="4ece" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">Node-fetch </strong>to make our HTTP request in node.js.</p><p id="4df4" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">I’ve added these modules to the dependencies, simply install them by running <code class="gn io ip iq ir b">npm i</code> in the command line.</p><p id="1469" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">3 — Creating our server</strong></p><p id="0e13" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Over at app.js, we have requested all our modules, set up our view engine and middlewares.</p><p id="5fd7" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">We can start by creating the home route, which will render the home temple and passing it the data in <code class="gn io ip iq ir b">data.json</code> (currently, <code class="gn io ip iq ir b">data,json</code>is empty).</p><pre class="fw fx fy fz ga kb kc kd"><span id="ebd3" class="ke hz dc bk ir b eg kf kg r kh">app.get('/', function(req, res) {<br>   res.render('home.handlebars', { data });<br>});</span></pre><p id="7e2b" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Open up a command line and run npm start, then in your browser open up localhost://3000.</p><p id="f120" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">You should get an empty home page, with just an input filed.</p><figure class="fw fx fy fz ga fm cl cm paragraph-image"><div class="gd ge gf gg ak"><div class="cl cm kn"><div class="gm r gf gn"><div class="ko r"><div class="gh gi cp t u gj ak eh gk gl"><img class="cp t u gj ak gp gq gr" src="https://miro.medium.com/max/60/1*EyrgqyH5r_ALEqdbfXhBVA.jpeg?q=20" width="1003" height="695" role="presentation"></div><img class="gh gi cp t u gj ak gs" width="1003" height="695" role="presentation"><noscript><img class="cp t u gj ak" src="https://miro.medium.com/max/2006/1*EyrgqyH5r_ALEqdbfXhBVA.jpeg" width="1003" height="695" role="presentation"></noscript></div></div></div></div></figure><p id="76e8" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Now we work on fetching the metadata we want to get from a web page, let’s create a post route to receive the id and URL from the front end once the add button is clicked.</p><p id="d6bc" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">In the request body, we are expecting a value from previewUrl and id</p><pre class="fw fx fy fz ga kb kc kd"><span id="3822" class="ke hz dc bk ir b eg kf kg r kh">app.post('/get-preview',(req, res) =&gt; {<br>  const { previewUrl, id } = req.body;<br>  console.log(previewUrl); //<a class="at cg hp hq hr hs" target="_blank" rel="noopener" href="/@adoolak/how-to-deploy-a-reactjs-and-express-app-to-heroku-afb5b117e0eb">https://medium.com/@adoolak/how-to-deploy-a-reactjs-and-express-app-to-heroku-afb5b117e0eb</a><br>  console.log(id);//"a60d491d-8c70-4620-aa18-111ae1abaea9"<br>});</span></pre><p id="e6e0" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Let’s work on fetching the HTML page from my last medium post.</p><p id="ffc5" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Make the anonymous function into an async/await function, and use the fetch API from the node-fetch module, then create a variable called html and give it the value of the fetch method (make sure you use the await keyword, to wait for a result from the fetch), pass it the previewUrl value from the request body, then chain a <code class="gn io ip iq ir b">.then(res =&gt; res.text())</code> to it.</p><p id="c972" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Next, we use cheerio, remember cheerio is an <em class="in">implementation of </em>core jquery for the server side.</p><p id="67ab" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Create a variable with the $ sign and give it the value of <code class="gn io ip iq ir b">cheerio.load(), </code>pass the html variable to the load method, you can now try and select an html element using the $ sign.</p><pre class="fw fx fy fz ga kb kc kd"><span id="7303" class="ke hz dc bk ir b eg kf kg r kh">app.post('/get-preview',async (req, res) =&gt; {<br>  const { previewUrl, id } = req.body;<br>  const html = await fetch(previewUrl).then(res =&gt; res.text());<br>  const $ = cheerio.load(html);<br>  console.log($('h1').text()) //How to deploy a Reactjs and Express App to Heroku</span><span id="be91" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">});</span></pre><p id="863c" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">We can now start getting the meta tags we want, create a variable named <code class="gn io ip iq ir b">metaTagData</code> which will hold an object of the data,</p><p id="d2a6" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">id</strong> — we will pass in the id from <code class="gn io ip iq ir b">req.body</code> to the object,</p><p id="3b0e" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">url</strong> — the web site url to the url key.</p><p id="6528" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">domain</strong> — For the domain we just need the domain name of the previewUrl, we can use the url module from nodejs to get the hostname.</p><p id="0cef" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">title</strong> — use cheerio to select the meta tag with the attribute of <code class="gn io ip iq ir b">name="title"</code></p><p id="3cde" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">img</strong>— use cheerio to select the meta tag with the attribute of <code class="gn io ip iq ir b">name="title"</code></p><p id="2d0a" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">description</strong>— use cheerio to select the meta tag with the attribute of <code class="gn io ip iq ir b">name="description"</code> and get the attribute of <code class="gn io ip iq ir b">content</code> .</p><p id="16eb" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">The meta tags have another attribute called <code class="gn io ip iq ir b">content</code> that's where the values are stored, to get the values, you need to chain the cheerio selectors with the <code class="gn io ip iq ir b">attr</code> method and pass it the string of <code class="gn io ip iq ir b">content</code> .</p><p id="0a77" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">You should end with an object like this.</p><pre class="fw fx fy fz ga kb kc kd"><span id="5eea" class="ke hz dc bk ir b eg kf kg r kh">const metaTagData = {<br>  id:id,<br>  url: previewUrl,<br>  domain: url.parse(previewUrl).hostname,<br>  title: $('meta[name="title"]').attr('content'),<br>  img: $('meta[name="image"]').attr('content'),<br>  description: $('meta[name="description"]').attr('content'),<br>}</span></pre><p id="d8d6" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Now, this should do it, but some web pages use a basic html meta tag, some use open graph, some use twitter cards, some use the property attribute instead of the name attribute, some don't add a image meta tag, we can basically end up with missing data or no data at all.</p><p id="3e33" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">Solution</strong></p><p id="0e24" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Creating a function which will return the first thing it finds</p><pre class="fw fx fy fz ga kb kc kd"><span id="eb2c" class="ke hz dc bk ir b eg kf kg r kh">const getMetaRag = (name) =&gt;  {</span><span id="ac58" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">  return(<br>    $(`meta[name=${name}]`).attr('content') ||<br>    $(`meta[name="og:${name}"]`).attr('content') ||<br>    $(`meta[name="twitter:${name}"]`).attr('content') ||<br>    $(`meta[property=${name}]`).attr('content') ||<br>    $(`meta[property="og:${name}"]`).attr('content') ||<br>    $(`meta[property="twitter:${name}"]`).attr('content')<br>  );</span><span id="066f" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">}</span></pre><p id="069a" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">We can now change the value of title, img and description of our metaTagData object to the getMetaTag function and pass it a the meta tag name as a string.</p><p id="8ca9" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">And what if a web page doesn't use meta tags at all ?</p><p id="fa56" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">We add a fallback value on our title, img and description keys.</p><pre class="fw fx fy fz ga kb kc kd"><span id="3b6b" class="ke hz dc bk ir b eg kf kg r kh">const metaTagData = {<br>  id:id,<br>  url: previewUrl,<br>  domain: url.parse(previewUrl).hostname,<br>  title: getMetaTag('title') || $(`h1`).text(),<br>  img: getMetaTag('image') || './images/no-image.png',<br>  description: getMetaTag('description') || $(`p`).text(),<br>}</span></pre><p id="99f3" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">title</strong> — will fall back to the first h1 tag on the page</p><p id="8241" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">img</strong> — will fall back to an image in the public/images folder</p><p id="d77e" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">description</strong> — will fall back to the first paragraph tag on the page</p><p id="fbfe" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Some descriptions can get a bit lengthy, I decided to keep all descriptions at a max of 200 character count.</p><pre class="fw fx fy fz ga kb kc kd"><span id="ca77" class="ke hz dc bk ir b eg kf kg r kh">let { description } = metaTagData;</span><span id="c230" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">if(description.length &gt; 200){<br> metaTagData.description = description.substring(0,200) + '...';<br>}</span></pre><p id="3db7" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Next, we push the data to the beginning of the data array, using the unshift array method, then write it to the <code class="gn io ip iq ir b">data.json</code> file using the <code class="gn io ip iq ir b">writeFile</code> from the fs (file system) nodejs module.</p><pre class="fw fx fy fz ga kb kc kd"><span id="7064" class="ke hz dc bk ir b eg kf kg r kh">data.unshift(metaTagData);</span><span id="3f8b" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">fs.writeFile("./data.json", JSON.stringify(data, null, 2),()=&gt;{<br>  res.status(201).json(data.shift());<br>});</span></pre><p id="bb56" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">The first parameter of the <code class="gn io ip iq ir b">writeFile</code> method takes in the file location, the second parameter we pass in the data we want to write to the file, since its a JSON file we need to stringify the data using the<code class="gn io ip iq ir b">JSON.stringify</code> method, the third parameter takes in a call back function, where we respond back with JSON and passing it the data using the shift array method and also set the HTTP status to 201.</p><p id="e09c" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">Test Run!</strong></p><p id="a598" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">If you start your app, and past a link of any web page then click on add, you should end up with this.</p><figure class="fw fx fy fz ga fm cl cm paragraph-image"><div class="gd ge gf gg ak"><div class="cl cm kp"><div class="gm r gf gn"><div class="kq r"><div class="gh gi cp t u gj ak eh gk gl"><img class="cp t u gj ak gp gq gr" src="https://miro.medium.com/freeze/max/60/1*UQiIXwzOZvzCzpIRAj4hCQ.gif?q=20" width="990" height="928" role="presentation"></div><img class="gh gi cp t u gj ak gs" width="990" height="928" role="presentation"><noscript><img class="cp t u gj ak" src="https://miro.medium.com/max/1980/1*UQiIXwzOZvzCzpIRAj4hCQ.gif" width="990" height="928" role="presentation"></noscript></div></div></div></div></figure><p id="acba" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">Deleting the card</strong></p><p id="959f" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">To remove a card, create another post route which will accept an ID from the URL parameter, create a variable named indexOfId and the value you map over the data json array and return the just the id of each object, then chaining the array method <code class="gn io ip iq ir b">indexOf()</code> to the map, will give you the exact position of the id you want to remove from the array (make sure you pass in the id from the url parameter to the indexOf method).</p><p id="590a" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Next, we use the splice array method to remove the data from the data json array and passing the first parameter the <code class="gn io ip iq ir b">indexOfId</code> variable and second parameter the value <code class="gn io ip iq ir b">1</code> , indicating we want to remove just the object from the array.</p><p id="4a89" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Then we use the fs nodejs module to rewrite the new edited data to the <code class="gn io ip iq ir b">data.json</code> file, and respond back with a status of 200 and using the respond <code class="gn io ip iq ir b">end()</code> to end the request.</p><pre class="fw fx fy fz ga kb kc kd"><span id="8e6d" class="ke hz dc bk ir b eg kf kg r kh">app.post('/remove/:id', (req, res) =&gt; {</span><span id="a4bc" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">  const { id } = req.params;</span><span id="abf2" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">  const indexOfId = data.map(dataId =&gt; dataId.id)</span><span id="e84e" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">.indexOf(id);</span><span id="c9a5" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">  data.splice(indexOfId,1);</span><span id="9698" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">  fs.writeFile("./data.json", JSON.stringify(data, null, 2), () =&gt; (</span><span id="7603" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">  res.status(200).end()</span><span id="3d07" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">  ));</span><span id="2bd7" class="ke hz dc bk ir b eg ki kj kk kl km kg r kh">});</span></pre><p id="8f80" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka">Test Run!</strong></p><p id="8450" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">If you try and remove a card, then refresh the page, the removed cards will it will no longer be there.</p><figure class="fw fx fy fz ga fm cl cm paragraph-image"><div class="gd ge gf gg ak"><div class="cl cm kp"><div class="gm r gf gn"><div class="kq r"><div class="gh gi cp t u gj ak eh gk gl"><img class="cp t u gj ak gp gq gr" src="https://miro.medium.com/freeze/max/60/1*BgEhnOulIwCpfOkp78Qawg.gif?q=20" width="990" height="928" role="presentation"></div><img class="gh gi cp t u gj ak gs" width="990" height="928" role="presentation"><noscript><img class="cp t u gj ak" src="https://miro.medium.com/max/1980/1*BgEhnOulIwCpfOkp78Qawg.gif" width="990" height="928" role="presentation"></noscript></div></div></div></div></figure><h2 id="bc3f" class="ke hz dc bk bj ia kr ks kt ku kv kw kx ky kz la lb">End Of The Road!!</h2><h2 id="6773" class="ke hz dc bk bj ia kr ks kt ku kv kw kx ky kz la lb">Conclusion</h2><p id="60e3" class="gy gz dc bk ha b hb js hd jt hf ju hh jv hj jw hl cu">We learned how to create a link preview by web scraping meta tags, but with the power of web scraping, you can do more than scrap meta tags.</p><p id="4751" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">Take Adidas as an example, they don't prove an API for their product, images, prices, etc…, and you want to create an eCommerce side project.</p><p id="cb34" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">You can go to their web page and start scraping the products, but if a web page like adidas.com uses react, angular or vue, it can get complicated to web scrap.</p><p id="b9e4" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu">You will need to use a headless browser to get around scraping that kind of web sites.</p><p id="e448" class="gy gz dc bk ha b hb hc hd he hf hg hh hi hj hk hl cu"><strong class="ha ka"><em class="in">!!! Watch out though, it is illegal to scrap some web sites !!!</em></strong></p><h2 id="7e3a" class="ke hz dc bk bj ia kr ks kt ku kv kw kx ky kz la lb">Headless browser resources</h2><div class="is it iu iv iw ix"><a rel="noopener" href="/swlh/an-introduction-to-web-scraping-with-puppeteer-3d35a51fdca0"><div class="ja n ar"><div class="jb n co p jc jd"><h2 class="bj ia je bl dc"><div class="eh iy ej ek iz em">An Introduction to Web Scraping with Puppeteer</div></h2><div class="jf r"><h3 class="bj ef eg bl bo"><div class="eh iy ej ek iz em">Learn Puppeteer with me in this article.</div></h3></div><div class="jg r"><h4 class="bj ef fb bl bo"><div class="eh iy ej ek iz em">medium.com</div></h4></div></div><div class="jh r"><div class="lc r jj jk jl jh jm jn jo"></div></div></div></a></div><div class="is it iu iv iw ix"><a href="https://codeburst.io/a-guide-to-automating-scraping-the-web-with-javascript-chrome-puppeteer-node-js-b18efb9e9921" rel="noopener"><div class="ja n ar"><div class="jb n co p jc jd"><h2 class="bj ia je bl dc"><div class="eh iy ej ek iz em">A Guide to Automating &amp; Scraping the Web with JavaScript (Chrome + Puppeteer + Node JS)</div></h2><div class="jf r"><h3 class="bj ef eg bl bo"><div class="eh iy ej ek iz em">Learn to Automate and Scrape the web with Headless Chrome</div></h3></div><div class="jg r"><h4 class="bj ef fb bl bo"><div class="eh iy ej ek iz em">codeburst.io</div></h4></div></div><div class="jh r"><div class="ld r jj jk jl jh jm jn jo"></div></div></div></a></div><div class="is it iu iv iw ix"><a rel="noopener" href="/@lynzt/scraping-using-zombie-js-8fb2877348fd"><div class="ja n ar"><div class="jb n co p jc jd"><h2 class="bj ia je bl dc"><div class="eh iy ej ek iz em">Scraping using zombie.js</div></h2><div class="jf r"><h3 class="bj ef eg bl bo"><div class="eh iy ej ek iz em">I’m using zombie for testing and want to pull back a list of anchor tags (so not really scraping but a common thing you…</div></h3></div><div class="jg r"><h4 class="bj ef fb bl bo"><div class="eh iy ej ek iz em">medium.com</div></h4></div></div></div></a></div><blockquote class="ik il im"><p id="faa2" class="gy gz dc in ha b hb hc hd he hf hg hh hi hj hk hl cu">Got any questions ?<br>DM me in twitter @Adel_xoxo and I’ll answer to the best of my knowledge</p><p id="533e" class="gy gz dc in ha b hb hc hd he hf hg hh hi hj hk hl cu">~Adel ak</p></blockquote></div></div></section></div><div><div class="dq u dr ds dt du"></div><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><div><div id="54af" class="eb ec ed at ee b ef eg eh ei ej ek el em en eo ep"><h1 class="ee b ef eq eh er ej es el et en eu ed">How I took a break from Job Searching and let Python do it for me.</h1></div><div class="ev"><div class="n ew ex ey ez"><div class="o n"><div><a href="/@umangkshah?source=post_page-----25f05c0f3dea----------------------" rel="noopener"><img alt="Umang Shah" class="r fa fb fc" src="https://miro.medium.com/fit/c/96/96/0*VUv8Qy5XzcpPBUxA." width="48" height="48"></a></div><div class="fd ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r ed q"><div class="fe n o ff"><span class="as cv fg au cd fh fi fj fk fl ed"><a href="/@umangkshah?source=post_page-----25f05c0f3dea----------------------" class="da db bb bc bd be bf bg bh bi fm bl bm fn fo" rel="noopener">Umang Shah</a></span><div class="fp r ap h"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fcodeburst.io%2Fhow-i-took-a-break-from-job-searching-and-let-python-do-it-for-me-25f05c0f3dea&amp;source=-5cd646935234-------------------------follow_byline-" class="fq ed q bq fr fs ft fu bi fn fv fw fx fy fz ga bt as b at gb cw aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cv fg au cd fh fi fj fk fl ax"><div><a class="da db bb bc bd be bf bg bh bi fm bl bm fn fo" rel="noopener" href="/how-i-took-a-break-from-job-searching-and-let-python-do-it-for-me-25f05c0f3dea?source=post_page-----25f05c0f3dea----------------------">Dec 28, 2017</a> <!-- -->·<!-- --> <!-- -->4<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewbox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div><div class="n gc gd ge gf gg gh gi gj ab"><div class="n o"><div class="gk r ap"><a href="//medium.com/p/25f05c0f3dea/share/twitter?source=post_actions_header---------------------------" class="da db bb bc bd be bf bg bh bi gl gm bl bm fn fo" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gk r ap"><a href="//medium.com/p/25f05c0f3dea/share/facebook?source=post_actions_header---------------------------" class="da db bb bc bd be bf bg bh bi gl gm bl bm fn fo" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gn r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fcodeburst.io%2Fhow-i-took-a-break-from-job-searching-and-let-python-do-it-for-me-25f05c0f3dea&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="da db bb bc bd be bf bg bh bi gl gm bl bm fn fo" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div><p id="6fe1" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">Another day in the Winter break. The ever looming blade of getting a full time job compels me to go to Indeed.com. I use the Advanced Search set my preferences, location, radius around the location, job title, entry level, full time, how long ago the jobs was posted (I prioritize applying to jobs no older than 15 days) the usual stuff. Hit search and the nightmare begins.</p><figure class="hd he hf hg hh hi hj hk bv hl hm hn ho hp bg hq hr hs ht hu hv paragraph-image"><div class="hw hx hy hz ak"><div class="dm dn hc"><div class="if r hy ig"><div class="ih r"><div class="ia ib dq t u ic ak cd id ie"><img class="dq t u ic ak ii ij ik" src="https://miro.medium.com/max/60/1*Ms_Ub7T5hbxxaSA_ogk1Fg.png?q=20" width="1558" height="1008" role="presentation"></div><img class="ia ib dq t u ic ak hj" width="1558" height="1008" role="presentation"><noscript><img class="dq t u ic ak" src="https://miro.medium.com/max/3116/1*Ms_Ub7T5hbxxaSA_ogk1Fg.png" width="1558" height="1008" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg il im in do dm dn io ip as cv">Advanced Search does have good parameters.</figcaption></figure><p id="c2c6" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">Now, before I complain about Indeed for no reason. I consider it to be one of the best websites for job search — in fact, I got my summer internship through Indeed. It has a great collection of relevant jobs and a nice set of filtering options. But finding full time jobs is kind of a mess because of the wider range of job type that all fall under the same umbrella.</p><ul class=""><li id="4e7c" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb iq ir is">First, there’s almost way too many results, for this instance 375 to be precise. (which is also a good thing?)</li><li id="46b7" class="go gp ed at gq b gr it gt iu gv iv gx iw gz ix hb iq ir is">Second, even though I filter for entry level jobs, the description often has a minimum work ex requirement — while It may not necessarily be entry level = 0 work experience, but having jobs with requirement of 5+ years under entry level is doing no good to anyone.</li><li id="de11" class="go gp ed at gq b gr it gt iu gv iv gx iw gz ix hb iq ir is">Third, I would eventually stumble upon Intern positions — again not what I am looking for. (So Internship ⊆ Full Time? maybe but come on..)</li><li id="d4d4" class="go gp ed at gq b gr it gt iu gv iv gx iw gz ix hb iq ir is">And four, it takes too much time, I’ve already filtered and filtered and I still find results that do not work for me.</li></ul><p id="d833" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">If only I could do this entire chore of going through each and every job title, job description and eliminating unsuitable ones and consolidating the right ones in one place. Well, yes I can, with Python. I consider Python to be a great automation tool, with its rich set of libraries and intuitive syntax, be it data cleaning or arranging my desktop, it never lets me down.</p><p id="6768" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">First, task is to find out if at all the job title matches my need. But, how do I know where is what on the webpage and how will my code know that. Lucky for me, Indeed has a very well defined html page structure and I can leverage the semantic class and id tags. View the page source or simply “inspect element” on your choice of page element. And Voila!</p><figure class="hd he hf hg hh hi dm dn paragraph-image"><div class="hw hx hy hz ak"><div class="dm dn iy"><div class="if r hy ig"><div class="iz r"><div class="ia ib dq t u ic ak cd id ie"><img class="dq t u ic ak ii ij ik" src="https://miro.medium.com/max/60/1*Gr045Y6zJLC2wFuVyUyDNQ.png?q=20" width="2766" height="1230" role="presentation"></div><img class="ia ib dq t u ic ak hj" width="2766" height="1230" role="presentation"><noscript><img class="dq t u ic ak" src="https://miro.medium.com/max/5532/1*Gr045Y6zJLC2wFuVyUyDNQ.png" width="2766" height="1230" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg il im in do dm dn io ip as cv">Its all right there.</figcaption></figure><p id="9bec" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">If you look closer. All results have the class tag “result”. That’s great, something to begin coding. Lets head over to <em class="ja">atom. </em><strong class="gq jb">BeautifulSoup</strong> is a great tool for all things HTML. Plus, it is great for nested search of elements and attributes. Grab the URL,</p><p id="d34c" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv"><code class="ig jc jd je jf b">url_base = “<a href="https://www.indeed.com/jobs?q=software+engineer" class="da by jg jh ji jj" target="_blank" rel="noopener nofollow">https://www.indeed.com/jobs?q=software+engineer</a>…”</code></p><p id="14c5" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">load the page into a soup</p><pre class="hd he hf hg hh jk jl cm"><span id="a77b" class="jm jn ed at jf b fg jo jp r jq">try:<br>    search_page = ur.urlopen(url_base+"&amp;start="+str(pgno)).read()<br>    soup = BeautifulSoup(search_page, 'html.parser')</span></pre><p id="5c28" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">and lets search for that job. <em class="ja">(pgno is what lets me go through all the pages, but I’m leaving out the details.)</em></p><figure class="hd he hf hg hh hi dm dn paragraph-image"><div class="hw hx hy hz ak"><div class="dm dn jr"><div class="if r hy ig"><div class="js r"><div class="ia ib dq t u ic ak cd id ie"><img class="dq t u ic ak ii ij ik" src="https://miro.medium.com/max/60/1*g4-VRI5Q65ma7aup7dlEyg.png?q=20" width="2046" height="60" role="presentation"></div><img class="ia ib dq t u ic ak hj" width="2046" height="60" role="presentation"><noscript><img class="dq t u ic ak" src="https://miro.medium.com/max/4092/1*g4-VRI5Q65ma7aup7dlEyg.png" width="2046" height="60" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg il im in do dm dn io ip as cv">URL for search results is where I start. This already has all my filters applied.</figcaption></figure><p id="bcc9" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">Now, having seen the html layout. I can get the <em class="ja">div </em>holding the “result”.</p><p id="ec7d" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv"><code class="ig jc jd je jf b">for job in soup.find_all(class_=’result’):</code></p><p id="6938" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">I have access to the title, company, location, short description, salary if listed, and the URL for the job.</p><pre class="hd he hf hg hh jk jl cm"><span id="1378" class="jm jn ed at jf b fg jo jp r jq">  link = job.find(class_="turnstileLink").get('href')<br>  job_title = link.get('title')<br>  company_name = job.find(class_='company').get_text()</span></pre><p id="2a4f" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">All kinds of checks can be applied on these, like ignore anything with the work “intern” or “senior”. After doing some more filtering from this information I use the URL to get the job description and get <strong class="gq jb">regex </strong>to find out if its good for me. The regex here checks for 2 things :</p><ol class=""><li id="b95e" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb jt ir is">The required previous experience is not more than 1 year.</li><li id="4c23" class="go gp ed at gq b gr it gt iu gv iv gx iw gz ix hb jt ir is">It should not be limited to US Citizens.</li></ol><p id="27f9" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">If I find either of these in a description, it is thrown out as not suitable.</p><pre class="hd he hf hg hh jk jl cm"><span id="6228" class="jm jn ed at jf b fg jo jp r jq">p1 = re.compile('[2-9]\s*\+?-?\s*[2-9]?\s*[yY]e?a?r[Ss]?')<br>p2 = re.compile('[Cc]itizen(ship)?')</span></pre><p id="c1a2" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">As evident, I may have played it fast and loose with the regular expressions. But based on the job descriptions I have encountered before (hundreds). This seemed to be enough to eliminate jobs which I did not fit for as well as prevent me loosing out on suitable postings.</p><figure class="hd he hf hg hh hi dm dn paragraph-image"><div class="hw hx hy hz ak"><div class="dm dn ju"><div class="if r hy ig"><div class="jv r"><div class="ia ib dq t u ic ak cd id ie"><img class="dq t u ic ak ii ij ik" src="https://miro.medium.com/max/60/1*zoAPwfd41yaEBHcjxtpH2A.png?q=20" width="1500" height="950" role="presentation"></div><img class="ia ib dq t u ic ak hj" width="1500" height="950" role="presentation"><noscript><img class="dq t u ic ak" src="https://miro.medium.com/max/3000/1*zoAPwfd41yaEBHcjxtpH2A.png" width="1500" height="950" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg il im in do dm dn io ip as cv">Job title and company name with the link, stored in a text file.</figcaption></figure><p id="4b53" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">So there it is a <em class="ja">s</em>ample of the process I followed. Although, I did a lot more tweaking and I was more specific about what job titles I targeted.</p><p id="ebc9" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">Here’s the github link to the code: <a href="https://github.com/umangkshah/job-scraping-python/blob/master/job_scraper.ipynb" class="da by jg jh ji jj" target="_blank" rel="noopener nofollow">https://github.com/umangkshah/job-scraping-python/blob/master/job_scraper.ipynb</a></p></div></div></section><hr class="jw cv jx jy jz in ka kb kc kd ke"><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><p id="46dc" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">All that’s left now is to go and apply. I always make sure that I am a good fit for the job description and that my resume and cover letter has all the relevant details. Currently, I am exploring roles in Self Driving Car teams dealing with Perception, Localization, Mapping or Motion Planning and tricks like this are helping me find specific titles.</p><p id="560f" class="go gp ed at gq b gr gs gt gu gv gw gx gy gz ha hb dv">This is my first post on Medium (or ever ). I got a lot of help from other Medium articles on writing and formatting. Look forward to write more. I plan to cover some topics in AI/Self Driving Cars too. So thanks for reading, let me know how you optimize your job hunt?</p></div></div></section></div><div><div class="ds u dt du dv dw"></div><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><figure class="ee ef eg eh ei ej do dp paragraph-image"><div class="ek el em en ak"><div class="do dp ed"><div class="et r em eu"><div class="ev r"><div class="eo ep ds t u eq ak cd er es"><img class="ds t u eq ak ew ex ey" src="https://miro.medium.com/max/60/1*JoUDopwxqh5qMVtnlMrxgA.png?q=20" width="1615" height="773" role="presentation"></div><img class="eo ep ds t u eq ak ez" width="1615" height="773" role="presentation"><noscript><img class="ds t u eq ak" src="https://miro.medium.com/max/3230/1*JoUDopwxqh5qMVtnlMrxgA.png" width="1615" height="773" role="presentation"></noscript></div></div></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx">Photo: <a href="https://www.promptcloud.com/blog/scrape-twitter-data-using-python-r/" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">https://www.promptcloud.com/blog/scrape-twitter-data-using-python-r/</a></figcaption></figure><div><div id="a89a" class="fk fl fm at fn b fo fp fq fr fs ft fu fv fw fx fy"><h1 class="fn b fo fz fq ga fs gb fu gc fw gd fm">A Serverless Pipeline to retrieve, validate, and immerse the data to Azure SQL Server from Twitter.</h1></div><div class="ge"><div class="n gf gg gh gi"><div class="o n"><div><a href="/@ksw352?source=post_page-----1a5757b39bc8----------------------" rel="noopener"><div class="em gj gk"><div class="gl n gm o p ds gn go gp gq gr dw"><svg width="57" height="57" viewbox="0 0 57 57"><path fill-rule="evenodd" clip-rule="evenodd" d="M28.5 1.2A27.45 27.45 0 0 0 4.06 15.82L3 15.27A28.65 28.65 0 0 1 28.5 0C39.64 0 49.29 6.2 54 15.27l-1.06.55A27.45 27.45 0 0 0 28.5 1.2zM4.06 41.18A27.45 27.45 0 0 0 28.5 55.8a27.45 27.45 0 0 0 24.44-14.62l1.06.55A28.65 28.65 0 0 1 28.5 57 28.65 28.65 0 0 1 3 41.73l1.06-.55z"></path></svg></div><img alt="karanpreet singh wadhwa" class="r gs gk gj" src="https://miro.medium.com/fit/c/96/96/0*_cSkznHqiWVHYgJN.jpg" width="48" height="48"></div></a></div><div class="gt ak r"><div class="n"><div style="flex:1"><span class="as b at au av aw r fm q"><div class="gu n o gv"><span class="as cx fa au cd gw gx gy gz ha fm"><a href="/@ksw352?source=post_page-----1a5757b39bc8----------------------" class="dc dd bb bc bd be bf bg bh bi hb bl bm hc hd" rel="noopener">karanpreet singh wadhwa</a></span><div class="he r ap h"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-serverless-pipeline-to-retrieve-validate-and-immerse-the-data-to-azure-sql-server-from-twitter-1a5757b39bc8&amp;source=-4ad651423231-------------------------follow_byline-" class="hf fm q bq hg hh hi hj bi hc hk hl hm hn ho hp bt as b at hq cy aw bu bv bw bx by bl" rel="noopener">Follow</a></div></div></span></div></div><span class="as b at au av aw r ax ay"><span class="as cx fa au cd gw gx gy gz ha ax"><div><a class="dc dd bb bc bd be bf bg bh bi hb bl bm hc hd" rel="noopener" href="/a-serverless-pipeline-to-retrieve-validate-and-immerse-the-data-to-azure-sql-server-from-twitter-1a5757b39bc8?source=post_page-----1a5757b39bc8----------------------">Nov 2, 2019</a> <!-- -->·<!-- --> <!-- -->3<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewbox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div><div class="n hr hs ht hu hv hw hx hy ab"><div class="n o"><div class="hz r ap"><a href="//medium.com/p/1a5757b39bc8/share/twitter?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi ia ib bl bm hc hd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="hz r ap"><a href="//medium.com/p/1a5757b39bc8/share/facebook?source=post_actions_header---------------------------" class="dc dd bb bc bd be bf bg bh bi ia ib bl bm hc hd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="ic r ao"><a href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-serverless-pipeline-to-retrieve-validate-and-immerse-the-data-to-azure-sql-server-from-twitter-1a5757b39bc8&amp;source=post_actions_header--------------------------bookmark_sidebar-" class="dc dd bb bc bd be bf bg bh bi ia ib bl bm hc hd" rel="noopener"><svg width="25" height="25" viewbox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></a></div></div></div></div></div></div></div></div></section><hr class="id cx ie if ig fd ih ii ij ik il"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><blockquote class="im"><div id="0673" class="in io ip at fn b iq ir is it iu iv iw"><p class="fn b ix iy ax">Learning how to do data science is like learning to ski. You have to do it.</p></div></blockquote></div></div></section><hr class="id cx ie if ig fd ih ii ij ik il"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><h2 id="6268" class="iz ja fm at as jb jc jd je jf jg jh ji jj jk jl jm">Project Statement:</h2><p id="3cd7" class="jn jo fm at jp b jq jr js jt ju jv jw jx jy jz iw dx">Given a twitter ID, get a minimum of 100 followers (Modified this to keep in Azure function 5–10 min timeout period) and for each follower gather up to 200 tweets.<br>Store the tuple (twitterID,followerID,tweetID,tweet) into a table managed in Azure SQL Service.<br>1) You will have to create and set up a free Azure account.<br>2) Create a database and a table in that Azure account.<br>3) Create a twitter account with API <br>4). given twitter ID, gather follower ids of that twitter ID<br>4.1) for each of the follower ID gather up to 200 original tweets <br> — — exclude retweets, messages <br>5) Store that into the Azure table<br>6) Write a client to query that Azure table.<br>6.1) List all tweets for a given twitter ID<br>6.2) List follower ID for a given twitter ID</p><h2 id="07d5" class="iz ja fm at as jb jc jd je jf jg jh ji jj jk jl jm">Technology Used:</h2><ol class=""><li id="51f4" class="jn jo fm at jp b jq jr js jt ju jv jw jx jy jz iw ka kb kc">Python</li><li id="b5f0" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw ka kb kc">Twython library to extract tweeter data</li><li id="f4dc" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw ka kb kc">Azure Function</li><li id="cd8a" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw ka kb kc">Azure SQL server</li></ol><h2 id="83e7" class="iz ja fm at as jb jc jd je jf jg jh ji jj jk jl jm">Things I learned:</h2><ol class=""><li id="88a5" class="jn jo fm at jp b jq jr js jt ju jv jw jx jy jz iw ka kb kc">Azure SQL Server Usage from localhost as well as Azure Server-less and Azure Databricks.</li><li id="c770" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw ka kb kc">Azure Function.</li><li id="2ed6" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw ka kb kc">Learned <a href="https://twython.readthedocs.io/en/latest/" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">twython</a> library usage to extract tweets.</li></ol></div></div></section><hr class="id cx ie if ig fd ih ii ij ik il"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><h2 id="26d0" class="iz ja fm at as jb jc jd je jf jg jh ji jj jk jl jm">Brief Summary of steps followed while doing the <a href="https://github.com/ksw25/Extract-Data-From-Tweeter-And-Save-In-Azure-SQL-Using-Azure-ServerLess" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">project</a>:</h2><ol class=""><li id="6172" class="jn jo fm at jp b jq jr js jt ju jv jw jx jy jz iw ka kb kc">Created a <a href="https://developer.twitter.com/en/apply-for-access.html" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">Tweeter Developer Account</a>.</li><li id="053e" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw ka kb kc">Wrote a python script to extract the Follower’s ID for a given User ID.</li></ol><figure class="ki kj kk kl km ej"><div class="et r em"><div class="kn r"><iframe src="https://towardsdatascience.com/media/202da6dc67e388b975b57f9af6f38504" allowfullscreen="" frameborder="0" height="0" width="0" title="script to extract the Follower's ID for a given User ID." class="ds t u eq ak" scrolling="auto"></iframe></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx">Code for getting followers ID for a Given User ID</figcaption></figure><p id="7899" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">3. Wrote a python script the take Followers ID extracted in the previous step and retrieve at max 200 tweets for each.</p><figure class="ki kj kk kl km ej"><div class="et r em"><div class="kn r"><iframe src="https://towardsdatascience.com/media/20f9ad55ca70ae1ba72765a5647229c1" allowfullscreen="" frameborder="0" height="0" width="0" title="Script the take Followers ID extracted in the previous step and retrieve at max 200 tweets for each." class="ds t u eq ak" scrolling="auto"></iframe></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx">Code for getting followers Tweet</figcaption></figure><p id="2f40" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">4. Created an Azure SQL database.</p><p id="9d41" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">5. Wrote a python script to take the result from step 3 and save it to the Azure SQL server.</p><figure class="ki kj kk kl km ej"><div class="et r em"><div class="kn r"><iframe src="https://towardsdatascience.com/media/97d5077256e128ac31cc4cf7b8cdd4e6" allowfullscreen="" frameborder="0" height="0" width="0" title="Saving extracted data to Azure SQl server" class="ds t u eq ak" scrolling="auto"></iframe></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx">Code to Save extracted data to Azure SQL server</figcaption></figure><p id="96ff" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">6. I created an Azure function project and func. Modify script to work on Azure Function.</p><p id="14ed" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">7. Create 2 More Client functions for the following purpose.</p><ul class=""><li id="1452" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw kt kb kc">List all tweets for a given twitter ID</li></ul><figure class="ki kj kk kl km ej"><div class="et r em"><div class="kn r"><iframe src="https://towardsdatascience.com/media/74b1339c252e76f15ce81ab8a63d9a4a" allowfullscreen="" frameborder="0" height="0" width="0" title="List all tweets for a given twitter ID" class="ds t u eq ak" scrolling="auto"></iframe></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx">List all tweets for a given twitter ID</figcaption></figure><ul class=""><li id="0b7a" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw kt kb kc">List follower ID for a given twitter ID</li></ul><figure class="ki kj kk kl km ej"><div class="et r em"><div class="kn r"><iframe src="https://towardsdatascience.com/media/e9beebff88282a04a5abfe409528d09d" allowfullscreen="" frameborder="0" height="0" width="0" title="List follower ID for a given twitter ID" class="ds t u eq ak" scrolling="auto"></iframe></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx">List follower ID for a given twitter ID</figcaption></figure></div></div></section><hr class="id cx ie if ig fd ih ii ij ik il"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><h2 id="1030" class="iz ja fm at as jb jc jd je jf jg jh ji jj jk jl jm">Links for each Azure Functions:</h2><p id="5a98" class="jn jo fm at jp b jq jr js jt ju jv jw jx jy jz iw dx">(These are templates for you guys to look over. I have turned off the activation of these links so that they won’t work. <strong class="jp ku">I am not rich, sadly 😢 😭 .</strong>)</p><p id="cde6" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">Task 1: To save Followers’ ID and their Respective Tweets. (Placed in TweetWork in MyFunctionProj Directory)</p><ul class=""><li id="e614" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw kt kb kc"><a href="https://demo.azurewebsites.net/api/Tweetwork" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">https://demo.azurewebsites.net/api/Tweetwork</a></li><li id="fa1e" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw kt kb kc">for example <a href="https://demo.azurewebsites.net/api/Tweetwork?name=25073877" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">https://demo.azurewebsites.net/api/Tweetwork?name=25073877</a></li></ul><p id="9a4b" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">Task 2: List all tweets for a given twitter ID. (Placed in Client1BigData in MyFunctionProj Directory)</p><ul class=""><li id="40f0" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw kt kb kc"><a href="https://demo.azurewebsites.net/api/Client1BigData?code=NyhLElXnjBz08QButk1jkbaYLVdJE9vAKnX09CN1vrg==" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">https://demo.azurewebsites.net/api/Client1BigData?code=NyhLElXnjBz08QButk1jkbaYLVdJE9vAKnX09CN1vrg==</a></li><li id="8f31" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw kt kb kc">for example <a href="https://demo.azurewebsites.net/api/Client1BigData?code=NyhLElXnjBz08QButk1jkbaYLVdJE9vAK9CN1vrg==&amp;name=979178022367461376" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">https://demo.azurewebsites.net/api/Client1BigData?code=NyhLElXnjBz08QButk1jkbaYLVdJE9vAK9CN1vrg==&amp;name=979178022367461376</a></li></ul><p id="0eb9" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">Task 3: List follower ID for a given twitter ID. (Placed in Client2BigData in MyFunctionProj Directory)</p><ul class=""><li id="d451" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw kt kb kc"><a href="https://demo.azurewebsites.net/api/Client1BigData?code=NyhLElXnjBz08QButk1jkbaYLVdJE9vAK9CN1vrg==" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">https://demo.azurewebsites.net/api/Client1BigData?code=NyhLElXnjBz08QButk1jkbaYLVdJE9vAK9CN1vrg==</a></li><li id="48f8" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw kt kb kc">for example <a href="https://demo.azurewebsites.net/api/client2bigdata?code=2MO/r/Wvk5JQFsbQ1KKkA0hdWF1OCfdeyZjpENpoNkVGIS57Waw==&amp;name=25073877" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">https://demo.azurewebsites.net/api/client2bigdata?code=2MO/r/Wvk5JQFsbQ1KKkA0hdWF1OCfdeyZjpENpoNkVGIS57Waw==&amp;name=25073877</a></li></ul><h2 id="59f7" class="iz ja fm at as jb jc jd je jf jg jh ji jj jk jl jm">Challenges Faced:</h2><ul class=""><li id="42a5" class="jn jo fm at jp b jq jr js jt ju jv jw jx jy jz iw kt kb kc">If you are using Mac for debugging Azure function in Visual Studio, it is very hard as sometimes Visual studio does not create an exact extension/helper file to make debugging work. Personally, For me, It didn’t work at all. I had to push function online every time I wanted to check it. <strong class="jp ku">But I found the solution for it now. </strong>There are three files in .vscode then are sometimes screwed up. I would be mentioning them and what should it looks like. Namely,</li></ul><ol class=""><li id="cc36" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw ka kb kc"><em class="kv">task.json</em></li></ol><figure class="ki kj kk kl km ej"><div class="et r em"><div class="kn r"><iframe src="https://towardsdatascience.com/media/9167dd28bffe1eef5900da42ab72f2ff" allowfullscreen="" frameborder="0" height="0" width="0" title="task.json" class="ds t u eq ak" scrolling="auto"></iframe></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx">task.json</figcaption></figure><p id="ee90" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx"><em class="kv">2. launch.json</em></p><figure class="ki kj kk kl km ej"><div class="et r em"><div class="kn r"><iframe src="https://towardsdatascience.com/media/625e76c26a9c3fe42aef9074367ea1b8" allowfullscreen="" frameborder="0" height="0" width="0" title="launch.json" class="ds t u eq ak" scrolling="auto"></iframe></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx"><em class="kw">launch.json</em></figcaption></figure><p id="746e" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx"><em class="kv">3. settings.json</em></p><figure class="ki kj kk kl km ej"><div class="et r em"><div class="kn r"><iframe src="https://towardsdatascience.com/media/3e7953254596a48831a80664973d0621" allowfullscreen="" frameborder="0" height="0" width="0" title="settings.json" class="ds t u eq ak" scrolling="auto"></iframe></div></div><figcaption class="ax fa fb fc fd dq do dp fe ff as cx"><em class="kw">settings.json</em></figcaption></figure><ul class=""><li id="117c" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw kt kb kc">Azure Functions has its limitation as compared to AWS lambda. When I started writing it, I thought it would be the same as AWS lambda as both are serverless, but implementing it was way hard for two reasons. First, Azure function does not allow online code editing, which is provided by AWS.</li></ul><h2 id="28d9" class="iz ja fm at as jb jc jd je jf jg jh ji jj jk jl jm">Follow up:</h2><ul class=""><li id="132a" class="jn jo fm at jp b jq jr js jt ju jv jw jx jy jz iw kt kb kc">If I were making this for a company and had enough resources, I would have gone with Azure function Dedicated App Plan, which has a maximum time limit of 30 mins.</li></ul></div></div></section><hr class="id cx ie if ig fd ih ii ij ik il"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><p id="3004" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">Github: <a href="https://github.com/ksw25/Extract-Data-From-Tweeter-And-Save-In-Azure-SQL-Using-Azure-ServerLess" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">https://github.com/ksw25/Extract-Data-From-Tweeter-And-Save-In-Azure-SQL-Using-Azure-ServerLess</a></p></div></div></section><hr class="id cx ie if ig fd ih ii ij ik il"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><h2 id="804c" class="iz ja fm at as jb jc jd je jf jg jh ji jj jk jl jm">Acknowledgments :</h2><ul class=""><li id="1931" class="jn jo fm at jp b jq jr js jt ju jv jw jx jy jz iw kt kb kc">I did this as part of CS6513 Big Data Tools and Techniques at the Tandon School of Engineering, NYU</li><li id="07ee" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw kt kb kc">I acknowledge the IBM Power Systems Academic initiative for underwriting computing resources.</li><li id="b5f9" class="jn jo fm at jp b jq kd js ke ju kf jw kg jy kh iw kt kb kc">I acknowledge MSFT Azure for providing free Azure access to students.</li></ul></div></div></section><hr class="id cx ie if ig fd ih ii ij ik il"><section class="dx dy dz ea eb"><div class="n p"><div class="ac ae af ag ah ec aj ak"><p id="f23e" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">Regards,</p><p id="e334" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx"><em class="kv">Karanpreet Singh Wadhwa</em></p><p id="76d6" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx"><strong class="jp ku">Master’s</strong> in <strong class="jp ku">Computer Science</strong> | Class 2020</p><p id="a1d0" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">Tandon School of Engineering | New York University</p><p id="6a25" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx">Graduate Teaching Assistant — Computer Vision | New York University</p><p id="a8d3" class="jn jo fm at jp b jq ko js kp ju kq jw kr jy ks iw dx"><a href="mailto:karan.wadhwa@nyu.edu" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">karan.wadhwa@nyu.edu</a>| (929) 287–9899 | <a href="https://www.linkedin.com/in/karanpreet-wadhwa-540388175/" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">LinkedIn</a> | <a href="https://github.com/ksw25" class="dc by fg fh fi fj" target="_blank" rel="noopener nofollow">Github</a></p></div></div></section></div>