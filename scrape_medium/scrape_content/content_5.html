<div class="gl gm gn go gp gq ag gr ah gs aj ak"><figure class="gt gu gv gw gx gk gy gz paragraph-image"><div class="ha hb hc hd ak"><div class="dm dn ai"><div class="hj r hc hk"><div class="hl r"><div class="he hf dq t u hg ak cc hh hi"><img class="dq t u hg ak hm hn ho" src="https://miro.medium.com/freeze/max/60/1*wjQtHlKnE3xDk4Y4ZqUSSg.gif?q=20" width="1152" height="648" role="presentation"></div><img class="he hf dq t u hg ak hp" width="1152" height="648" role="presentation"><noscript><img class="dq t u hg ak" src="https://miro.medium.com/max/2304/1*wjQtHlKnE3xDk4Y4ZqUSSg.gif" width="1152" height="648" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg hq hr hs do dm dn ht hu as cw"><a href="https://dribbble.com/kidwill" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure></div><p id="125b" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv"><em class="in">So you need to extract data from a webpage into your application? How do you do it? Simple! Its called Webscaping and here‚Äôs how it's done.</em></p><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><h1 id="1f1d" class="ix iy ed at as iz ef ja eh jb jc jd je jf jg jh ji">What Is Web Scraping? ü§∑‚Äç‚ôÇÔ∏è</h1><blockquote class="jj jk jl"><p id="5879" class="hz ia ed in ib b ic id ie if ig ih ii ij ik il im dv"><strong class="ib jm">Web scraping</strong>, <strong class="ib jm">web harvesting</strong>, or <strong class="ib jm">web data extraction</strong> is <a href="https://en.wikipedia.org/wiki/Data_scraping" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">data scraping</a> used for <a href="https://en.wikipedia.org/wiki/Data_extraction" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">extracting data</a> from <a href="https://en.wikipedia.org/wiki/Website" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">websites</a>.</p></blockquote><p id="c834" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Webscraping software may access the World Wide Web directly using the <a href="https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Hypertext Transfer Protocol</a> or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a <a href="https://en.wikipedia.org/wiki/Internet_bot" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">bot</a> or <a href="https://en.wikipedia.org/wiki/Web_crawler" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">web crawler</a>. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local <a href="https://en.wikipedia.org/wiki/Database" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">database</a> or spreadsheet, for later <a href="https://en.wikipedia.org/wiki/Data_retrieval" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">retrieval</a> or <a href="https://en.wikipedia.org/wiki/Data_analysis" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">analysis</a>.</p><p id="bf78" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is the main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be <a href="https://en.wikipedia.org/wiki/Parsing" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">parsed</a>, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies, and their URLs, to a list (contact scraping).</p><p id="327e" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">There are, however, some web scraping software that will automatically load and extract data from multiple pages of websites based on your requirements. It is either custom-built for a specific website or is one that can be configured to work with any website. With the click of a button, you can easily save the data available on the website to a file on your computer.</p><p id="0d10" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Many services offer web scraping like <a href="https://medium.com/u/bbe5609206b0?source=post_page-----56117ed12b62----------------------" class="jn az by" target="_blank" rel="noopener">Scrapestorm Jp</a>, <a href="https://medium.com/u/3b0f669c90bc?source=post_page-----56117ed12b62----------------------" class="jn az by" target="_blank" rel="noopener">Grepsr</a>, and <a href="https://medium.com/u/4d3a276154a2?source=post_page-----56117ed12b62----------------------" class="jn az by" target="_blank" rel="noopener">ScrapingHub</a>. But today, I will be discussing how to build your own web scraper application using Java, NodeJs and Python.</p><h1 id="5ba7" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">Java WebScraper ‚òïÔ∏è</h1><p id="64c8" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv">The best library to use for Java webscraping is <a href="https://jsoup.org/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Jsoup</a>.</p><blockquote class="jj jk jl"><p id="de98" class="hz ia ed in ib b ic id ie if ig ih ii ij ik il im dv"><code class="hk jy jz ka kb b"><em class="at">jsoup</em></code> is a Java library for working with real-world HTML. It provides a very convenient API for extracting and manipulating data, using the best of DOM, CSS, and jquery-like methods.</p></blockquote><p id="b41a" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv"><code class="hk jy jz ka kb b">jsoup</code> implements the <a href="https://whatwg.org/html" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">WHATWG HTML5</a> specification, and parses HTML to the same DOM as modern browsers do.</p><ul class=""><li id="306d" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im kc kd ke">scrape and <a href="https://jsoup.org/cookbook/input/parse-document-from-string" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">parse</a> HTML from a URL, file, or string</li><li id="28f4" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://jsoup.org/cookbook/extracting-data/selector-syntax" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">find</a> and extract data, using DOM traversal or CSS selectors</li><li id="f722" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://jsoup.org/cookbook/modifying-data/set-html" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">manipulate</a> the HTML elements, attributes, and text</li><li id="c090" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://jsoup.org/cookbook/cleaning-html/whitelist-sanitizer" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">clean</a> user-submitted content against a safe white-list, to prevent XSS attacks</li><li id="5280" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://jsoup.org/apidocs/org/jsoup/select/Elements.html#html--" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">output</a> tidy HTML</li></ul><p id="7c58" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">jsoup is designed to deal with all varieties of HTML found in the wild, from pristine and validating, to invalid tag-soup; jsoup will create a sensible parse tree.</p><p id="df4a" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Download the Jsoup JAR file from <a href="https://jsoup.org/download" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a> and then create a java class containing the URL that you need to scrape:</p><pre class="gt gu gv gw gx kk kl cl"><span id="0c6a" class="km iy ed at kb b fg kn ko r kp">import java.io.IOException;<br>import org.jsoup.Jsoup;</span><span id="3bae" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">public class JsoupFromStringEx {</span><span id="679a" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">    public static void main(String[] args) throws IOException {</span><span id="b9fa" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">        String webPage = "https://www.google.com/";</span><span id="9bf0" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">        String html = Jsoup.<em class="in">connect</em>(webPage).get().html();</span><span id="c309" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">        System.<em class="in">out</em>.println(html);</span><span id="2f6d" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">    }<br>}</span></pre><p id="b8bf" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">After running the java class, the webpage data should be printed out. This is the most basic way of webscraping in Java. Of course, this does not separate the data; many functions need to be placed for the application to do so. To create a more elaborate webscraping application follow <a href="https://stackabuse.com/web-scraping-the-java-way/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">this</strong></a><strong class="ib jm">.</strong></p><h1 id="bbc3" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">NodeJs WebScraper üï∏</h1><p id="7c57" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv">By using the superb tutorial <a href="https://pusher.com/tutorials/web-scraper-node" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a><strong class="ib jm">, </strong>we create a new <code class="hk jy jz ka kb b">scraper</code> directory for this tutorial and initialize it with a <code class="hk jy jz ka kb b">package.json</code> file by running <code class="hk jy jz ka kb b">npm init -y</code> from the project root. Then run this command to install all the dependencies needed:</p><pre class="gt gu gv gw gx kk kl cl"><span id="ed99" class="km iy ed at kb b fg kn ko r kp"><strong class="kb jm">npm install </strong>axios<strong class="kb jm"> </strong>cheerio<strong class="kb jm"> </strong>puppeteer<strong class="kb jm"> --save</strong></span></pre><p id="0eea" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Here‚Äôs what each one does:</p><ul class=""><li id="3e7e" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im kc kd ke"><a href="https://github.com/axios/axios" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">Axios</strong></a>: Promise-based HTTP client for Node.js and the browser</li><li id="a8fa" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://cheerio.js.org/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">Cheerio</strong></a>: jQuery implementation for Node.js. Cheerio makes it easy to select, edit, and view DOM elements.</li><li id="50f1" class="hz ia ed at ib b ic kf ie kg ig kh ii ki ik kj im kc kd ke"><a href="https://github.com/GoogleChrome/puppeteer" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">Puppeteer</strong></a>: A Node.js library for controlling Google Chrome or Chromium.</li></ul><p id="e1fe" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">When the installation is complete, create a new <code class="hk jy jz ka kb b">pl-scraper.js</code> file in the root of your project directory and populate it with the following code:</p><pre class="gt gu gv gw gx kk kl cl"><span id="fbef" class="km iy ed at kb b fg kn ko r kp"><em class="in">// pl-scraper.js</em></span><span id="bfff" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">const axios = require('axios');</span><span id="38c2" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">const url = 'https://www.premierleague.com/stats/top/players/goals?se=-1&amp;cl=-1&amp;iso=-1&amp;po=-1?se=-1';</span><span id="241f" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">axios(url)<br>      .then(response =&gt; {<br>        const html = response.data;<br>        console.log(html);<br>      })<br>      .catch(console.error);</span></pre><p id="b796" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">If you run the code with <code class="hk jy jz ka kb b"><strong class="ib jm">node</strong> pl-scraper.js</code>, a long string of HTML will be printed to the console.</p><p id="0de6" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">And that‚Äôs it, you just retrieved all the data from a webpage using a NodeJs webscraper. But how can you parse the HTML for the exact data you need? Continue following <a href="https://medium.com/u/2b9d77ff34df?source=post_page-----56117ed12b62----------------------" class="jn az by" target="_blank" rel="noopener">Pusher</a>‚Äôs tutorial <a href="https://pusher.com/tutorials/web-scraper-node" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a>.</p><h1 id="ab68" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">Python Webscraper üêç</h1><p id="6cc6" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv">With reference to Python Docs found <a href="https://docs.python.org/3/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a><strong class="ib jm">, </strong>we start off by downloading <a href="http://lxml.de/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">lxml</a> that is a pretty extensive library written for parsing XML and HTML documents very quickly, even handling messed up tags in the process. We will also be using the <a href="http://docs.python-requests.org/en/latest/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Requests</a> module instead of the already built-in urllib2 module due to improvements in speed and readability. You can easily install both using <code class="hk jy jz ka kb b"><strong class="ib jm">pip</strong> <strong class="ib jm">install</strong> lxml</code> and <code class="hk jy jz ka kb b"><strong class="ib jm">pip</strong> <strong class="ib jm">install</strong> requests</code>.</p><p id="7918" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Let‚Äôs start with the imports:</p><pre class="gt gu gv gw gx kk kl cl"><span id="3c2d" class="km iy ed at kb b fg kn ko r kp"><strong class="kb jm">from</strong> lxml <strong class="kb jm">import</strong> html<br><strong class="kb jm">import</strong> requests</span></pre><p id="01b9" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Next, we will use <code class="hk jy jz ka kb b">requests.get</code> to retrieve the web page with our data, parse it using the <code class="hk jy jz ka kb b">html</code> module, and save the results in <code class="hk jy jz ka kb b">tree</code>:</p><pre class="gt gu gv gw gx kk kl cl"><span id="12a9" class="km iy ed at kb b fg kn ko r kp">page = requests.get<strong class="kb jm">(</strong>'http://econpy.pythonanywhere.com/ex/001.html'<strong class="kb jm">)</strong><br>tree = html.fromstring<strong class="kb jm">(</strong>page.content<strong class="kb jm">)</strong></span></pre><p id="7a35" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">(We need to use <code class="hk jy jz ka kb b">page.content</code> rather than <code class="hk jy jz ka kb b">page.text</code> because <code class="hk jy jz ka kb b">html.fromstring</code> implicitly expects <code class="hk jy jz ka kb b">bytes</code> as input.)</p><p id="70db" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv"><code class="hk jy jz ka kb b">tree</code> now contains the whole HTML file in a nice tree structure which we can go over two different ways: XPath and CSSSelect. In this example, we will focus on the former.</p><p id="527e" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">XPath is a way of locating information in structured documents such as HTML or XML documents. A good introduction to XPath is on W3Schools. There are also various tools for obtaining the XPath of elements such as FireBug for Firefox or the Chrome Inspector. If you‚Äôre using Chrome, you can right-click an element, choose ‚ÄòInspect element‚Äô, highlight the code, right-click again, and choose ‚ÄòCopy XPath‚Äô.</p><p id="04be" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">After a quick analysis, we see that in our page the data is contained in two elements ‚Äî one is a div with title ‚Äòbuyer-name‚Äô and the other is a span with class ‚Äòitem-price‚Äô:</p><pre class="gt gu gv gw gx kk kl cl"><span id="5a82" class="km iy ed at kb b fg kn ko r kp"><strong class="kb jm">&lt;div</strong> title="buyer-name"<strong class="kb jm">&gt;</strong>Carson Busses<strong class="kb jm">&lt;/div&gt;</strong><br><strong class="kb jm">&lt;span</strong> class="item-price"<strong class="kb jm">&gt;</strong>$29.95<strong class="kb jm">&lt;/span&gt;</strong></span></pre><p id="a9a5" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Knowing this we can create the correct XPath query and use the lxml <code class="hk jy jz ka kb b">xpath</code> function like this:</p><pre class="gt gu gv gw gx kk kl cl"><span id="7455" class="km iy ed at kb b fg kn ko r kp"><em class="in">#This will create a list of buyers:</em><br>buyers = tree.xpath<strong class="kb jm">(</strong>'//div[@title="buyer-name"]/text()'<strong class="kb jm">)</strong><br><em class="in">#This will create a list of prices</em><br>prices = tree.xpath<strong class="kb jm">(</strong>'//span[@class="item-price"]/text()'<strong class="kb jm">)</strong></span></pre><p id="c439" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Let‚Äôs see what we got exactly:</p><pre class="gt gu gv gw gx kk kl cl"><span id="116c" class="km iy ed at kb b fg kn ko r kp"><strong class="kb jm">print</strong> 'Buyers: '<strong class="kb jm">,</strong> buyers<br><strong class="kb jm">print</strong> 'Prices: '<strong class="kb jm">,</strong> prices</span><span id="0d1b" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">Buyers<strong class="kb jm">:</strong>  <strong class="kb jm">[</strong>'Carson Busses'<strong class="kb jm">,</strong> 'Earl E. Byrd'<strong class="kb jm">,</strong> 'Patty Cakes'<strong class="kb jm">,</strong><br>'Derri Anne Connecticut'<strong class="kb jm">,</strong> 'Moe Dess'<strong class="kb jm">,</strong> 'Leda Doggslife'<strong class="kb jm">,</strong> 'Dan Druff'<strong class="kb jm">,</strong><br>'Al Fresco'<strong class="kb jm">,</strong> 'Ido Hoe'<strong class="kb jm">,</strong> 'Howie Kisses'<strong class="kb jm">,</strong> 'Len Lease'<strong class="kb jm">,</strong> 'Phil Meup'<strong class="kb jm">,</strong><br>'Ira Pent'<strong class="kb jm">,</strong> 'Ben D. Rules'<strong class="kb jm">,</strong> 'Ave Sectomy'<strong class="kb jm">,</strong> 'Gary Shattire'<strong class="kb jm">,</strong><br>'Bobbi Soks'<strong class="kb jm">,</strong> 'Sheila Takya'<strong class="kb jm">,</strong> 'Rose Tattoo'<strong class="kb jm">,</strong> 'Moe Tell'<strong class="kb jm">]</strong></span><span id="6ec9" class="km iy ed at kb b fg kq kr ks kt ku ko r kp">Prices<strong class="kb jm">:</strong>  <strong class="kb jm">[</strong>'$29.95'<strong class="kb jm">,</strong> '$8.37'<strong class="kb jm">,</strong> '$15.26'<strong class="kb jm">,</strong> '$19.25'<strong class="kb jm">,</strong> '$19.25'<strong class="kb jm">,</strong><br>'$13.99'<strong class="kb jm">,</strong> '$31.57'<strong class="kb jm">,</strong> '$8.49'<strong class="kb jm">,</strong> '$14.47'<strong class="kb jm">,</strong> '$15.86'<strong class="kb jm">,</strong> '$11.11'<strong class="kb jm">,</strong><br>'$15.98'<strong class="kb jm">,</strong> '$16.27'<strong class="kb jm">,</strong> '$7.50'<strong class="kb jm">,</strong> '$50.85'<strong class="kb jm">,</strong> '$14.26'<strong class="kb jm">,</strong> '$5.68'<strong class="kb jm">,</strong><br>'$15.00'<strong class="kb jm">,</strong> '$114.07'<strong class="kb jm">,</strong> '$10.09'<strong class="kb jm">]</strong></span></pre><p id="898a" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Congratulations! We have successfully scraped all the data we wanted from a web page using lxml and Requests. We have it stored in memory as two lists. Now we can do all sorts of cool stuff with it: we can analyze it using Python, or we can save it to a file and share it with the world.</p><h1 id="3fbf" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">Caution ‚ö†Ô∏è</h1><p id="31b2" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv">So is it legal or illegal? Web scraping and crawling aren‚Äôt illegal by themselves. After all, you could scrape or crawl your own website, without a hitch‚Ä¶</p><figure class="gt gu gv gw gx gk hp kw bv kx ky kz la lb bg lc ld le lf lg lh paragraph-image"><div class="ha hb hc hd ak"><div class="dm dn kv"><div class="hj r hc hk"><div class="hl r"><div class="he hf dq t u hg ak cc hh hi"><img class="dq t u hg ak hm hn ho" src="https://miro.medium.com/max/60/1*XMwWhmkmiSs484luuLRQ7Q.jpeg?q=20" width="1920" height="1080" role="presentation"></div><img class="he hf dq t u hg ak hp" width="1920" height="1080" role="presentation"><noscript><img class="dq t u hg ak" src="https://miro.medium.com/max/3840/1*XMwWhmkmiSs484luuLRQ7Q.jpeg" width="1920" height="1080" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg hq hr hs do dm dn ht hu as cw"><a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjou97ZrrzmAhUJaBoKHZePBakQjRx6BAgBEAQ&amp;url=https%3A%2F%2Fwallpaperaccess.com%2Fhd-law&amp;psig=AOvVaw2_rG6d_1UqvKqyjJUMBEMT&amp;ust=1576661150366495" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><p id="9091" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">In 2016, the US Congress passed its first legislation specifically to target bad bots ‚Äî the <a href="https://www.congress.gov/bill/114th-congress/senate-bill/3183" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Better Online Ticket Sales (BOTS) Act</a>, which bans the use of software that circumvents security measures on ticket seller websites. Automated ticket scalping bots use several techniques to do their dirty work including web scraping that incorporates advanced business logic to identify scalping opportunities, input purchase details into shopping carts, and even resell inventory on secondary markets.</p><p id="0fad" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">In other words, if you‚Äôre a venue, organization or ticketing software platform, it is still on you to defend against this fraudulent activity during your major on sales. But of course, this depends on where in the world you are:</p><p id="4c17" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">The UK however, seems to have followed the US with its <a href="https://www.gov.uk/government/news/a-better-deal-for-consumers-in-the-digital-age" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Digital Economy Act 2017</a> which achieved Royal Assent in April. The Act seeks to protect consumers in a number of ways in an increasingly digital society, including by ‚Äúcracking down on ticket touts by making it a criminal offence for those that misuse bot technology to sweep up tickets and sell them at inflated prices in the secondary market.‚Äù</p><p id="89e0" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">You can read more about this <a href="https://resources.distilnetworks.com/all-blog-posts/is-web-scraping-illegal-depends-on-what-the-meaning-of-the-word-is-is" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><strong class="ib jm">here</strong></a><strong class="ib jm">.</strong></p><p id="88d3" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">To put that into perspective, companies themselves have the responsibility of protecting their own data from web scrapers as they have to invoke the law themselves. So before you go off and try to web scrape from a .gov webpage with your python program, think again!</p><h1 id="cc05" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">Use Cases „ÄΩÔ∏è</h1><p id="d3a3" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv"><a href="https://www.quora.com/What-are-examples-of-how-real-businesses-use-web-scraping-Are-there-any-types-of-businesses-which-use-this-more-than-others" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Businesses</a> use web scraping for different purposes and it varies on a case to case basis.</p><figure class="gt gu gv gw gx gk hp kw bv kx ky kz la lb bg lc ld le lf lg lh paragraph-image"><div class="ha hb hc hd ak"><div class="dm dn li"><div class="hj r hc hk"><div class="lj r"><div class="he hf dq t u hg ak cc hh hi"><img class="dq t u hg ak hm hn ho" src="https://miro.medium.com/freeze/max/60/1*wNGxHlTCsH9zU90WDouoDQ.gif?q=20" width="800" height="600" role="presentation"></div><img class="he hf dq t u hg ak hp" width="800" height="600" role="presentation"><noscript><img class="dq t u hg ak" src="https://miro.medium.com/max/1600/1*wNGxHlTCsH9zU90WDouoDQ.gif" width="800" height="600" role="presentation"></noscript></div></div></div></div><figcaption class="ax fg hq hr hs do dm dn ht hu as cw"><a href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwixrqj2r7zmAhUlzYUKHeRwAZ8QjRx6BAgBEAQ&amp;url=https%3A%2F%2Fdribbble.com%2Fshots%2F4171367-Coding-Freak&amp;psig=AOvVaw1Jdl8ZMhLi5rrYeb595BO2&amp;ust=1576661497136416" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">Source</a></figcaption></figure><p id="7e9e" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">In <strong class="ib jm">eCommerce</strong>, Retailers/ marketplaces use web scraping to monitor their competitor prices and to improve their product attributes. Also, collect product reviews to do sentimental analysis. <strong class="ib jm">Lawyers</strong> use web scraping to see the past judgment report for their case reference. <strong class="ib jm">Lead generation</strong> companies use it to scrape the email address and phone numbers. <strong class="ib jm">Recruiters</strong> use it to collects user's profiles. Some <strong class="ib jm">travel companies</strong> collect data in real-time to provide live tracking details. <strong class="ib jm">Media companies</strong> collect trending topics and use hashtags to collect information from social media profiles. <strong class="ib jm">Business directories</strong> scrape complete information about the business profile, address, email, phone, products/services, working hours, Geocodes, etc.<br>Each business has competition in the present world, So companies scrape their competitor information regularly to monitor the movements. <strong class="ib jm">Government</strong> secret agencies also scrape for national securities purpose.</p><p id="108d" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">It's safe to say that webscaping is a big field, and you have just finished a brief tour of that field, using Java, NodeJs, and Python as your guide. You have also learned that it is illegal to scrape some sites, and you should check their terms and conditions before scraping. So do your webscraping wisely!</p><h1 id="43d5" class="ix iy ed at as iz ef jo eh jp jc jq je jr jg js ji">References üìñ</h1><div class="lk ll lm ln lo lp"><a href="https://www.webharvy.com/articles/what-is-web-scraping.html" rel="noopener nofollow"><div class="ls n ap"><div class="lt n dp p lu lv"><h2 class="as iz lw au ed"><div class="cc lq fi fj lr fl">Web Scraping Explained</div></h2><div class="lx r"><h3 class="as cw fg au ax"><div class="cc lq fi fj lr fl">Web Scraping (also termed Screen Scraping, Web Data Extraction, Web Harvesting, etc.) is a technique employed to extract‚Ä¶</div></h3></div><div class="ly r"><h4 class="as cw cx au ax"><div class="cc lq fi fj lr fl">www.webharvy.com</div></h4></div></div><div class="lz r"><div class="ma r mb mc md lz me mf mg"></div></div></div></a></div></div></div></section><section class="dv dw dx dy dz"><div class="n p"><div class="ac ae af ag ah ea aj ak"><p id="caeb" class="hz ia ed at ib b ic id ie if ig ih ii ij ik il im dv">Still worried about implementing applications, API‚Äôs or backends? Oracle is here to help, with industry-standard cloud applications, their team of experts will make implementation more than enjoyable.</p><h2 id="85ee" class="km iy ed at as iz mh mi mj mk ml mm mn mo mp mq mr">‚òÅÔ∏è Follow to get a <a href="http://bit.ly/2HzFQJE" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow">free 30-day trial with Oracle Cloud services</a> ‚òÅÔ∏è</h2><p id="a983" class="hz ia ed at ib b ic jt ie ju ig jv ii jw ik jx im dv"><em class="in">Thank you for taking the time to read my article, if you‚Äôre looking for more posts like this, you can find me on </em><a href="https://www.linkedin.com/in/andrei-elekes/" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><em class="in">Linkedin</em></a><em class="in">, </em><a href="https://twitter.com/ElekesAndrei" class="da by hv hw hx hy" target="_blank" rel="noopener nofollow"><em class="in">Twitter</em></a><em class="in">, or </em><a class="da by hv hw hx hy" target="_blank" rel="noopener" href="/@aele54"><em class="in">Medium</em></a><em class="in">.</em></p></div></div></section>